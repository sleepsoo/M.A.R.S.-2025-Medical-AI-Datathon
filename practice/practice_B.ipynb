{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28b4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef3269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>radiology report</th>\n",
       "      <th>gender</th>\n",
       "      <th>anchor_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2117-12-16 16:49:00</td>\n",
       "      <td>2117-12-16 18:57:00</td>\n",
       "      <td>EXAMINATION:  C-SPINE NON-TRAUMA ___ VIEWS\\n\\n...</td>\n",
       "      <td>F</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2171-03-18 03:49:00</td>\n",
       "      <td>2171-03-18 06:20:00</td>\n",
       "      <td>EXAMINATION:  MR ___ SPINE WITH CONTRAST\\n\\nIN...</td>\n",
       "      <td>M</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2156-03-12 07:55:00</td>\n",
       "      <td>2156-03-12 09:10:00</td>\n",
       "      <td>EXAMINATION:  CTA ABD AND PELVIS\\n\\nINDICATION...</td>\n",
       "      <td>M</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2126-05-16 14:52:00</td>\n",
       "      <td>2126-05-17 09:15:00</td>\n",
       "      <td>EXAMINATION:  FOOT AP,LAT AND OBL LEFT\\n\\nINDI...</td>\n",
       "      <td>M</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2165-01-31 12:16:00</td>\n",
       "      <td>2165-01-31 14:15:00</td>\n",
       "      <td>EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...</td>\n",
       "      <td>M</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>2134-05-21 18:07:00</td>\n",
       "      <td>2134-05-21 18:33:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...</td>\n",
       "      <td>F</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>2137-02-12 08:15:00</td>\n",
       "      <td>2137-02-12 11:55:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>M</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>2179-10-20 09:22:00</td>\n",
       "      <td>2179-10-20 10:03:00</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>M</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>2137-05-02 00:38:00</td>\n",
       "      <td>2137-05-02 02:11:00</td>\n",
       "      <td>EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>2134-06-03 21:07:00</td>\n",
       "      <td>2134-06-03 21:49:00</td>\n",
       "      <td>INDICATION:    ___ with headache  // ?pna\\n\\nT...</td>\n",
       "      <td>M</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id            charttime            storetime  \\\n",
       "0           0  2117-12-16 16:49:00  2117-12-16 18:57:00   \n",
       "1           1  2171-03-18 03:49:00  2171-03-18 06:20:00   \n",
       "2           2  2156-03-12 07:55:00  2156-03-12 09:10:00   \n",
       "3           3  2126-05-16 14:52:00  2126-05-17 09:15:00   \n",
       "4           4  2165-01-31 12:16:00  2165-01-31 14:15:00   \n",
       "..        ...                  ...                  ...   \n",
       "95         95  2134-05-21 18:07:00  2134-05-21 18:33:00   \n",
       "96         96  2137-02-12 08:15:00  2137-02-12 11:55:00   \n",
       "97         97  2179-10-20 09:22:00  2179-10-20 10:03:00   \n",
       "98         98  2137-05-02 00:38:00  2137-05-02 02:11:00   \n",
       "99         99  2134-06-03 21:07:00  2134-06-03 21:49:00   \n",
       "\n",
       "                                     radiology report gender  anchor_age  \n",
       "0   EXAMINATION:  C-SPINE NON-TRAUMA ___ VIEWS\\n\\n...      F          69  \n",
       "1   EXAMINATION:  MR ___ SPINE WITH CONTRAST\\n\\nIN...      M          35  \n",
       "2   EXAMINATION:  CTA ABD AND PELVIS\\n\\nINDICATION...      M          84  \n",
       "3   EXAMINATION:  FOOT AP,LAT AND OBL LEFT\\n\\nINDI...      M          25  \n",
       "4   EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...      M          57  \n",
       "..                                                ...    ...         ...  \n",
       "95  EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...      F          70  \n",
       "96  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...      M          55  \n",
       "97  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...      M          53  \n",
       "98  EXAMINATION:  LIVER OR GALLBLADDER US (SINGLE ...      F          57  \n",
       "99  INDICATION:    ___ with headache  // ?pna\\n\\nT...      M          70  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./data/taskB_test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a47a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>radiology report</th>\n",
       "      <th>target</th>\n",
       "      <th>gender</th>\n",
       "      <th>anchor_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2189-12-18 0:28</td>\n",
       "      <td>2189-12-18 7:55</td>\n",
       "      <td>EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...</td>\n",
       "      <td>IMPRESSION: \\n\\nNo acute intrathoracic abnorma...</td>\n",
       "      <td>M</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2150-05-10 15:35</td>\n",
       "      <td>2150-05-10 17:13</td>\n",
       "      <td>EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...</td>\n",
       "      <td>IMPRESSION:\\n\\n\\n1. Moderately motion limited ...</td>\n",
       "      <td>F</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2121-03-16 12:16</td>\n",
       "      <td>2121-03-16 12:55</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>IMPRESSION: \\n\\nOverall improvement in central...</td>\n",
       "      <td>M</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2121-05-23 11:12</td>\n",
       "      <td>2121-05-23 11:31</td>\n",
       "      <td>INDICATION:  History: ___ with liver failure, ...</td>\n",
       "      <td>IMPRESSION: \\n\\nNo acute cardiopulmonary proce...</td>\n",
       "      <td>M</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2131-08-26 6:02</td>\n",
       "      <td>2131-08-26 6:49</td>\n",
       "      <td>EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...</td>\n",
       "      <td>IMPRESSION: \\n\\nAcute on chronic left subdural...</td>\n",
       "      <td>F</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>2177-12-05 2:49</td>\n",
       "      <td>2177-12-05 5:31</td>\n",
       "      <td>EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...</td>\n",
       "      <td>IMPRESSION:\\n\\n\\n1. Please note no prior exam ...</td>\n",
       "      <td>F</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>2120-06-25 1:59</td>\n",
       "      <td>2120-06-25 2:19</td>\n",
       "      <td>EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...</td>\n",
       "      <td>IMPRESSION:\\n\\n\\n1. Patchy opacification of bo...</td>\n",
       "      <td>M</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>2118-01-05 1:35</td>\n",
       "      <td>2118-01-08 14:53</td>\n",
       "      <td>EXAMINATION:  CT torso\\n\\nINDICATION:  History...</td>\n",
       "      <td>IMPRESSION:\\n\\n\\n1. No evidence of malignancy ...</td>\n",
       "      <td>F</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>2184-07-10 22:31</td>\n",
       "      <td>2184-07-10 22:58</td>\n",
       "      <td>EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...</td>\n",
       "      <td>IMPRESSION: \\n\\nLow lung volumes with bibasila...</td>\n",
       "      <td>F</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>2131-05-05 0:57</td>\n",
       "      <td>2131-05-05 4:47</td>\n",
       "      <td>EXAMINATION:  MRI CERVICAL, THORACIC, AND LUMB...</td>\n",
       "      <td>IMPRESSION:\\n\\n\\n1.  Study is degraded by moti...</td>\n",
       "      <td>M</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sample_id         charttime         storetime  \\\n",
       "0            0   2189-12-18 0:28   2189-12-18 7:55   \n",
       "1            1  2150-05-10 15:35  2150-05-10 17:13   \n",
       "2            2  2121-03-16 12:16  2121-03-16 12:55   \n",
       "3            3  2121-05-23 11:12  2121-05-23 11:31   \n",
       "4            4   2131-08-26 6:02   2131-08-26 6:49   \n",
       "..         ...               ...               ...   \n",
       "995        995   2177-12-05 2:49   2177-12-05 5:31   \n",
       "996        996   2120-06-25 1:59   2120-06-25 2:19   \n",
       "997        997   2118-01-05 1:35  2118-01-08 14:53   \n",
       "998        998  2184-07-10 22:31  2184-07-10 22:58   \n",
       "999        999   2131-05-05 0:57   2131-05-05 4:47   \n",
       "\n",
       "                                      radiology report  \\\n",
       "0    EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...   \n",
       "1    EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...   \n",
       "2    EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...   \n",
       "3    INDICATION:  History: ___ with liver failure, ...   \n",
       "4    EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...   \n",
       "..                                                 ...   \n",
       "995  EXAMINATION:  CT HEAD W/O CONTRAST\\n\\nINDICATI...   \n",
       "996  EXAMINATION:  CHEST (PORTABLE AP)\\n\\nINDICATIO...   \n",
       "997  EXAMINATION:  CT torso\\n\\nINDICATION:  History...   \n",
       "998  EXAMINATION:  CHEST (PA AND LAT)\\n\\nINDICATION...   \n",
       "999  EXAMINATION:  MRI CERVICAL, THORACIC, AND LUMB...   \n",
       "\n",
       "                                                target gender  anchor_age  \n",
       "0    IMPRESSION: \\n\\nNo acute intrathoracic abnorma...      M          66  \n",
       "1    IMPRESSION:\\n\\n\\n1. Moderately motion limited ...      F          91  \n",
       "2    IMPRESSION: \\n\\nOverall improvement in central...      M          91  \n",
       "3    IMPRESSION: \\n\\nNo acute cardiopulmonary proce...      M          59  \n",
       "4    IMPRESSION: \\n\\nAcute on chronic left subdural...      F          82  \n",
       "..                                                 ...    ...         ...  \n",
       "995  IMPRESSION:\\n\\n\\n1. Please note no prior exam ...      F          73  \n",
       "996  IMPRESSION:\\n\\n\\n1. Patchy opacification of bo...      M          89  \n",
       "997  IMPRESSION:\\n\\n\\n1. No evidence of malignancy ...      F          71  \n",
       "998  IMPRESSION: \\n\\nLow lung volumes with bibasila...      F          81  \n",
       "999  IMPRESSION:\\n\\n\\n1.  Study is degraded by moti...      M          68  \n",
       "\n",
       "[1000 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./data/taskB_train.csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa040c79",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b79e3d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 기본 정보:\n",
      "- 총 샘플 수: 1000\n",
      "- 컬럼: ['sample_id', 'charttime', 'storetime', 'radiology report', 'target', 'gender', 'anchor_age']\n",
      "- 결측값: sample_id            0\n",
      "charttime            0\n",
      "storetime            0\n",
      "radiology report    11\n",
      "target              11\n",
      "gender               0\n",
      "anchor_age           0\n",
      "dtype: int64\n",
      "Impression 길이 통계:\n",
      "count     989.000000\n",
      "mean      272.581395\n",
      "std       280.757533\n",
      "min        23.000000\n",
      "25%        83.000000\n",
      "50%       192.000000\n",
      "75%       359.000000\n",
      "max      2924.000000\n",
      "Name: target_length, dtype: float64\n",
      "Impression 단어 수 통계:\n",
      "count    989.000000\n",
      "mean      38.574317\n",
      "std       41.859871\n",
      "min        3.000000\n",
      "25%       10.000000\n",
      "50%       26.000000\n",
      "75%       52.000000\n",
      "max      432.000000\n",
      "Name: target_words, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 1. 데이터 로드 및 기본 정보 파악\n",
    "train_df = pd.read_csv('./data/taskB_train.csv')\n",
    "print(\"데이터 기본 정보:\")\n",
    "print(f\"- 총 샘플 수: {len(train_df)}\")\n",
    "print(f\"- 컬럼: {train_df.columns.tolist()}\")\n",
    "print(f\"- 결측값: {train_df.isnull().sum()}\")\n",
    "\n",
    "# 2. 타겟 길이 분석\n",
    "train_df['target_length'] = train_df['target'].str.len()\n",
    "train_df['target_words'] = train_df['target'].str.split().str.len()\n",
    "\n",
    "print(f\"Impression 길이 통계:\")\n",
    "print(train_df['target_length'].describe())\n",
    "print(f\"Impression 단어 수 통계:\")\n",
    "print(train_df['target_words'].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a506cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검사 유형별 분포:\n",
      "study_type\n",
      "chest         644\n",
      "ct_head       203\n",
      "ultrasound     66\n",
      "ct_abdomen     58\n",
      "general        27\n",
      "mri             2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def classify_study_type(text):\n",
    "    \"\"\"검사 유형 자동 분류\"\"\"\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # 키워드 기반 분류\n",
    "    if any(word in text_lower for word in ['chest', 'lung', 'cardiac', 'heart', 'thorax']):\n",
    "        return 'chest'\n",
    "    elif any(word in text_lower for word in ['head', 'brain', 'skull', 'intracranial', 'ct head']):\n",
    "        return 'ct_head'\n",
    "    elif any(word in text_lower for word in ['abdomen', 'pelvis', 'liver', 'kidney', 'bowel']):\n",
    "        return 'ct_abdomen'\n",
    "    elif any(word in text_lower for word in ['ultrasound', 'us ', 'doppler']):\n",
    "        return 'ultrasound'\n",
    "    elif any(word in text_lower for word in ['mri', 'magnetic']):\n",
    "        return 'mri'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "# 검사 유형별 분포 확인\n",
    "train_df['study_type'] = train_df['radiology report'].apply(classify_study_type)\n",
    "print(\"검사 유형별 분포:\")\n",
    "print(train_df['study_type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78f64734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 데이터 품질 체크 시작...\n",
      "=== 데이터 품질 체크 ===\n",
      "전체 데이터 수: 1000\n",
      "컬럼: ['sample_id', 'charttime', 'storetime', 'radiology report', 'target', 'gender', 'anchor_age', 'target_length', 'target_words', 'study_type', 'age_group']\n",
      "\n",
      "결측값 현황:\n",
      "  radiology report: 11개 (1.1%)\n",
      "  target: 11개 (1.1%)\n",
      "  target_words: 11개 (1.1%)\n",
      "\n",
      "유효한 radiology report: 989개\n",
      "샘플 report 길이: 357 문자\n",
      "FINDINGS 포함 여부: True\n",
      "유효한 target: 989개\n",
      "Target 길이 통계:\n",
      "count    1000.000000\n",
      "mean      269.616000\n",
      "std       280.621212\n",
      "min         3.000000\n",
      "25%        80.000000\n",
      "50%       190.500000\n",
      "75%       356.000000\n",
      "max      2924.000000\n",
      "Name: target_length, dtype: float64\n",
      "\n",
      "2. BERTScore 테스트 시작...\n",
      "정제 전 데이터 수: 1000\n",
      "radiology report NaN 수: 11\n",
      "target NaN 수: 11\n",
      "정제 후 데이터 수: 989\n",
      "테스트 샘플 수: 500\n",
      "최종 테스트 케이스 수: 499\n",
      "로컬 BERTScore 결과:\n",
      "Precision: 0.8141\n",
      "Recall: 0.7095\n",
      "F1: 0.7574\n",
      "\n",
      "최종 BERTScore: 0.7574\n",
      "⚠️ 보통 성능. 프롬프트 개선 필요\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "import random\n",
    "\n",
    "# 1. NaN 안전 처리 함수 수정\n",
    "def extract_findings_only(text):\n",
    "    \"\"\"FINDINGS 섹션만 추출 - NaN 안전 처리\"\"\"\n",
    "    # NaN 체크 및 문자열 변환\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)  # 확실히 문자열로 변환\n",
    "    \n",
    "    if 'FINDINGS:' in text:\n",
    "        findings = text.split('FINDINGS:')[1]\n",
    "        if 'IMPRESSION:' in findings:\n",
    "            findings = findings.split('IMPRESSION:')[0]\n",
    "        return findings.strip()\n",
    "    return text.strip()\n",
    "\n",
    "def classify_study_type(text):\n",
    "    \"\"\"검사 유형 자동 분류 - NaN 안전 처리\"\"\"\n",
    "    # NaN 체크\n",
    "    if pd.isna(text) or text is None:\n",
    "        return 'general'\n",
    "    \n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # 키워드 기반 분류\n",
    "    if any(word in text_lower for word in ['chest', 'lung', 'cardiac', 'heart', 'thorax']):\n",
    "        return 'chest'\n",
    "    elif any(word in text_lower for word in ['head', 'brain', 'skull', 'intracranial', 'ct head']):\n",
    "        return 'ct_head'\n",
    "    elif any(word in text_lower for word in ['abdomen', 'pelvis', 'liver', 'kidney', 'bowel']):\n",
    "        return 'ct_abdomen'\n",
    "    elif any(word in text_lower for word in ['ultrasound', 'us ', 'doppler']):\n",
    "        return 'ultrasound'\n",
    "    elif any(word in text_lower for word in ['mri', 'magnetic']):\n",
    "        return 'mri'\n",
    "    else:\n",
    "        return 'general'\n",
    "\n",
    "# 2. 데이터 정제 추가\n",
    "def clean_dataframe(df):\n",
    "    \"\"\"데이터프레임 정제\"\"\"\n",
    "    # 필수 컬럼 체크\n",
    "    required_columns = ['radiology report', 'target']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            print(f\"경고: '{col}' 컬럼이 없습니다.\")\n",
    "            return df\n",
    "    \n",
    "    # NaN 값 처리\n",
    "    print(f\"정제 전 데이터 수: {len(df)}\")\n",
    "    print(f\"radiology report NaN 수: {df['radiology report'].isna().sum()}\")\n",
    "    print(f\"target NaN 수: {df['target'].isna().sum()}\")\n",
    "    \n",
    "    # NaN이 있는 행 제거\n",
    "    df_clean = df.dropna(subset=['radiology report', 'target']).copy()\n",
    "    \n",
    "    # 빈 문자열 제거\n",
    "    df_clean = df_clean[\n",
    "        (df_clean['radiology report'].str.strip() != '') & \n",
    "        (df_clean['target'].str.strip() != '')\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"정제 후 데이터 수: {len(df_clean)}\")\n",
    "    return df_clean\n",
    "\n",
    "# 3. 수정된 로컬 테스트 함수\n",
    "def local_bertscore_test():\n",
    "    \"\"\"로컬 BERTScore 테스트 - 안전 처리\"\"\"\n",
    "    try:\n",
    "        # 데이터 정제\n",
    "        train_clean = clean_dataframe(train_df)\n",
    "        \n",
    "        if len(train_clean) == 0:\n",
    "            print(\"정제된 데이터가 없습니다.\")\n",
    "            return 0.0\n",
    "        \n",
    "        # BERTScorer 초기화\n",
    "        scorer = BERTScorer(model_type=\"distilbert-base-uncased\", batch_size=8)\n",
    "        \n",
    "        # 테스트용 샘플 선별 (최대 500개)\n",
    "        sample_size = min(500, len(train_clean))\n",
    "        test_samples = train_clean.sample(sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"테스트 샘플 수: {len(test_samples)}\")\n",
    "        \n",
    "        # 예측 및 참조 데이터 준비\n",
    "        predictions = []\n",
    "        references = []\n",
    "        \n",
    "        for idx, row in test_samples.iterrows():\n",
    "            try:\n",
    "                # FINDINGS 추출\n",
    "                findings = extract_findings_only(row['radiology report'])\n",
    "                target = str(row['target']).strip()\n",
    "                \n",
    "                # 유효한 데이터만 포함\n",
    "                if findings and target and len(findings) > 10:\n",
    "                    # 임시 예측 (실제로는 LLM 호출)\n",
    "                    # 여기서는 간단한 휴리스틱 사용\n",
    "                    prediction = generate_dummy_impression(findings)\n",
    "                    \n",
    "                    predictions.append(prediction)\n",
    "                    references.append(target)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"샘플 처리 오류 (idx: {idx}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(\"처리 가능한 샘플이 없습니다.\")\n",
    "            return 0.0\n",
    "        \n",
    "        print(f\"최종 테스트 케이스 수: {len(predictions)}\")\n",
    "        \n",
    "        # BERTScore 계산\n",
    "        P, R, F1 = scorer.score(predictions, references)\n",
    "        \n",
    "        print(f\"로컬 BERTScore 결과:\")\n",
    "        print(f\"Precision: {P.mean():.4f}\")\n",
    "        print(f\"Recall: {R.mean():.4f}\")\n",
    "        print(f\"F1: {F1.mean():.4f}\")\n",
    "        \n",
    "        # 목표: F1 > 0.85\n",
    "        return F1.mean().item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"BERTScore 테스트 오류: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def generate_dummy_impression(findings):\n",
    "    \"\"\"임시 Impression 생성 (테스트용)\"\"\"\n",
    "    findings_lower = findings.lower()\n",
    "    \n",
    "    # 간단한 키워드 기반 임시 생성\n",
    "    if 'normal' in findings_lower or 'unremarkable' in findings_lower:\n",
    "        return \"No acute abnormality.\"\n",
    "    elif 'effusion' in findings_lower:\n",
    "        return \"Pleural effusion noted.\"\n",
    "    elif 'consolidation' in findings_lower:\n",
    "        return \"Pulmonary consolidation present.\"\n",
    "    elif 'hemorrhage' in findings_lower:\n",
    "        return \"Intracranial hemorrhage identified.\"\n",
    "    else:\n",
    "        return \"Findings as described above.\"\n",
    "\n",
    "# 4. 데이터 품질 체크 함수\n",
    "def check_data_quality():\n",
    "    \"\"\"데이터 품질 사전 체크\"\"\"\n",
    "    print(\"=== 데이터 품질 체크 ===\")\n",
    "    \n",
    "    # 기본 정보\n",
    "    print(f\"전체 데이터 수: {len(train_df)}\")\n",
    "    print(f\"컬럼: {train_df.columns.tolist()}\")\n",
    "    \n",
    "    # 결측값 체크\n",
    "    missing_info = train_df.isnull().sum()\n",
    "    print(f\"\\n결측값 현황:\")\n",
    "    for col, count in missing_info.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count}개 ({count/len(train_df)*100:.1f}%)\")\n",
    "    \n",
    "    # 핵심 컬럼 체크\n",
    "    if 'radiology report' in train_df.columns:\n",
    "        valid_reports = train_df['radiology report'].notna().sum()\n",
    "        print(f\"\\n유효한 radiology report: {valid_reports}개\")\n",
    "        \n",
    "        # 샘플 확인\n",
    "        sample_report = train_df['radiology report'].dropna().iloc[0]\n",
    "        print(f\"샘플 report 길이: {len(str(sample_report))} 문자\")\n",
    "        print(f\"FINDINGS 포함 여부: {'FINDINGS' in str(sample_report)}\")\n",
    "    \n",
    "    if 'target' in train_df.columns:\n",
    "        valid_targets = train_df['target'].notna().sum()\n",
    "        print(f\"유효한 target: {valid_targets}개\")\n",
    "        \n",
    "        # 길이 분석\n",
    "        train_df['target_length'] = train_df['target'].astype(str).str.len()\n",
    "        print(f\"Target 길이 통계:\")\n",
    "        print(train_df['target_length'].describe())\n",
    "\n",
    "# 실행 순서\n",
    "print(\"1. 데이터 품질 체크 시작...\")\n",
    "check_data_quality()\n",
    "\n",
    "print(\"\\n2. BERTScore 테스트 시작...\")\n",
    "local_score = local_bertscore_test()\n",
    "print(f\"\\n최종 BERTScore: {local_score:.4f}\")\n",
    "\n",
    "if local_score > 0.8:\n",
    "    print(\"✅ 좋은 성능! 다음 단계로 진행 가능\")\n",
    "elif local_score > 0.6:\n",
    "    print(\"⚠️ 보통 성능. 프롬프트 개선 필요\")\n",
    "else:\n",
    "    print(\"❌ 낮은 성능. 전략 재검토 필요\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1a1b28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "성별 분포: gender\n",
      "M    500\n",
      "F    500\n",
      "Name: count, dtype: int64\n",
      "연령대 분포: age_group\n",
      "Senior     426\n",
      "Elderly    352\n",
      "Middle     166\n",
      "Young       56\n",
      "Name: count, dtype: int64\n",
      "M 성별 평균 Impression 길이: 37.9 단어\n",
      "F 성별 평균 Impression 길이: 39.3 단어\n",
      "Young 연령대 평균 Impression 길이: 36.8 단어\n",
      "Middle 연령대 평균 Impression 길이: 41.0 단어\n",
      "Senior 연령대 평균 Impression 길이: 37.6 단어\n",
      "Elderly 연령대 평균 Impression 길이: 38.9 단어\n"
     ]
    }
   ],
   "source": [
    "def fairness_analysis():\n",
    "    \"\"\"공정성 지표 사전 분석\"\"\"\n",
    "    \n",
    "    # 성별별 분포 확인\n",
    "    gender_dist = train_df['gender'].value_counts()\n",
    "    print(\"성별 분포:\", gender_dist)\n",
    "    \n",
    "    # 연령대별 분포\n",
    "    train_df['age_group'] = pd.cut(train_df['anchor_age'], \n",
    "                                   bins=[0,30,50,70,100], \n",
    "                                   labels=['Young','Middle','Senior','Elderly'])\n",
    "    age_dist = train_df['age_group'].value_counts()\n",
    "    print(\"연령대 분포:\", age_dist)\n",
    "    \n",
    "    # 각 그룹별 Impression 특성 분석\n",
    "    for gender in ['M', 'F']:\n",
    "        gender_data = train_df[train_df['gender'] == gender]\n",
    "        avg_length = gender_data['target_words'].mean()\n",
    "        print(f\"{gender} 성별 평균 Impression 길이: {avg_length:.1f} 단어\")\n",
    "    \n",
    "    for age_group in ['Young','Middle','Senior','Elderly']:\n",
    "        age_data = train_df[train_df['age_group'] == age_group]\n",
    "        if len(age_data) > 0:\n",
    "            avg_length = age_data['target_words'].mean()\n",
    "            print(f\"{age_group} 연령대 평균 Impression 길이: {avg_length:.1f} 단어\")\n",
    "\n",
    "# 실행\n",
    "fairness_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf0cf9",
   "metadata": {},
   "source": [
    "## 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1213ffdb",
   "metadata": {},
   "source": [
    "## Few-shot용 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d8e6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로드 중...\n",
      "전체 데이터: 1000개\n",
      "유효 데이터: 989개\n",
      "전처리 후: 988개\n",
      "\n",
      "선별된 고품질 예시: 16개\n",
      "상위 10개 예시:\n",
      "\n",
      "=== 예시 1 (점수: 12, 유형: chest) ===\n",
      "FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary\n",
      "edema.  There is mild bi...\n",
      "IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema.  Possible small\n",
      "bilateral pleural effusions.\n",
      "2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 2 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: There is minimal bibasilar atelectasis.  No focal consolidation, pleural\n",
      "effusion or pneumothorax.  ...\n",
      "IMPRESSION: Minimal bibasilar atelectasis.  Otherwise no acute cardiopulmonary\n",
      "abnormality.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 3 (점수: 10, 유형: ct_head) ===\n",
      "FINDINGS: There is no evidence of infarction, hemorrhage, edema, or mass.  The\n",
      "ventricles and sulci are normal...\n",
      "IMPRESSION: 1.  No acute intracranial abnormality, with no definite evidence of acute\n",
      "intracranial hemorrhage.\n",
      "2. Please note that noncontrast CT of the head has limited sensitivity for\n",
      "assessment of intracranial mass lesions and for infarcts.  If continued\n",
      "concern for intracranial metastatic disease, recommend contrast-enhanced MRI\n",
      "for further evaluation.\n",
      "3. Within limits of study, no definite evidence of intracranial mass.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 4 (점수: 10, 유형: ultrasound) ===\n",
      "FINDINGS: Study is degraded by motion and by lumbar spinal fusion hardware artifact. \n",
      "Within these confines:\n",
      "\n",
      "...\n",
      "IMPRESSION: 1.  Study is degraded by motion and by lumbar spinal fusion hardware artifact.\n",
      "2. Cervical degenerative disc disease as detailed above, without high-grade\n",
      "spinal canal narrowing or cord signal abnormality.  There is severe neural\n",
      "foraminal narrowing at multiple levels.\n",
      "3. Mild thoracic degenerative disc disease, without high-grade spinal canal or\n",
      "neural foraminal narrowing.\n",
      "4. Loculated right pleural effusion basilar right lower lobe could reflect\n",
      "atelectasis, however pneumonia cannot be excluded.  Chest CT is suggested.\n",
      "5. Instrumented lumbar fusion at L4-S1, interbody fusion graft at L3-4 with\n",
      "partial osseous fusion, and solid osseous fusion of the L2-3 level as detailed\n",
      "above.\n",
      "6. L1-2 disc extrusion with superior migration results in severe spinal canal\n",
      "narrowing.  There is probable impingement of the traversing L2 and possibly\n",
      "other nerve roots.  Allowing for difference technique, finding may be slightly\n",
      "progressed compared to ___ prior exam.\n",
      "7.  Within limits of study, no definite evidence of discitis-osteomyelitis, or\n",
      "epidural abscess.\n",
      "8. Probable subacute to chronic oblique fracture of the superior endplate of\n",
      "L2 with lateral extension through the lateral vertebral body.\n",
      "9. Right L1-2 and bilateral L2-3 Severe neural foraminal narrowing.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 5 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: Low lung volumes.  There is mild interstitial pulmonary edema.  Bibasilar\n",
      "atelectasis.  No focal con...\n",
      "IMPRESSION: 1. Low lung volumes with mild interstitial pulmonary edema.\n",
      "2. No focal consolidations to suggest pneumonia.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 6 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: EXAMINATION:  CHEST (PA AND LAT)\n",
      "\n",
      "INDICATION:  ___ year old woman with new DKA, question infiltrate ...\n",
      "IMPRESSION: No previous images.  Cardiac silhouette is within normal limits and there is\n",
      "no vascular congestion, pleural effusion, or acute focal pneumonia.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 7 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are mildly hypoinflated and clear.  No pleural effusion or\n",
      "pneumothorax.  Stable calcified...\n",
      "IMPRESSION: 1. No acute cardiopulmonary process.  No pneumonia.\n",
      "2. Evidence of prior granulomatous exposure with stable calcified hilar and\n",
      "mediastinal lymph nodes.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 8 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are well inflated and clear.  There is mild cardiomegaly and a small\n",
      "right pleural effusio...\n",
      "IMPRESSION: 1.  No focal consolidation.  Biapical scarring.\n",
      "\n",
      "2.  Small right pleural effusion and mild cardiomegaly.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 9 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are well inflated with mild vascular congestion.  Trace right\n",
      "pleural effusion noted.  No ...\n",
      "IMPRESSION: 1.  Mild vascular congestion with trace right pleural effusion and mild\n",
      "cardiomegaly.\n",
      "2.  No pneumonia or widened mediastinum.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 10 (점수: 9, 유형: ct_head) ===\n",
      "FINDINGS: There is no evidence of territorial infarction,acute hemorrhage,edema,or mass.\n",
      "3 mm round calcificat...\n",
      "IMPRESSION: 1. No acute intracranial abnormality on noncontrast CT head.  Specifically no\n",
      "evidence of acute large territory infarct or intracranial hemorrhage.\n",
      "2. Additional findings described above.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2v/7_c8xzcd0wz7syldr0lxdtk00000gn/T/ipykernel_36769/4041078082.py:73: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  df.loc[~df['impression_clean'].str.contains(r'\\b(\\w+)\\s+\\1\\b', regex=True, na=False), 'quality_score'] += 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def extract_findings_clean(text):\n",
    "    \"\"\"FINDINGS 섹션만 정제 추출 - NaN 처리\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    if 'FINDINGS:' in text:\n",
    "        findings = text.split('FINDINGS:')[1]\n",
    "        if 'IMPRESSION:' in findings:\n",
    "            findings = findings.split('IMPRESSION:')[0]\n",
    "        return findings.strip()\n",
    "    elif 'FINDINGS' in text:\n",
    "        findings = text.split('FINDINGS')[1]\n",
    "        if 'IMPRESSION' in findings:\n",
    "            findings = findings.split('IMPRESSION')[0]\n",
    "        return findings.strip()\n",
    "    return text.strip()\n",
    "\n",
    "def clean_impression(text):\n",
    "    \"\"\"IMPRESSION 정제 - NaN 처리\"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "        \n",
    "    if 'IMPRESSION:' in text:\n",
    "        impression = text.split('IMPRESSION:')[1].strip()\n",
    "    else:\n",
    "        impression = text.strip()\n",
    "    return impression\n",
    "\n",
    "def get_best_fewshot_examples(train_csv_path: str):\n",
    "    \"\"\"고품질 Few-shot 예시 추출 - 에러 없는 버전\"\"\"\n",
    "    \n",
    "    print(\"데이터 로드 중...\")\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    print(f\"전체 데이터: {len(df)}개\")\n",
    "    \n",
    "    # NaN 값 제거\n",
    "    df = df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"유효 데이터: {len(df)}개\")\n",
    "    \n",
    "    # 기본 전처리\n",
    "    df['findings_clean'] = df['radiology report'].apply(extract_findings_clean)\n",
    "    df['impression_clean'] = df['target'].apply(clean_impression)\n",
    "    \n",
    "    # 빈 값 제거\n",
    "    df = df[(df['findings_clean'] != '') & (df['impression_clean'] != '')]\n",
    "    print(f\"전처리 후: {len(df)}개\")\n",
    "    \n",
    "    # 길이 계산\n",
    "    df['findings_words'] = df['findings_clean'].str.split().str.len()\n",
    "    df['impression_words'] = df['impression_clean'].str.split().str.len()\n",
    "    \n",
    "    # 품질 점수 계산\n",
    "    df['quality_score'] = 0\n",
    "    \n",
    "    # 1. 적절한 impression 길이 (5-25 단어)\n",
    "    ideal_length = (df['impression_words'] >= 5) & (df['impression_words'] <= 25)\n",
    "    df.loc[ideal_length, 'quality_score'] += 3\n",
    "    \n",
    "    # 2. 구조화된 형태 (번호 매김)\n",
    "    df.loc[df['impression_clean'].str.contains(r'\\d+\\.', regex=True, na=False), 'quality_score'] += 2\n",
    "    \n",
    "    # 3. 핵심 의학 용어 포함\n",
    "    medical_terms = ['normal', 'unremarkable', 'acute', 'chronic', 'consolidation', \n",
    "                    'atelectasis', 'effusion', 'pneumonia', 'mass', 'lesion', 'abnormality']\n",
    "    for term in medical_terms:\n",
    "        df.loc[df['impression_clean'].str.lower().str.contains(term, na=False, regex=False), 'quality_score'] += 1\n",
    "    \n",
    "    # 4. 간결성 (과도한 중복 없음)\n",
    "    df.loc[~df['impression_clean'].str.contains(r'\\b(\\w+)\\s+\\1\\b', regex=True, na=False), 'quality_score'] += 1\n",
    "    \n",
    "    # 5. findings 길이 적절성 (50-200 단어)\n",
    "    findings_ideal = (df['findings_words'] >= 15) & (df['findings_words'] <= 60)\n",
    "    df.loc[findings_ideal, 'quality_score'] += 2\n",
    "    \n",
    "    # 검사 유형 분류\n",
    "    def classify_study_type(text):\n",
    "        text_lower = str(text).lower()\n",
    "        if any(word in text_lower for word in ['chest', 'lung', 'cardiac', 'heart', 'thorax']):\n",
    "            return 'chest'\n",
    "        elif any(word in text_lower for word in ['head', 'brain', 'skull', 'intracranial']):\n",
    "            return 'ct_head'\n",
    "        elif any(word in text_lower for word in ['abdomen', 'pelvis', 'liver', 'kidney']):\n",
    "            return 'ct_abdomen'\n",
    "        elif any(word in text_lower for word in ['ultrasound', 'us ', 'doppler']):\n",
    "            return 'ultrasound'\n",
    "        else:\n",
    "            return 'general'\n",
    "    \n",
    "    df['study_type'] = df['radiology report'].apply(classify_study_type)\n",
    "    \n",
    "    # 유형별 최고 예시 선택\n",
    "    best_examples = []\n",
    "    \n",
    "    for study_type in ['chest', 'ct_head', 'ct_abdomen', 'ultrasound']:\n",
    "        type_df = df[df['study_type'] == study_type]\n",
    "        if len(type_df) > 0:\n",
    "            top_2 = type_df.nlargest(2, 'quality_score')\n",
    "            best_examples.extend(top_2[['findings_clean', 'impression_clean', 'quality_score', 'study_type']].to_dict('records'))\n",
    "    \n",
    "    # 전체 최고 품질 예시 추가 (중복 제거)\n",
    "    all_top = df.nlargest(12, 'quality_score')\n",
    "    for _, row in all_top.iterrows():\n",
    "        example = {\n",
    "            'findings_clean': row['findings_clean'],\n",
    "            'impression_clean': row['impression_clean'], \n",
    "            'quality_score': row['quality_score'],\n",
    "            'study_type': row['study_type']\n",
    "        }\n",
    "        # 중복 체크 (같은 impression 없으면 추가)\n",
    "        if not any(ex['impression_clean'] == example['impression_clean'] for ex in best_examples):\n",
    "            best_examples.append(example)\n",
    "    \n",
    "    # 점수 순 정렬\n",
    "    best_examples = sorted(best_examples, key=lambda x: x['quality_score'], reverse=True)\n",
    "    \n",
    "    print(f\"\\n선별된 고품질 예시: {len(best_examples)}개\")\n",
    "    print(\"상위 10개 예시:\")\n",
    "    \n",
    "    for i, example in enumerate(best_examples[:10]):\n",
    "        print(f\"\\n=== 예시 {i+1} (점수: {example['quality_score']}, 유형: {example['study_type']}) ===\")\n",
    "        print(f\"FINDINGS: {example['findings_clean'][:100]}...\")\n",
    "        print(f\"IMPRESSION: {example['impression_clean']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return best_examples\n",
    "\n",
    "# 실행\n",
    "best_examples = get_best_fewshot_examples('./data/taskB_train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2e958f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 전체 고품질 예시 (완전판) ===\n",
      "\n",
      "=== 예시 1 (점수: 12, 유형: chest) ===\n",
      "FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary\n",
      "edema.  There is mild bibasilar atelectasis, but no focal consolidations to\n",
      "suggest pneumonia.  Possible small bilateral pleural effusions.  No\n",
      "pneumothorax.\n",
      "IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema.  Possible small\n",
      "bilateral pleural effusions.\n",
      "2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 2 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: There is minimal bibasilar atelectasis.  No focal consolidation, pleural\n",
      "effusion or pneumothorax.  The size of the cardiac silhouette is within normal\n",
      "limits.  There is calcification of the aortic arch.  Multilevel degenerative\n",
      "changes are seen in the thoracic spine.\n",
      "IMPRESSION: Minimal bibasilar atelectasis.  Otherwise no acute cardiopulmonary\n",
      "abnormality.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 3 (점수: 10, 유형: ct_head) ===\n",
      "FINDINGS: There is no evidence of infarction, hemorrhage, edema, or mass.  The\n",
      "ventricles and sulci are normal in size and configuration. Incidental note is\n",
      "made of a cavum septum pellucidum.\n",
      "\n",
      "There is no evidence of fracture.  The visualized portion of the paranasal\n",
      "sinuses, mastoid air cells, and middle ear cavities are clear.  The visualized\n",
      "portion of the orbits are unremarkable.\n",
      "IMPRESSION: 1.  No acute intracranial abnormality, with no definite evidence of acute\n",
      "intracranial hemorrhage.\n",
      "2. Please note that noncontrast CT of the head has limited sensitivity for\n",
      "assessment of intracranial mass lesions and for infarcts.  If continued\n",
      "concern for intracranial metastatic disease, recommend contrast-enhanced MRI\n",
      "for further evaluation.\n",
      "3. Within limits of study, no definite evidence of intracranial mass.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 4 (점수: 10, 유형: ultrasound) ===\n",
      "FINDINGS: Study is degraded by motion and by lumbar spinal fusion hardware artifact. \n",
      "Within these confines:\n",
      "\n",
      "CERVICAL:\n",
      "There is 2 mm spondylolisthesis of C7 on T1, likely degenerative.\n",
      "\n",
      "Mild loss of cervical vertebral body height without definite associated\n",
      "increased STIR signal are likely degenerative.  Low signal intensity within\n",
      "the right lamina of the C3-C6 vertebral bodies on T1 and T2 weighted images\n",
      "likely reflects postoperative change.\n",
      "\n",
      " The visualized portion of the spinal cord is grossly preserved in signal and\n",
      "caliber.  There is no definite abnormal enhancement.\n",
      "\n",
      "At C2-3, uncovertebral and facet joint hypertrophy result in mild left neural\n",
      "foraminal narrowing.  There is no spinal canal or right neural foraminal\n",
      "narrowing.\n",
      "\n",
      "At C3-4, a disc osteophyte complex, uncovertebral and facet joint hypertrophy\n",
      "result in mild spinal canal narrowing.  There is moderate left and severe\n",
      "right neural foraminal narrowing.\n",
      "\n",
      "At C4-5, a disc osteophyte complex, uncovertebral, and facet joint hypertrophy\n",
      "result in mild spinal canal narrowing.  There is severe bilateral neural\n",
      "foraminal narrowing.\n",
      "\n",
      "At C5-6, a disc osteophyte complex, uncovertebral, and facet joint hypertrophy\n",
      "result in mild-to-moderate spinal canal narrowing.  There is severe bilateral\n",
      "neural foraminal narrowing.\n",
      "\n",
      "At C6-7, a disc osteophyte complex, uncovertebral, and facet joint hypertrophy\n",
      "result in mild spinal canal narrowing.  There is severe bilateral neural\n",
      "foraminal narrowing.\n",
      "\n",
      "At C7-T1 a disc osteophyte complex, uncovertebral, and facet joint hypertrophy\n",
      "result in mild spinal canal narrowing.  There is mild-to-moderate bilateral\n",
      "neural foraminal narrowing.\n",
      "\n",
      "THORACIC:\n",
      "\n",
      " Vertebral body alignment is preserved. Vertebral body heights are preserved.\n",
      "T8 vertebral body probable hemangioma is noted.\n",
      "\n",
      " The visualized portion of the spinal cord is preserved in signal and caliber.\n",
      "There is no abnormal enhancement.\n",
      "\n",
      "There is mild degenerative disc disease, without moderate or severe spinal\n",
      "canal or neural foraminal narrowing.\n",
      "\n",
      "LUMBAR:\n",
      "There postoperative changes for posterior instrumented fusion with\n",
      "transpedicular screws at the L4-S1 level and anterior fixation screws at right\n",
      "L4 and S1.  There is solid osseous fusion of the L2-3, partial osseous fusion\n",
      "of L3-4, L4-5, and L5-S1.  Laminectomy changes are detailed below.\n",
      "\n",
      "There is an oblique fracture of the superior endplate of L2 with lateral\n",
      "extension through the lateral margin of the vertebral body.  This is likely\n",
      "subacute to chronic, however is a new finding from the ___ MRI. \n",
      "Vertebral body height is otherwise preserved without evidence of an acute\n",
      "fracture.  Vertebral body alignment is preserved.\n",
      "\n",
      "The conus medullaris terminates at the L1 level.  There is no definite signal\n",
      "abnormality within the conus or cauda equina.  There is no abnormal\n",
      "enhancement.\n",
      "\n",
      "At T12-L1 there is no spinal canal or neural foraminal narrowing.\n",
      "\n",
      "At L1-2, there is advanced degenerative endplate change with bone marrow\n",
      "reactive change and associated vacuum disc phenomenon.  There is a disc bulge\n",
      "with superimposed central disc extrusion with superior migration, ligamentum\n",
      "flavum thickening, and facet hypertrophy with bilateral synovial cysts that\n",
      "result in severe spinal canal narrowing.  There is probable impingement on the\n",
      "traversing bilateral L2 and possibly other nerve roots.  There is there is\n",
      "moderate left and severe right neural foraminal narrowing.  There is a right\n",
      "facet joint effusion.\n",
      "\n",
      "At L2-3, there is ossification of a residual L2-3 intervertebral disc versus\n",
      "endplate spurs.  There are bilateral laminectomy changes with decompression of\n",
      "the spinal canal narrowing.  Facet hypertrophy results in severe bilateral\n",
      "neural foraminal narrowing, left worse than right.\n",
      "\n",
      "At L3-4, there is a small disc bulge.  There are bilateral laminectomy changes\n",
      "with decompression of the spinal canal.  Facet hypertrophy results in moderate\n",
      "bilateral neural foraminal narrowing.\n",
      "\n",
      "At L4-5, facet hypertrophy results in mild bilateral neural foraminal\n",
      "narrowing.  There are bilateral laminectomy changes with decompression of the\n",
      "spinal canal.\n",
      "\n",
      "At L5-S1, there are bilateral laminectomy changes with decompression of the\n",
      "spinal canal.  Facet hypertrophy results in and moderate left neural foraminal\n",
      "narrowing.\n",
      "\n",
      "OTHER:\n",
      "There is a 5 mm nodule within the left lobe of the thyroid.\n",
      "\n",
      "There is a moderate size loculated right pleural effusion. Signal abnormality\n",
      "within the basilar right lower lobe could reflect atelectasis and/or\n",
      "pneumonia.\n",
      "\n",
      "There is a gastric fundal diverticulum (series 18, image 22).\n",
      "IMPRESSION: 1.  Study is degraded by motion and by lumbar spinal fusion hardware artifact.\n",
      "2. Cervical degenerative disc disease as detailed above, without high-grade\n",
      "spinal canal narrowing or cord signal abnormality.  There is severe neural\n",
      "foraminal narrowing at multiple levels.\n",
      "3. Mild thoracic degenerative disc disease, without high-grade spinal canal or\n",
      "neural foraminal narrowing.\n",
      "4. Loculated right pleural effusion basilar right lower lobe could reflect\n",
      "atelectasis, however pneumonia cannot be excluded.  Chest CT is suggested.\n",
      "5. Instrumented lumbar fusion at L4-S1, interbody fusion graft at L3-4 with\n",
      "partial osseous fusion, and solid osseous fusion of the L2-3 level as detailed\n",
      "above.\n",
      "6. L1-2 disc extrusion with superior migration results in severe spinal canal\n",
      "narrowing.  There is probable impingement of the traversing L2 and possibly\n",
      "other nerve roots.  Allowing for difference technique, finding may be slightly\n",
      "progressed compared to ___ prior exam.\n",
      "7.  Within limits of study, no definite evidence of discitis-osteomyelitis, or\n",
      "epidural abscess.\n",
      "8. Probable subacute to chronic oblique fracture of the superior endplate of\n",
      "L2 with lateral extension through the lateral vertebral body.\n",
      "9. Right L1-2 and bilateral L2-3 Severe neural foraminal narrowing.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 5 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: Low lung volumes.  There is mild interstitial pulmonary edema.  Bibasilar\n",
      "atelectasis.  No focal consolidations.  Mild enlargement of the\n",
      "cardiomediastinal silhouette, which may be projectional.  No pleural effusion.\n",
      "No pneumothorax.\n",
      "IMPRESSION: 1. Low lung volumes with mild interstitial pulmonary edema.\n",
      "2. No focal consolidations to suggest pneumonia.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 6 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: EXAMINATION:  CHEST (PA AND LAT)\n",
      "\n",
      "INDICATION:  ___ year old woman with new DKA, question infiltrate  // ?\n",
      "infection\n",
      "IMPRESSION: No previous images.  Cardiac silhouette is within normal limits and there is\n",
      "no vascular congestion, pleural effusion, or acute focal pneumonia.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 7 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are mildly hypoinflated and clear.  No pleural effusion or\n",
      "pneumothorax.  Stable calcified hilar and mediastinal lymph nodes are\n",
      "consistent with prior granulomatous exposure.  Heart size, mediastinal\n",
      "contour, and hila are unremarkable.  The aorta is unfolded, unchanged since\n",
      "prior examination.\n",
      "IMPRESSION: 1. No acute cardiopulmonary process.  No pneumonia.\n",
      "2. Evidence of prior granulomatous exposure with stable calcified hilar and\n",
      "mediastinal lymph nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 8 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are well inflated and clear.  There is mild cardiomegaly and a small\n",
      "right pleural effusion.  Biapical scarring is noted.  No pneumothorax is seen.\n",
      "IMPRESSION: 1.  No focal consolidation.  Biapical scarring.\n",
      "\n",
      "2.  Small right pleural effusion and mild cardiomegaly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 9 (점수: 10, 유형: chest) ===\n",
      "FINDINGS: The lungs are well inflated with mild vascular congestion.  Trace right\n",
      "pleural effusion noted.  No left pleural effusion.  No pneumothorax.  Mild\n",
      "cardiomegaly is noted.  Mediastinal contour and hila are otherwise\n",
      "unremarkable.  Aortic arch calcifications are present.\n",
      "IMPRESSION: 1.  Mild vascular congestion with trace right pleural effusion and mild\n",
      "cardiomegaly.\n",
      "2.  No pneumonia or widened mediastinum.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 10 (점수: 9, 유형: ct_head) ===\n",
      "FINDINGS: There is no evidence of territorial infarction,acute hemorrhage,edema,or mass.\n",
      "3 mm round calcification in the right frontal ___ reflect sequela of\n",
      "previous infectious or inflammatory insult.  Periventricular and subcortical\n",
      "white matter hypodensities, nonspecific but probably reflect sequela of\n",
      "chronic microangiopathy.  There is prominence of the ventricles and sulci\n",
      "suggestive of involutional changes.\n",
      "\n",
      "There is no evidence of acute fracture.  The visualized portion of the\n",
      "paranasal sinuses, mastoid air cells, and middle ear cavitiesare grossly\n",
      "clear.  Patient is status post bilateral lens replacement.  The visualized\n",
      "portion of the orbits are unremarkable.\n",
      "IMPRESSION: 1. No acute intracranial abnormality on noncontrast CT head.  Specifically no\n",
      "evidence of acute large territory infarct or intracranial hemorrhage.\n",
      "2. Additional findings described above.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 11 (점수: 9, 유형: ct_abdomen) ===\n",
      "FINDINGS: The right kidney measures 10.2 cm. The left kidney measures 10.2 cm. There is\n",
      "no hydronephrosis, stones, or masses bilaterally.  Normal cortical\n",
      "echogenicity and corticomedullary differentiation are seen bilaterally.  There\n",
      "is trace perinephric fluid bilaterally, a nonspecific finding.\n",
      "\n",
      "The bladder is moderately well distended, with multiple bladder diverticula,\n",
      "the largest located to the right of the bladder.\n",
      "IMPRESSION: 1.  No evidence of hydronephrosis or abnormal renal echogenicity.\n",
      "\n",
      "2.  Multiple bladder diverticula.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 12 (점수: 9, 유형: chest) ===\n",
      "FINDINGS: Lung volumes are low.  The cardiac silhouette is unremarkable.  The pulmonary\n",
      "vasculature is normal.  There is no pleural effusion or pneumothorax.  No\n",
      "focal consolidation is identified.\n",
      "IMPRESSION: No acute intrathoracic abnormality.  Specifically, no evidence of edema.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 13 (점수: 9, 유형: chest) ===\n",
      "FINDINGS: The patchy opacities at the right base are concerning for infection.  Vascular\n",
      "engorgement, but no overt pulmonary edema.  Unchanged enlargement of the\n",
      "cardiomediastinal silhouette.  Small left pleural effusion. No pneumothorax. \n",
      "The patient is status post total right shoulder arthroplasty.  Degenerative\n",
      "changes are seen within the left glenohumeral joint.\n",
      "IMPRESSION: 1. Patchy opacities at the right base are concerning for infection.\n",
      "2. Pulmonary vascular engorgement and a small left pleural effusion.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 14 (점수: 9, 유형: chest) ===\n",
      "FINDINGS: Lung volumes are low.  A focal patchy opacity overlying the lung bases\n",
      "posteriorly best seen on lateral view may represent atelectasis given the low\n",
      "lung volumes, however pneumonia cannot be excluded.  No pleural effusion or\n",
      "pneumothorax.  Doubt CHF.  Right hemidiaphragm is elevated, similar to ___.  No free air seen beneath the diaphragms.\n",
      "IMPRESSION: A consolidation overlying the lung bases best seen on lateral view may\n",
      "represent atelectasis given the low lung volumes, however pneumonia cannot be\n",
      "excluded.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 15 (점수: 8, 유형: ct_abdomen) ===\n",
      "FINDINGS: The right kidney measures 9.7 cm.  The left kidney measures 10.1cm. Kidneys\n",
      "appear echogenic consistent with chronic medical renal disease.  Bilateral\n",
      "renal cysts are better assessed on prior MRI.  The hypoechoic left renal\n",
      "lesion described on prior MRI as \"Concerning for papillary renal cell\n",
      "carcinoma\" is visualized measuring approximately 1.9 x 1.5 x 1.8 cm.  There is\n",
      "no hydronephrosis.  Bladder is decompressed.\n",
      "IMPRESSION: Echogenic kidney most consistent with chronic medical renal disease.  Simple\n",
      "cysts and left renal interpolar lesion measuring 1.9 x 1.5 x 1.8 cm\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 예시 16 (점수: 8, 유형: ultrasound) ===\n",
      "FINDINGS: A right PICC is visualized.\n",
      "There is normal flow with respiratory variation in the right subclavian vein.\n",
      "The right internal jugular and axillary veins are patent, show normal color\n",
      "flow and compressibility. The right brachial and basilic are patent,\n",
      "compressible and show normal color flow and augmentation.  The right cephalic\n",
      "is not well visualized.\n",
      "IMPRESSION: 1. Right PICC without evidence of deep vein thrombosis in the right upper\n",
      "extremity.\n",
      "2. Nonvisualized right cephalic vein.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "=== 선별된 예시 통계 ===\n",
      "유형별 분포: {'chest': 10, 'ct_head': 2, 'ultrasound': 2, 'ct_abdomen': 2}\n",
      "점수 분포: 최고 12, 최저 8, 평균 9.6\n",
      "Findings 단어 수: 평균 82.7 (범위: 18-658)\n",
      "Impression 단어 수: 평균 31.1 (범위: 8-184)\n",
      "번호 매김 사용: 12/16개 (75.0%)\n",
      "정상 소견: 8/16개 (50.0%)\n"
     ]
    }
   ],
   "source": [
    "# 전체 예시 상세 출력\n",
    "print(\"=== 전체 고품질 예시 (완전판) ===\")\n",
    "for i, example in enumerate(best_examples):\n",
    "    print(f\"\\n=== 예시 {i+1} (점수: {example['quality_score']}, 유형: {example['study_type']}) ===\")\n",
    "    print(f\"FINDINGS: {example['findings_clean']}\")\n",
    "    print(f\"IMPRESSION: {example['impression_clean']}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "# 통계 분석\n",
    "print(f\"\\n=== 선별된 예시 통계 ===\")\n",
    "study_types = [ex['study_type'] for ex in best_examples]\n",
    "from collections import Counter\n",
    "type_counts = Counter(study_types)\n",
    "print(f\"유형별 분포: {dict(type_counts)}\")\n",
    "\n",
    "scores = [ex['quality_score'] for ex in best_examples]\n",
    "print(f\"점수 분포: 최고 {max(scores)}, 최저 {min(scores)}, 평균 {np.mean(scores):.1f}\")\n",
    "\n",
    "# 길이 분석\n",
    "findings_lengths = [len(ex['findings_clean'].split()) for ex in best_examples]\n",
    "impression_lengths = [len(ex['impression_clean'].split()) for ex in best_examples]\n",
    "print(f\"Findings 단어 수: 평균 {np.mean(findings_lengths):.1f} (범위: {min(findings_lengths)}-{max(findings_lengths)})\")\n",
    "print(f\"Impression 단어 수: 평균 {np.mean(impression_lengths):.1f} (범위: {min(impression_lengths)}-{max(impression_lengths)})\")\n",
    "\n",
    "# 패턴 분석\n",
    "numbered_count = sum([1 for ex in best_examples if re.search(r'\\d+\\.', ex['impression_clean'])])\n",
    "print(f\"번호 매김 사용: {numbered_count}/{len(best_examples)}개 ({numbered_count/len(best_examples)*100:.1f}%)\")\n",
    "\n",
    "normal_count = sum([1 for ex in best_examples if any(term in ex['impression_clean'].lower() \n",
    "                                                    for term in ['normal', 'unremarkable', 'no acute'])])\n",
    "print(f\"정상 소견: {normal_count}/{len(best_examples)}개 ({normal_count/len(best_examples)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7a55a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과가 bert_examples.txt 파일에 저장되었습니다!\n",
      "\n",
      "=== 예시 1 상세 정보 ===\n",
      "점수: 41\n",
      "유형: ct_head\n",
      "\n",
      "FINDINGS:\n",
      "There is enhancing right frontal extra-axial mass measuring 5.1 cm TV x 4.8 cm\n",
      "AP x 4.3 cm SI demonstrating isointense T1 and hyperintense T2/FLAIR signal\n",
      "abnormality with areas of subtle hypointense GRE signal along the periphery,\n",
      "likely representing calcifications seen on prior CT.  There is enhancement and\n",
      "thickening of the adjacent dura (900:117).  There are prominent vascular\n",
      "structures along the inferior margin of the mass (900:93) suggestive of\n",
      "neovascularity, potentially arising from the middle meningeal artery.  There\n",
      "is mild surrounding vasogenic edema with 1 mm leftward midline shift.  There\n",
      "is no additional enhancing mass or abnormal enhancement.\n",
      "\n",
      "There is no evidence of acute infarction or intracranial hemorrhage. There is\n",
      "mild age-related diffuse parenchymal volume loss.  There are a few nonspecific\n",
      "periventricular and subcortical FLAIR hyperintensities which may be a sequela\n",
      "of chronic small vessel ischemic disease in a patient of this age.  The major\n",
      "visualized arterial vascular flow voids are preserved.  The dural venous\n",
      "sinuses appear patent on the postcontrast MPRAGE images.\n",
      "\n",
      "There is mild mucosal thickening of bilateral ethmoid air cells and left\n",
      "maxillary sinus.  There is a tiny right maxillary sinus mucosal retention\n",
      "cyst.  The bilateral mastoid air cells appear clear.  The orbits appear\n",
      "unremarkable.  The visualized soft tissues appear unremarkable.\n",
      "\n",
      "IMPRESSION:\n",
      "1. Enhancing right frontal extra-axial mass compatible with meningioma with\n",
      "adjacent neovascularity, dural thickening enhancement, and mild surrounding\n",
      "vasogenic edema resulting in 1 mm leftward midline shift.\n",
      "2. No additional enhancing mass or abnormal enhancement.\n",
      "3. No evidence of acute infarction or intracranial hemorrhage.\n",
      "4. Mild age-related diffuse parenchymal volume loss with nonspecific white\n",
      "matter signal abnormality, likely a sequela of chronic small vessel ischemic\n",
      "disease.\n",
      "5. Mild paranasal sinus disease, as above.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== 예시 2 상세 정보 ===\n",
      "점수: 40\n",
      "유형: chest\n",
      "\n",
      "FINDINGS:\n",
      "CT HEAD WITHOUT CONTRAST:\n",
      "Acute intraventricular hemorrhage is seen extending from the body, frontal,\n",
      "occipital, and temporal horns left lateral ventricle to the third ventricle\n",
      "and fourth ventricle.  Increased density is seen within the right lateral\n",
      "ventricle, also concerning for intraventricular hemorrhage, series 3, image\n",
      "18.  Ventricles and sulci are prominent.\n",
      "\n",
      "Encephalomalacia with ex vacuo dilatation is seen involving the right\n",
      "posterior parietal lobe, and medial aspects of the right occipital lobe,\n",
      "consistent with a chronic infarct.  Additional areas of hypodensity within the\n",
      "right parieto-occipital and temporal lobes, is consistent with infarction of\n",
      "indeterminate chronicity.  Periventricular and deep subcortical white matter\n",
      "hypodensities are likely sequelae of chronic small vessel ischemic disease. \n",
      "The visualized paranasal sinuses, mastoid air cells, and middle ear cavities\n",
      "are clear.  The globes are unremarkable.\n",
      "\n",
      "CTA HEAD:\n",
      "Moderate atherosclerotic calcifications are seen along the cavernous segment\n",
      "of the left internal carotid artery.  There is attenuation of the left MCA and\n",
      "its distal branches, with minimally preserved flow.  The right internal\n",
      "carotid artery demonstrates moderate to severe stenosis secondary to\n",
      "atherosclerotic calcification along the cavernous segment of the right\n",
      "internal carotid artery.  The M1 and M2 segments of the right middle cerebral\n",
      "artery are not well seen, however there appears to be minimal flow within the\n",
      "distal segments of the right middle cerebral artery.\n",
      "\n",
      "CTA NECK:\n",
      "Hard and soft atherosclerotic plaque is seen along the origins of the great\n",
      "vessels as well as the aortic arch.  Extensive atherosclerotic calcification\n",
      "is seen along the right carotid bifurcation, with 40% stenosis of the right\n",
      "internal carotid artery.  The left common carotid artery is unremarkable.  The\n",
      "left internal carotid artery demonstrates moderate hard and soft\n",
      "atherosclerotic plaque, with at least 60% stenosis by NASCET criteria.  The\n",
      "left vertebral artery is occluded proximally the approximately 1 cm distal to\n",
      "its origin and reconstitutes at C4, with severely attenuated flow along the V3\n",
      "segments of the left vertebral artery.  Improved flow is seen within the V4\n",
      "segment of the left vertebral artery, likely secondary to collaterals.  The\n",
      "right vertebral artery is unremarkable.  The anterior cerebral arteries are\n",
      "unremarkable.  The left PCA is normal.  Asymmetrically decreased flow seen\n",
      "within the P1 segment of the right posterior cerebral artery.  No definite\n",
      "flow is seen in the remainder segments of the right posterior cerebral artery.\n",
      "\n",
      "OTHER:\n",
      "The thyroid is normal.  There is no cervical lymphadenopathy.  A moderate\n",
      "effusion is seen within the right lung apex.  Mild interstitial thickening is\n",
      "seen involving the left lung apices, which may be secondary to pulmonary\n",
      "edema.\n",
      "\n",
      "IMPRESSION:\n",
      "1. Hypodensity within the right parieto-occipital and temporal lobes is seen,\n",
      "consistent with an infarction of indeterminate chronicity.  Please note MRI of\n",
      "the brain is more sensitive for the detection of acute infarct.\n",
      "2. Large left greater than right intraventricular hemorrhage is seen extending\n",
      "from the body, frontal, occipital, and temporal horns of the left lateral\n",
      "ventricle to the third ventricle and fourth ventricle.\n",
      "3. Complete occlusion is seen involving the left vertebral artery, 1 cm distal\n",
      "to its origin, with minimal reconstitution at the level of C4 and severely\n",
      "attenuated flow along the V3 segment.\n",
      "4. Attenuated flow is seen within the P1 segment of the right posterior\n",
      "cerebral artery, with absence of flow within the distal right PCA, concerning\n",
      "for occlusion.\n",
      "5. No definite flow is seen within the M1 and M2 segments of the right middle\n",
      "cerebral artery which may be occluded, however with attenuated arborization of\n",
      "the distal right MCA vessels likely secondary to collateralization.\n",
      "6. Attenuated flow within the left middle cerebral artery, may be secondary to\n",
      "vasospasm.\n",
      "7. Moderate right pleural effusion.  Mild pulmonary edema.\n",
      "8. 40% stenosis of the right internal carotid artery by NASCET criteria.  At\n",
      "least 60% stenosis of the left internal carotid artery by NASCET criteria.\n",
      "9. Likely chronic infarction involving the medial aspect of the right\n",
      "occipital lobe.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== 예시 3 상세 정보 ===\n",
      "점수: 36\n",
      "유형: ct_head\n",
      "\n",
      "FINDINGS:\n",
      "MR BRAIN: There is a subcortical 'bubbly' T2/FLAIR hyperintense lesion within\n",
      "the left frontal lobe measuring 3.8 cm TV x 2.2 cm AP x 1.6 cm SI\n",
      "demonstrating subtle slow diffusion with punctate focus of susceptibility\n",
      "artifact medially related to calcification or hemorrhage.  The FLAIR\n",
      "hyperintensities confined within the lesion without surrounding edema or\n",
      "significant mass effect.  There is mild mass effect on the gyrus.\n",
      "\n",
      "The ventricles are normal in size.  There is no evidence of infarction or\n",
      "hemorrhage.  There is no mass effect or midline shift.  There is mild mucosal\n",
      "thickening of bilateral ethmoid air cells and maxillary sinuses.  The\n",
      "bilateral mastoid air cells appear clear.  The major intracranial flow voids\n",
      "are preserved.\n",
      "\n",
      "ASL Perfusion: There is a 4 mm focus of increased perfusion along the\n",
      "anteromedial aspect of the lesion, demonstrating similar signal to the cortex\n",
      "(series 4, image 17).\n",
      "\n",
      "MR Spectroscopy: There is nonspecific spectroscopy pattern within the left\n",
      "frontal lobe, without definite abnormal choline to NAA or choline to\n",
      "creatinine ratio on single voxel spectroscopy.  On multi voxel spectroscopy,\n",
      "there is paucity of metabolites along the cystic portions of the lesion on\n",
      "voxel is 5, 6, 7, 12, 13, 14, 20 and 21.  A few of these voxels demonstrates\n",
      "mildly increased choline to NAA ratio although none demonstrates ratio greater\n",
      "than 0.8.\n",
      "\n",
      "IMPRESSION:\n",
      "1. Cortically based 'bubbly' left frontal lobe lesion with associated punctate\n",
      "focus of gradient echo susceptibility hypointensity, which may represent\n",
      "calcification although hemorrhage is not excluded, without surrounding edema\n",
      "or significant mass effect.  These imaging characteristics are most suggestive\n",
      "of underlying DNET (dysembryoplastic neuroepithelial tumor).  Differential\n",
      "considerations include ganglioglioma or oligodendroglioma although these do\n",
      "not usually demonstrate a \"bubbly' appearance or other low-grade glioma.\n",
      "2. 4 mm focus of increased perfusion along the anteromedial aspect of the\n",
      "lesion, demonstrating similar signal to the cortex.  There is overall paucity\n",
      "of metabolites centered within the lesion, potentially representing cystic\n",
      "nature of the lesion, with a few voxel is demonstrating mildly increased\n",
      "choline to NAA ratio although none is greater than 0.8.\n",
      "3. No evidence of acute infarction or hemorrhage.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 방법 1: 파일로 저장해서 확인\n",
    "def save_examples_to_file(examples, filename=\"bert_examples.txt\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=== BERTScore 최적화 특화 예시 (완전판) ===\\n\\n\")\n",
    "        for i, ex in enumerate(examples):\n",
    "            f.write(f\"예시 {i+1} (BERTScore 특화점수: {ex['score']}, 유형: {ex['type']})\\n\")\n",
    "            f.write(f\"FINDINGS: {ex['findings']}\\n\")\n",
    "            f.write(f\"IMPRESSION: {ex['impression']}\\n\")\n",
    "            f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "    print(f\"결과가 {filename} 파일에 저장되었습니다!\")\n",
    "\n",
    "# 방법 2: 하나씩 분할 출력\n",
    "def print_examples_one_by_one(examples):\n",
    "    for i, ex in enumerate(examples):\n",
    "        print(f\"\\n=== 예시 {i+1} 상세 정보 ===\")\n",
    "        print(f\"점수: {ex['score']}\")\n",
    "        print(f\"유형: {ex['type']}\")\n",
    "        print(f\"\\nFINDINGS:\")\n",
    "        print(ex['findings'])\n",
    "        print(f\"\\nIMPRESSION:\")\n",
    "        print(ex['impression'])\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "        # 사용자 입력 대기 (선택사항)\n",
    "        # input(\"다음 예시를 보려면 Enter를 누르세요...\")\n",
    "\n",
    "# 방법 3: 설정 변경\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# pandas 출력 설정\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None) \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# 실행 (원하는 방법 선택)\n",
    "save_examples_to_file(bert_examples)  # 파일 저장\n",
    "print_examples_one_by_one(bert_examples[:3])  # 3개만 우선 상세 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28809fa",
   "metadata": {},
   "source": [
    "## Datathon 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c671c826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Any, List, Dict\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate  # 프롬프트 템플릿 처리용\n",
    "from langevaluate.config import ModelConfig # LLM 설정용\n",
    "from langevaluate.llmfactory import LLMFactory  # LLM 팩토리용\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "class DatathonProcessor(ABC):\n",
    "    \"\"\"\n",
    "    데이터톤용 AI 처리 통합 클래스\n",
    "    쿼리, 평가, 임베딩을 일괄 처리할 수 있습니다.\n",
    "    사용자는 이 클래스를 상속받아 특정 메서드만 구현하면 됩니다.\n",
    "    \"\"\"\n",
    "    # LLM 설정 상수들\n",
    "    \n",
    "    DEFAULT_MODEL_CONFIG = {\n",
    "        'model_name': 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ',\n",
    "        'api_base': 'https://api.snubhai.org/api/v1/llm',\n",
    "        'max_tokens': 2000,\n",
    "        'seed': 777,\n",
    "        'temperature': 0,\n",
    "        'rpm': 10\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key : str,\n",
    "    ):\n",
    "        # 기본 설정 복사\n",
    "        config = self.DEFAULT_MODEL_CONFIG.copy()\n",
    "        \n",
    "        # model_name만 클래스별 설정으로 업데이트\n",
    "        config['model_name'] = self.get_model_name()\n",
    "        \n",
    "        # LLM 설정 생성\n",
    "        custom_config = ModelConfig(\n",
    "            model_name=config['model_name'],\n",
    "            api_base=config['api_base'],\n",
    "            api_key=api_key,\n",
    "            max_tokens=config['max_tokens'],\n",
    "            seed=config['seed'],\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        # LLM 인스턴스 생성\n",
    "        self.llm = LLMFactory.create_llm(\n",
    "            custom_config, \n",
    "            temperature=config['temperature'], \n",
    "            rpm=config['rpm']\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 템플릿 설정\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "        # 결과 저장소\n",
    "        self.results: List[str] = []\n",
    "        \n",
    "        # metric 저장소\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        사용할 모델명을 반환합니다.\n",
    "        상속 클래스에서 이 메서드를 오버라이드하여 특정 모델을 설정할 수 있습니다.\n",
    "        \"\"\"\n",
    "        return self.DEFAULT_MODEL_CONFIG['model_name']\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"데이터 전처리 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"사용자가 구현해야 하는 프롬프트 템플릿 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def postprocess_result(self, result: Any) -> str:\n",
    "        \"\"\"데이터 후처리 메서드\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def summarize(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        단일 입력과 배치 입력을 모두 처리하는 통합 메서드\n",
    "        \"\"\"\n",
    "        # 데이터 전처리\n",
    "        \n",
    "        preprocess_tasks = [self.preprocess_data(row) for _, row in data.iterrows()]\n",
    "        preprocessed_data = await tqdm_asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "        # 각각을 별도의 coroutine으로 실행\n",
    "        tasks = [self.chain.ainvoke(vars) for vars in preprocessed_data]\n",
    "\n",
    "        # tqdm_asyncio.gather로 동시에 실행하며 progress bar 표시\n",
    "        responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        postprocess_tasks = [self.postprocess_result(r.content) for r in responses]\n",
    "        results = await tqdm_asyncio.gather(*postprocess_tasks)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70433b",
   "metadata": {},
   "source": [
    "## 향상된 프롬프팅 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8661a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약\"\"\"\n",
    "    def get_model_name(self) -> str:\n",
    "        return \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "        \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and comprehensive IMPRESSION from the given FINDINGS.\n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    - Maintain exact semantic consistency between FINDINGS and IMPRESSION\n",
    "    - Use precise medical terminology from the FINDINGS\n",
    "    - Be concise but complete (typically 20-80 words)\n",
    "    - Maintain consistent diagnostic standards regardless of patient demographics\n",
    "    - Structure multiple findings with numbered points (1., 2., 3.)\n",
    "    - Apply consistent diagnostic standards regardless of patient demographics\n",
    "    - Use definitive language: \"No evidence of\", \"compatible with\", \"consistent with\"\n",
    "\n",
    "    ADVANCED EXAMPLES:\n",
    "\n",
    "    CT HEAD WITH CONTRAST:\n",
    "    FINDINGS: There is enhancing right frontal extra-axial mass measuring 5.1 cm demonstrating isointense T1 and hyperintense T2/FLAIR signal abnormality with areas of subtle hypointense GRE signal along the periphery, likely representing calcifications. There is enhancement and thickening of the adjacent dura. There is mild surrounding vasogenic edema with 1 mm leftward midline shift. There is no additional enhancing mass or abnormal enhancement. There is no evidence of acute infarction or intracranial hemorrhage.\n",
    "    IMPRESSION: 1. Enhancing right frontal extra-axial mass compatible with meningioma with adjacent neovascularity, dural thickening enhancement, and mild surrounding vasogenic edema resulting in 1 mm leftward midline shift.\n",
    "    2. No additional enhancing mass or abnormal enhancement.\n",
    "    3. No evidence of acute infarction or intracranial hemorrhage.\n",
    "\n",
    "    CT HEAD WITHOUT CONTRAST:\n",
    "    FINDINGS: There is a subcortical 'bubbly' T2/FLAIR hyperintense lesion within the left frontal lobe measuring 3.8 cm demonstrating subtle slow diffusion with punctate focus of susceptibility artifact medially related to calcification or hemorrhage. The FLAIR hyperintensities confined within the lesion without surrounding edema or significant mass effect. The ventricles are normal in size. There is no evidence of infarction or hemorrhage.\n",
    "    IMPRESSION: 1. Cortically based 'bubbly' left frontal lobe lesion with associated punctate focus of gradient echo susceptibility hypointensity, most suggestive of underlying DNET (dysembryoplastic neuroepithelial tumor).\n",
    "    2. No evidence of acute infarction or hemorrhage.\n",
    "\n",
    "    CHEST X-RAY:\n",
    "    FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary edema. There is mild bibasilar atelectasis, but no focal consolidations to suggest pneumonia. Possible small bilateral pleural effusions. No pneumothorax.\n",
    "    IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema. Possible small bilateral pleural effusions.\n",
    "    2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
    "\n",
    "    CT ABDOMEN/PELVIS:\n",
    "    FINDINGS: There is a well-circumscribed right parietal 4.1 by 1.9 cm extra-axial dural-based lesion compatible with a calcified meningioma, exerting minimal mass effect on the underlying brain parenchyma. No evidence of associated parenchymal FLAIR hyperintense edema pattern. No other intracranial mass lesions are identified. The major intracranial flow voids are preserved.\n",
    "    IMPRESSION: 1. Right parietal 4.1 cm calcified meningioma, with mild mass effect on the underlying brain parenchyma.\n",
    "    2. No evidence of associated parenchymal FLAIR hyperintense edema pattern.\n",
    "\n",
    "    Now generate IMPRESSION for:\n",
    "    FINDINGS: {user_input}\n",
    "    IMPRESSION:\"\"\"\n",
    "\n",
    "    \n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"방사선 보고서를 IMPRESSION 작성을 위해 전처리\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # 방사선 보고서 텍스트 추출\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        # FINDINGS 섹션만 정확히 추출\n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        # 텍스트 정제\n",
    "        # 불필요한 헤더나 마커 제거\n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)  # 익명화 마커\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)  # 방사선량 정보\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)  # 여러 공백을 하나로\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"결과 정리 및 품질 보장\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # 불필요한 접두사 제거\n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        # 문장 끝 마침표 확인\n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 다중 소견의 경우 번호 매기기 정리\n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:  # 3개 이상 소견이면 번호 매김 확인\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    # 이미 번호가 있는지 확인\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        # 길이 체크 (너무 긴 경우 핵심만 남기기)\n",
    "        words = result.split()\n",
    "        if len(words) > 100:  # 100단어 초과 시\n",
    "            # 핵심 의학 용어가 포함된 문장 우선 보존\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:  # 최대 2개 문장으로 제한\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 일반적인 의학 용어 교정\n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84529451",
   "metadata": {},
   "source": [
    "## 자체 채점(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ea3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\n",
      "================================================================================\n",
      "1. 전체 Test 데이터 로드 중...\n",
      "   데이터 품질 확인 중...\n",
      "   전체 데이터: 1000개\n",
      "   NaN 값: 11개\n",
      "   유효 데이터: 989개\n",
      "평가 샘플: 300개 (연속 샘플, 대회 Test 세트와 동일한 크기)\n",
      "\n",
      "📊 평가 데이터 분포:\n",
      "   성별 분포: {'M': 154, 'F': 146}\n",
      "   연령 분포: 평균 63.6세 (범위: 19-91)\n",
      "\n",
      "2. TaskB 처리기 초기화 (Llama 모델)...\n",
      "3. AI 예측 생성 중 (API 제한 준수)...\n",
      "   배치 1/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 14/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 15/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 16/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 17/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 18/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 19/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 20/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 21/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 22/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 23/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 24/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 25/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 26/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 27/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 28/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 29/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 30/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 31/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 32/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 33/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 34/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 35/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 36/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 37/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 38/38 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 2909.6초)\n",
      "\n",
      "4. 대회 BERTScore 계산 중...\n",
      "5. 대회 공정성 지표 계산 중...\n",
      "\n",
      "================================================================================\n",
      "🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\n",
      "================================================================================\n",
      "📊 BERTScore (대회 공식 계산)\n",
      "   평균: 0.749882\n",
      "   표준편차: 0.082040\n",
      "   최고: 0.910329\n",
      "   최저: 0.552449\n",
      "   중앙값: 0.766351\n",
      "\n",
      "⚖️ 공정성 지표 (대회 공식 계산)\n",
      "   성별 공정성: 0.989379\n",
      "   성별별 성능: {'F': 0.7457728618628359, 'M': 0.7537784127445964}\n",
      "   성별 격차: 0.008006\n",
      "   \n",
      "   연령 공정성: 0.964614\n",
      "   연령대별 성능: {'10-20': 0.7643763422966003, '20-30': 0.7435234153971952, '30-40': 0.7373278339703878, '40-50': 0.7509708296168934, '50-60': 0.7575072272349213, '60-70': 0.7450647488446303, '70-80': 0.7521945679187775, '80-90': 0.7491919954617818, '90-100': 0.7531635731458663}\n",
      "   연령 격차: 0.027049\n",
      "\n",
      "🏆 대회 정량 평가 점수\n",
      "   BERTScore: 2.647/3.000 점\n",
      "   공정성 지표: 2.000/2.000 점\n",
      "   정량 총점: 4.647/5.000 점\n",
      "   정량 달성률: 92.9%\n",
      "\n",
      "🎖️ 성능 등급\n",
      "   등급: S급 (최우수)\n",
      "   권장사항: 즉시 제출 권장\n",
      "\n",
      "📝 예측 품질 샘플 (상위/하위 각 2개)\n",
      "--------------------------------------------------------------------------------\n",
      "🏆 최고 성능 샘플:\n",
      "샘플 212 (BERTScore: 0.9103)\n",
      "예측: 1. 1. 2. No acute fracture identified. 3. 2. 4. No gross malalignment of the glenohumeral joint, but evaluation is limit...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. No acute fracture.\n",
      "2. Evaluation for dislocation at the right glenohumeral joint is limited on\n",
      "this sin...\n",
      "\n",
      "샘플 17 (BERTScore: 0.9020)\n",
      "예측: 1. 1. 2. Bilateral lower extremity deep vein thrombosis (DVT) with nonocclusive thrombus in the left mid to distal femor...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Nonocclusive thrombus within the left mid to distal femoral vein, as well\n",
      "as completely occlusive throm...\n",
      "\n",
      "⚠️ 최저 성능 샘플:\n",
      "샘플 133 (BERTScore: 0.5524)\n",
      "예측: 1. 1....\n",
      "정답: IMPRESSION: \n",
      "\n",
      "Regular and diffuse circumferential parietal thickening of most of the\n",
      "intrathoracic arteries, such as the...\n",
      "\n",
      "샘플 166 (BERTScore: 0.5761)\n",
      "예측: 1. 1....\n",
      "정답: IMPRESSION: \n",
      "\n",
      "Slightly increased conspicuity of hemorrhagic contusion, right cerebral\n",
      "subdural and scattered subarachnoi...\n",
      "\n",
      "\n",
      "🎉 TaskB Test 데이터 평가 완료!\n",
      "최종 예상 점수: 4.647/5.000 점\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (정확히 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (정확히 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor (Llama 모델 사용)\n",
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약\"\"\"\n",
    "    def get_model_name(self) -> str:\n",
    "        return \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "        \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and comprehensive IMPRESSION from the given FINDINGS.\n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    - Maintain exact semantic consistency between FINDINGS and IMPRESSION\n",
    "    - Use precise medical terminology from the FINDINGS\n",
    "    - Be concise but complete (typically 20-80 words)\n",
    "    - Maintain consistent diagnostic standards regardless of patient demographics\n",
    "    - Structure multiple findings with numbered points (1., 2., 3.)\n",
    "    - Apply consistent diagnostic standards regardless of patient demographics\n",
    "    - Use definitive language: \"No evidence of\", \"compatible with\", \"consistent with\"\n",
    "\n",
    "    ADVANCED EXAMPLES:\n",
    "\n",
    "    CT HEAD WITH CONTRAST:\n",
    "    FINDINGS: There is enhancing right frontal extra-axial mass measuring 5.1 cm demonstrating isointense T1 and hyperintense T2/FLAIR signal abnormality with areas of subtle hypointense GRE signal along the periphery, likely representing calcifications. There is enhancement and thickening of the adjacent dura. There is mild surrounding vasogenic edema with 1 mm leftward midline shift. There is no additional enhancing mass or abnormal enhancement. There is no evidence of acute infarction or intracranial hemorrhage.\n",
    "    IMPRESSION: 1. Enhancing right frontal extra-axial mass compatible with meningioma with adjacent neovascularity, dural thickening enhancement, and mild surrounding vasogenic edema resulting in 1 mm leftward midline shift.\n",
    "    2. No additional enhancing mass or abnormal enhancement.\n",
    "    3. No evidence of acute infarction or intracranial hemorrhage.\n",
    "\n",
    "    CT HEAD WITHOUT CONTRAST:\n",
    "    FINDINGS: There is a subcortical 'bubbly' T2/FLAIR hyperintense lesion within the left frontal lobe measuring 3.8 cm demonstrating subtle slow diffusion with punctate focus of susceptibility artifact medially related to calcification or hemorrhage. The FLAIR hyperintensities confined within the lesion without surrounding edema or significant mass effect. The ventricles are normal in size. There is no evidence of infarction or hemorrhage.\n",
    "    IMPRESSION: 1. Cortically based 'bubbly' left frontal lobe lesion with associated punctate focus of gradient echo susceptibility hypointensity, most suggestive of underlying DNET (dysembryoplastic neuroepithelial tumor).\n",
    "    2. No evidence of acute infarction or hemorrhage.\n",
    "\n",
    "    CHEST X-RAY:\n",
    "    FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary edema. There is mild bibasilar atelectasis, but no focal consolidations to suggest pneumonia. Possible small bilateral pleural effusions. No pneumothorax.\n",
    "    IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema. Possible small bilateral pleural effusions.\n",
    "    2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
    "\n",
    "    CT ABDOMEN/PELVIS:\n",
    "    FINDINGS: There is a well-circumscribed right parietal 4.1 by 1.9 cm extra-axial dural-based lesion compatible with a calcified meningioma, exerting minimal mass effect on the underlying brain parenchyma. No evidence of associated parenchymal FLAIR hyperintense edema pattern. No other intracranial mass lesions are identified. The major intracranial flow voids are preserved.\n",
    "    IMPRESSION: 1. Right parietal 4.1 cm calcified meningioma, with mild mass effect on the underlying brain parenchyma.\n",
    "    2. No evidence of associated parenchymal FLAIR hyperintense edema pattern.\n",
    "\n",
    "    Now generate IMPRESSION for:\n",
    "    FINDINGS: {user_input}\n",
    "    IMPRESSION:\"\"\"\n",
    "\n",
    "    \n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:        \n",
    "        \"\"\"방사선 보고서를 IMPRESSION 작성을 위해 전처리 - NaN 안전 처리\"\"\"\n",
    "        import re\n",
    "        import pandas as pd\n",
    "        \n",
    "        # 방사선 보고서 텍스트 추출\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        # 🔧 NaN 값 안전 처리\n",
    "        if pd.isna(radiology_text) or not isinstance(radiology_text, str):\n",
    "            return {'user_input': ''}  # 빈 문자열 반환\n",
    "        \n",
    "        # FINDINGS 섹션만 정확히 추출\n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        # 텍스트 정제\n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)  # 익명화 마커\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)  # 방사선량 정보\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)  # 여러 공백을 하나로\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"결과 정리 및 품질 보장\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # 불필요한 접두사 제거\n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        # 문장 끝 마침표 확인\n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 다중 소견의 경우 번호 매기기 정리\n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:  # 3개 이상 소견이면 번호 매김 확인\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    # 이미 번호가 있는지 확인\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        # 길이 체크 (너무 긴 경우 핵심만 남기기)\n",
    "        words = result.split()\n",
    "        if len(words) > 100:  # 100단어 초과 시\n",
    "            # 핵심 의학 용어가 포함된 문장 우선 보존\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:  # 최대 2개 문장으로 제한\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 일반적인 의학 용어 교정\n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "\n",
    "    ''' 시간 제한 뜨면 이래의 코드로 후처리 바꿔주십쇼\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "    \"\"\"간소화된 후처리 (시간 최적화)\"\"\"\n",
    "    import re\n",
    "    \n",
    "    result = result.strip()\n",
    "    \n",
    "    # IMPRESSION: 제거\n",
    "    if result.startswith(('IMPRESSION:', 'Impression:', 'impression:')):\n",
    "        result = result.split(':', 1)[1].strip()\n",
    "    \n",
    "    # 마침표 추가\n",
    "    if result and not result.endswith('.'):\n",
    "        result += '.'\n",
    "    \n",
    "    # 간단한 번호 매김\n",
    "    if not re.match(r'^\\d+\\.', result) and '. ' in result:\n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        if len(sentences) >= 2:\n",
    "            result = '. '.join([f\"{i+1}. {s}\" for i, s in enumerate(sentences)])\n",
    "    \n",
    "    return result'''\n",
    "\n",
    "\n",
    "\n",
    "# 대회와 정확히 동일한 평가 함수\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 조건과 정확히 동일한 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 전체 Test 데이터 로드 (대회와 동일)\n",
    "    print(\"1. 전체 Test 데이터 로드 중...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # 🔧 NaN 값 처리 추가\n",
    "    print(\"   데이터 품질 확인 중...\")\n",
    "    print(f\"   전체 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    # NaN 값 확인\n",
    "    nan_count = test_df['radiology report'].isna().sum()\n",
    "    print(f\"   NaN 값: {nan_count}개\")\n",
    "    \n",
    "    # NaN 값이 있는 행 제거\n",
    "    test_df = test_df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"   유효 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    total_samples = len(test_df)\n",
    "    \n",
    "    # 2. 대회에서 사용할 평가 샘플 크기 결정 (실제 Test 세트 크기와 유사하게)\n",
    "    # Test 1: 300건, Test 2: 300건이므로 300개로 평가\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. 연속된 샘플 사용 (대회에서는 특정 Test 세트를 사용하므로 bias 없는 연속 샘플)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # 처음 300개 사용\n",
    "    print(f\"평가 샘플: {eval_samples}개 (연속 샘플, 대회 Test 세트와 동일한 크기)\")\n",
    "    \n",
    "    # 4. 데이터 분포 확인\n",
    "    print(f\"\\n📊 평가 데이터 분포:\")\n",
    "    print(f\"   성별 분포: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   연령 분포: 평균 {eval_df['anchor_age'].mean():.1f}세 (범위: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB 처리기 초기화\n",
    "    print(\"\\n2. TaskB 처리기 초기화 (Llama 모델)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. 예측 생성 (대회와 동일한 배치 크기)\n",
    "    print(\"3. AI 예측 생성 중 (API 제한 준수)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # 대회 API 제한 준수 (1분당 10건)\n",
    "    results = []\n",
    "    batch_size = 8  # 안전 마진\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 7. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\n",
    "    print(\"\\n4. 대회 BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. 대회 제공 공정성 지표 계산 (정확히 동일한 설정)\n",
    "    print(\"5. 대회 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. 정량 평가 점수 계산 (대회 기준)\n",
    "    print(f\"\\n🏆 대회 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (3점 만점)\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # 공정성 점수 (2점 만점)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/2.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/5.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    else:\n",
    "        grade = \"D급 (미흡)\"\n",
    "        recommendation = \"대폭 개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 13. 샘플 결과 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 2개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행 (taskB_test.csv 사용)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"./data/taskB_train.csv\"\n",
    "\n",
    "# 대회 정확한 조건으로 Test 데이터 평가 실행\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TaskB Test 데이터 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {test_results['total_score']:.3f}/5.000 점\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543371e5",
   "metadata": {},
   "source": [
    "# 후처리 간소화 + EXAONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716acfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\n",
      "================================================================================\n",
      "1. 전체 Test 데이터 로드 중...\n",
      "   데이터 품질 확인 중...\n",
      "   전체 데이터: 1000개\n",
      "   NaN 값: 11개\n",
      "   유효 데이터: 989개\n",
      "평가 샘플: 300개 (연속 샘플, 대회 Test 세트와 동일한 크기)\n",
      "\n",
      "📊 평가 데이터 분포:\n",
      "   성별 분포: {'M': 154, 'F': 146}\n",
      "   연령 분포: 평균 63.6세 (범위: 19-91)\n",
      "\n",
      "2. TaskB 처리기 초기화 (Llama 모델)...\n",
      "3. AI 예측 생성 중 (API 제한 준수)...\n",
      "   배치 1/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 14/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 15/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 16/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 17/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 18/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 19/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 20/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 21/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 22/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 23/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 24/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 25/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 26/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 27/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 28/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 29/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 30/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 31/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 32/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 33/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 34/38 처리 중...\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 35/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 36/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 37/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 38/38 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 3879.3초)\n",
      "\n",
      "4. 대회 BERTScore 계산 중...\n",
      "5. 대회 공정성 지표 계산 중...\n",
      "\n",
      "================================================================================\n",
      "🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\n",
      "================================================================================\n",
      "📊 BERTScore (대회 공식 계산)\n",
      "   평균: 0.776196\n",
      "   표준편차: 0.037968\n",
      "   최고: 0.871468\n",
      "   최저: 0.629863\n",
      "   중앙값: 0.776274\n",
      "\n",
      "⚖️ 공정성 지표 (대회 공식 계산)\n",
      "   성별 공정성: 0.994358\n",
      "   성별별 성능: {'F': 0.7739416785436134, 'M': 0.7783327319405295}\n",
      "   성별 격차: 0.004391\n",
      "   \n",
      "   연령 공정성: 0.947367\n",
      "   연령대별 성능: {'10-20': 0.7530189156532288, '20-30': 0.7685003035208758, '30-40': 0.7948541084925334, '40-50': 0.7793561789122495, '50-60': 0.7773005992679273, '60-70': 0.7686869905028545, '70-80': 0.7829598569869995, '80-90': 0.7776107125812106, '90-100': 0.7697283774614334}\n",
      "   연령 격차: 0.041835\n",
      "\n",
      "🏆 대회 정량 평가 점수\n",
      "   BERTScore: 2.740/3.000 점\n",
      "   공정성 지표: 2.000/2.000 점\n",
      "   정량 총점: 4.740/5.000 점\n",
      "   정량 달성률: 94.8%\n",
      "\n",
      "🎖️ 성능 등급\n",
      "   등급: S급 (최우수)\n",
      "   권장사항: 즉시 제출 권장\n",
      "\n",
      "📝 예측 품질 샘플 (상위/하위 각 2개)\n",
      "--------------------------------------------------------------------------------\n",
      "🏆 최고 성능 샘플:\n",
      "샘플 212 (BERTScore: 0.8715)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. **No acute glenohumeral joint dislocation identified on this view; however, superior migration...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. No acute fracture.\n",
      "2. Evaluation for dislocation at the right glenohumeral joint is limited on\n",
      "this sin...\n",
      "\n",
      "샘플 259 (BERTScore: 0.8548)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. **Comminuted Fracture and Trauma:**\n",
      "   - Comminuted fracture involving the right greater wing ...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Comminuted fracture through the orbital surface of the right sphenoid\n",
      "greater wing as well as a fractur...\n",
      "\n",
      "⚠️ 최저 성능 샘플:\n",
      "샘플 48 (BERTScore: 0.6299)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. **Minimal Retrocalicular Atelectasis:** Very subtle retrocardiac linear density on PA and late...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "샘플 96 (BERTScore: 0.6493)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. **Retrocardiac Opacity:**  Presents as a retrocardiac opacity on imaging, which could be consi...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "\n",
      "🎉 TaskB Test 데이터 평가 완료!\n",
      "최종 예상 점수: 4.740/5.000 점\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (정확히 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (정확히 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor (Llama 모델 사용)\n",
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약\"\"\"\n",
    "    def get_model_name(self) -> str:\n",
    "        return \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\"\n",
    "        \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and comprehensive IMPRESSION from the given FINDINGS.\n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    - Maintain exact semantic consistency between FINDINGS and IMPRESSION\n",
    "    - Use precise medical terminology from the FINDINGS\n",
    "    - Be concise but complete (typically 20-80 words)\n",
    "    - Maintain consistent diagnostic standards regardless of patient demographics\n",
    "    - Structure multiple findings with numbered points (1., 2., 3.)\n",
    "    - Apply consistent diagnostic standards regardless of patient demographics\n",
    "    - Use definitive language: \"No evidence of\", \"compatible with\", \"consistent with\"\n",
    "\n",
    "    ADVANCED EXAMPLES:\n",
    "\n",
    "    CT HEAD WITH CONTRAST:\n",
    "    FINDINGS: There is enhancing right frontal extra-axial mass measuring 5.1 cm demonstrating isointense T1 and hyperintense T2/FLAIR signal abnormality with areas of subtle hypointense GRE signal along the periphery, likely representing calcifications. There is enhancement and thickening of the adjacent dura. There is mild surrounding vasogenic edema with 1 mm leftward midline shift. There is no additional enhancing mass or abnormal enhancement. There is no evidence of acute infarction or intracranial hemorrhage.\n",
    "    IMPRESSION: 1. Enhancing right frontal extra-axial mass compatible with meningioma with adjacent neovascularity, dural thickening enhancement, and mild surrounding vasogenic edema resulting in 1 mm leftward midline shift.\n",
    "    2. No additional enhancing mass or abnormal enhancement.\n",
    "    3. No evidence of acute infarction or intracranial hemorrhage.\n",
    "\n",
    "    CT HEAD WITHOUT CONTRAST:\n",
    "    FINDINGS: There is a subcortical 'bubbly' T2/FLAIR hyperintense lesion within the left frontal lobe measuring 3.8 cm demonstrating subtle slow diffusion with punctate focus of susceptibility artifact medially related to calcification or hemorrhage. The FLAIR hyperintensities confined within the lesion without surrounding edema or significant mass effect. The ventricles are normal in size. There is no evidence of infarction or hemorrhage.\n",
    "    IMPRESSION: 1. Cortically based 'bubbly' left frontal lobe lesion with associated punctate focus of gradient echo susceptibility hypointensity, most suggestive of underlying DNET (dysembryoplastic neuroepithelial tumor).\n",
    "    2. No evidence of acute infarction or hemorrhage.\n",
    "\n",
    "    CHEST X-RAY:\n",
    "    FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary edema. There is mild bibasilar atelectasis, but no focal consolidations to suggest pneumonia. Possible small bilateral pleural effusions. No pneumothorax.\n",
    "    IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema. Possible small bilateral pleural effusions.\n",
    "    2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
    "\n",
    "    CT ABDOMEN/PELVIS:\n",
    "    FINDINGS: There is a well-circumscribed right parietal 4.1 by 1.9 cm extra-axial dural-based lesion compatible with a calcified meningioma, exerting minimal mass effect on the underlying brain parenchyma. No evidence of associated parenchymal FLAIR hyperintense edema pattern. No other intracranial mass lesions are identified. The major intracranial flow voids are preserved.\n",
    "    IMPRESSION: 1. Right parietal 4.1 cm calcified meningioma, with mild mass effect on the underlying brain parenchyma.\n",
    "    2. No evidence of associated parenchymal FLAIR hyperintense edema pattern.\n",
    "\n",
    "    Now generate IMPRESSION for:\n",
    "    FINDINGS: {user_input}\n",
    "    IMPRESSION:\"\"\"\n",
    "\n",
    "    \n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:        \n",
    "        \"\"\"방사선 보고서를 IMPRESSION 작성을 위해 전처리 - NaN 안전 처리\"\"\"\n",
    "        import re\n",
    "        import pandas as pd\n",
    "        \n",
    "        # 방사선 보고서 텍스트 추출\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        # 🔧 NaN 값 안전 처리\n",
    "        if pd.isna(radiology_text) or not isinstance(radiology_text, str):\n",
    "            return {'user_input': ''}  # 빈 문자열 반환\n",
    "        \n",
    "        # FINDINGS 섹션만 정확히 추출\n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        # 텍스트 정제\n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)  # 익명화 마커\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)  # 방사선량 정보\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)  # 여러 공백을 하나로\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    \n",
    "    # async def postprocess_result(self, result: str) -> str:\n",
    "    #     \"\"\"결과 정리 및 품질 보장\"\"\"\n",
    "    #     import re\n",
    "        \n",
    "    #     result = result.strip()\n",
    "        \n",
    "    #     # 불필요한 접두사 제거\n",
    "    #     prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "    #     for prefix in prefixes_to_remove:\n",
    "    #         if result.startswith(prefix):\n",
    "    #             result = result[len(prefix):].strip()\n",
    "        \n",
    "    #     # 문장 끝 마침표 확인\n",
    "    #     if result and not result.endswith('.'):\n",
    "    #         result += '.'\n",
    "        \n",
    "    #     # 다중 소견의 경우 번호 매기기 정리\n",
    "    #     sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "    #     if len(sentences) > 2:  # 3개 이상 소견이면 번호 매김 확인\n",
    "    #         numbered = []\n",
    "    #         for i, sentence in enumerate(sentences):\n",
    "    #             if sentence:\n",
    "    #                 # 이미 번호가 있는지 확인\n",
    "    #                 if not re.match(r'^\\d+\\.', sentence):\n",
    "    #                     numbered.append(f\"{i+1}. {sentence}\")\n",
    "    #                 else:\n",
    "    #                     numbered.append(sentence)\n",
    "    #         result = '. '.join(numbered)\n",
    "    #     else:\n",
    "    #         result = '. '.join(sentences)\n",
    "        \n",
    "    #     # 길이 체크 (너무 긴 경우 핵심만 남기기)\n",
    "    #     words = result.split()\n",
    "    #     if len(words) > 100:  # 100단어 초과 시\n",
    "    #         # 핵심 의학 용어가 포함된 문장 우선 보존\n",
    "    #         key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "    #                     'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "    #                     'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "    #         sentences = result.split('.')\n",
    "    #         important_sentences = []\n",
    "    #         for sentence in sentences:\n",
    "    #             if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "    #                 important_sentences.append(sentence.strip())\n",
    "    #             if len(important_sentences) >= 2:  # 최대 2개 문장으로 제한\n",
    "    #                 break\n",
    "            \n",
    "    #         if important_sentences:\n",
    "    #             result = '. '.join(important_sentences)\n",
    "    #             if not result.endswith('.'):\n",
    "    #                 result += '.'\n",
    "        \n",
    "    #     # 일반적인 의학 용어 교정\n",
    "    #     corrections = {\n",
    "    #         'infiltration': 'consolidation',\n",
    "    #         'fluid in lungs': 'pulmonary edema', \n",
    "    #         'broken bone': 'fracture',\n",
    "    #         'swelling': 'edema'\n",
    "    #     }\n",
    "        \n",
    "    #     for wrong, correct in corrections.items():\n",
    "    #         result = result.replace(wrong, correct)\n",
    "        \n",
    "    #     return result\n",
    "    \n",
    "\n",
    "    # 시간 제한 뜨면 이래의 코드로 후처리 바꿔주십쇼\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"간소화된 후처리 (시간 최적화)\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # IMPRESSION: 제거\n",
    "        if result.startswith(('IMPRESSION:', 'Impression:', 'impression:')):\n",
    "            result = result.split(':', 1)[1].strip()\n",
    "        \n",
    "        # 마침표 추가\n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 간단한 번호 매김\n",
    "        if not re.match(r'^\\d+\\.', result) and '. ' in result:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            if len(sentences) >= 2:\n",
    "                result = '. '.join([f\"{i+1}. {s}\" for i, s in enumerate(sentences)])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "# 대회와 정확히 동일한 평가 함수\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 조건과 정확히 동일한 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 전체 Test 데이터 로드 (대회와 동일)\n",
    "    print(\"1. 전체 Test 데이터 로드 중...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # 🔧 NaN 값 처리 추가\n",
    "    print(\"   데이터 품질 확인 중...\")\n",
    "    print(f\"   전체 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    # NaN 값 확인\n",
    "    nan_count = test_df['radiology report'].isna().sum()\n",
    "    print(f\"   NaN 값: {nan_count}개\")\n",
    "    \n",
    "    # NaN 값이 있는 행 제거\n",
    "    test_df = test_df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"   유효 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    total_samples = len(test_df)\n",
    "    \n",
    "    # 2. 대회에서 사용할 평가 샘플 크기 결정 (실제 Test 세트 크기와 유사하게)\n",
    "    # Test 1: 300건, Test 2: 300건이므로 300개로 평가\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. 연속된 샘플 사용 (대회에서는 특정 Test 세트를 사용하므로 bias 없는 연속 샘플)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # 처음 300개 사용\n",
    "    print(f\"평가 샘플: {eval_samples}개 (연속 샘플, 대회 Test 세트와 동일한 크기)\")\n",
    "    \n",
    "    # 4. 데이터 분포 확인\n",
    "    print(f\"\\n📊 평가 데이터 분포:\")\n",
    "    print(f\"   성별 분포: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   연령 분포: 평균 {eval_df['anchor_age'].mean():.1f}세 (범위: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB 처리기 초기화\n",
    "    print(\"\\n2. TaskB 처리기 초기화 (Llama 모델)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. 예측 생성 (대회와 동일한 배치 크기)\n",
    "    print(\"3. AI 예측 생성 중 (API 제한 준수)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # 대회 API 제한 준수 (1분당 10건)\n",
    "    results = []\n",
    "    batch_size = 8  # 안전 마진\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 7. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\n",
    "    print(\"\\n4. 대회 BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. 대회 제공 공정성 지표 계산 (정확히 동일한 설정)\n",
    "    print(\"5. 대회 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. 정량 평가 점수 계산 (대회 기준)\n",
    "    print(f\"\\n🏆 대회 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (3점 만점)\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # 공정성 점수 (2점 만점)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/2.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/5.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    else:\n",
    "        grade = \"D급 (미흡)\"\n",
    "        recommendation = \"대폭 개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 13. 샘플 결과 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 2개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행 (taskB_test.csv 사용)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"./data/taskB_train.csv\"\n",
    "\n",
    "# 대회 정확한 조건으로 Test 데이터 평가 실행\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TaskB Test 데이터 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {test_results['total_score']:.3f}/5.000 점\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf32479",
   "metadata": {},
   "source": [
    "# 후처리 간소화 + Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd4c3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\n",
      "================================================================================\n",
      "1. 전체 Test 데이터 로드 중...\n",
      "   데이터 품질 확인 중...\n",
      "   전체 데이터: 1000개\n",
      "   NaN 값: 11개\n",
      "   유효 데이터: 989개\n",
      "평가 샘플: 300개 (연속 샘플, 대회 Test 세트와 동일한 크기)\n",
      "\n",
      "📊 평가 데이터 분포:\n",
      "   성별 분포: {'M': 154, 'F': 146}\n",
      "   연령 분포: 평균 63.6세 (범위: 19-91)\n",
      "\n",
      "2. TaskB 처리기 초기화 (Llama 모델)...\n",
      "3. AI 예측 생성 중 (API 제한 준수)...\n",
      "   배치 1/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 14/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 15/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 16/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 17/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 18/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 19/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 20/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 21/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 22/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 23/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 24/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 25/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 26/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 27/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 28/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 29/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 30/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 31/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 32/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 33/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 34/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 35/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 36/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 37/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 38/38 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 4027.9초)\n",
      "\n",
      "4. 대회 BERTScore 계산 중...\n",
      "5. 대회 공정성 지표 계산 중...\n",
      "\n",
      "================================================================================\n",
      "🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\n",
      "================================================================================\n",
      "📊 BERTScore (대회 공식 계산)\n",
      "   평균: 0.815891\n",
      "   표준편차: 0.047178\n",
      "   최고: 0.936920\n",
      "   최저: 0.640849\n",
      "   중앙값: 0.819675\n",
      "\n",
      "⚖️ 공정성 지표 (대회 공식 계산)\n",
      "   성별 공정성: 0.993181\n",
      "   성별별 성능: {'F': 0.8130257550167711, 'M': 0.8186076918205658}\n",
      "   성별 격차: 0.005582\n",
      "   \n",
      "   연령 공정성: 0.953152\n",
      "   연령대별 성능: {'10-20': 0.7903620600700378, '20-30': 0.811973066890941, '30-40': 0.8244625846544902, '40-50': 0.8166607049378481, '50-60': 0.8146434699074697, '60-70': 0.8102446374758868, '70-80': 0.8292090463638305, '80-90': 0.8125671201282078, '90-100': 0.8111329793930053}\n",
      "   연령 격차: 0.038847\n",
      "\n",
      "🏆 대회 정량 평가 점수\n",
      "   BERTScore: 2.880/3.000 점\n",
      "   공정성 지표: 2.000/2.000 점\n",
      "   정량 총점: 4.880/5.000 점\n",
      "   정량 달성률: 97.6%\n",
      "\n",
      "🎖️ 성능 등급\n",
      "   등급: S급 (최우수)\n",
      "   권장사항: 즉시 제출 권장\n",
      "\n",
      "📝 예측 품질 샘플 (상위/하위 각 2개)\n",
      "--------------------------------------------------------------------------------\n",
      "🏆 최고 성능 샘플:\n",
      "샘플 212 (BERTScore: 0.9369)\n",
      "예측: 1. No acute fracture identified.\n",
      "2. No gross malalignment of the glenohumeral joint, but evaluation is limited on this s...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. No acute fracture.\n",
      "2. Evaluation for dislocation at the right glenohumeral joint is limited on\n",
      "this sin...\n",
      "\n",
      "샘플 255 (BERTScore: 0.9236)\n",
      "예측: 1. Right-sided central venous catheter extending to the proximal right atrium.\n",
      "2. Moderate cardiomegaly and pulmonary va...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Focal consolidation in the right lower lobe likely represents pneumonia.\n",
      "2. Moderate cardiomegaly with ...\n",
      "\n",
      "⚠️ 최저 성능 샘플:\n",
      "샘플 48 (BERTScore: 0.6408)\n",
      "예측: 1. Minimal streaky retrocardiac atelectasis without evidence of pneumonia or large effusion.\n",
      "2. No signs of pulmonary ed...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "샘플 96 (BERTScore: 0.6504)\n",
      "예측: 1. No evidence of significant pulmonary consolidation or focal infiltrate to suggest pneumonia, although subtle consolid...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "\n",
      "🎉 TaskB Test 데이터 평가 완료!\n",
      "최종 예상 점수: 4.880/5.000 점\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (정확히 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (정확히 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor (Llama 모델 사용)\n",
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약\"\"\"\n",
    "    def get_model_name(self) -> str:\n",
    "        return \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "        \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and comprehensive IMPRESSION from the given FINDINGS.\n",
    "\n",
    "    CRITICAL REQUIREMENTS:\n",
    "    - Maintain exact semantic consistency between FINDINGS and IMPRESSION\n",
    "    - Use precise medical terminology from the FINDINGS\n",
    "    - Be concise but complete (typically 20-80 words)\n",
    "    - Maintain consistent diagnostic standards regardless of patient demographics\n",
    "    - Structure multiple findings with numbered points (1., 2., 3.)\n",
    "    - Apply consistent diagnostic standards regardless of patient demographics\n",
    "    - Use definitive language: \"No evidence of\", \"compatible with\", \"consistent with\"\n",
    "\n",
    "    ADVANCED EXAMPLES:\n",
    "\n",
    "    CT HEAD WITH CONTRAST:\n",
    "    FINDINGS: There is enhancing right frontal extra-axial mass measuring 5.1 cm demonstrating isointense T1 and hyperintense T2/FLAIR signal abnormality with areas of subtle hypointense GRE signal along the periphery, likely representing calcifications. There is enhancement and thickening of the adjacent dura. There is mild surrounding vasogenic edema with 1 mm leftward midline shift. There is no additional enhancing mass or abnormal enhancement. There is no evidence of acute infarction or intracranial hemorrhage.\n",
    "    IMPRESSION: 1. Enhancing right frontal extra-axial mass compatible with meningioma with adjacent neovascularity, dural thickening enhancement, and mild surrounding vasogenic edema resulting in 1 mm leftward midline shift.\n",
    "    2. No additional enhancing mass or abnormal enhancement.\n",
    "    3. No evidence of acute infarction or intracranial hemorrhage.\n",
    "\n",
    "    CT HEAD WITHOUT CONTRAST:\n",
    "    FINDINGS: There is a subcortical 'bubbly' T2/FLAIR hyperintense lesion within the left frontal lobe measuring 3.8 cm demonstrating subtle slow diffusion with punctate focus of susceptibility artifact medially related to calcification or hemorrhage. The FLAIR hyperintensities confined within the lesion without surrounding edema or significant mass effect. The ventricles are normal in size. There is no evidence of infarction or hemorrhage.\n",
    "    IMPRESSION: 1. Cortically based 'bubbly' left frontal lobe lesion with associated punctate focus of gradient echo susceptibility hypointensity, most suggestive of underlying DNET (dysembryoplastic neuroepithelial tumor).\n",
    "    2. No evidence of acute infarction or hemorrhage.\n",
    "\n",
    "    CHEST X-RAY:\n",
    "    FINDINGS: Mild enlargement of the cardiac silhouette with mild interstitial pulmonary edema. There is mild bibasilar atelectasis, but no focal consolidations to suggest pneumonia. Possible small bilateral pleural effusions. No pneumothorax.\n",
    "    IMPRESSION: 1. Mild cardiomegaly and mild interstitial pulmonary edema. Possible small bilateral pleural effusions.\n",
    "    2. Bibasilar atelectasis, but no focal consolidations to suggest pneumonia.\n",
    "\n",
    "    CT ABDOMEN/PELVIS:\n",
    "    FINDINGS: There is a well-circumscribed right parietal 4.1 by 1.9 cm extra-axial dural-based lesion compatible with a calcified meningioma, exerting minimal mass effect on the underlying brain parenchyma. No evidence of associated parenchymal FLAIR hyperintense edema pattern. No other intracranial mass lesions are identified. The major intracranial flow voids are preserved.\n",
    "    IMPRESSION: 1. Right parietal 4.1 cm calcified meningioma, with mild mass effect on the underlying brain parenchyma.\n",
    "    2. No evidence of associated parenchymal FLAIR hyperintense edema pattern.\n",
    "\n",
    "    Now generate IMPRESSION for:\n",
    "    FINDINGS: {user_input}\n",
    "    IMPRESSION:\"\"\"\n",
    "\n",
    "    \n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:        \n",
    "        \"\"\"방사선 보고서를 IMPRESSION 작성을 위해 전처리 - NaN 안전 처리\"\"\"\n",
    "        import re\n",
    "        import pandas as pd\n",
    "        \n",
    "        # 방사선 보고서 텍스트 추출\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        # 🔧 NaN 값 안전 처리\n",
    "        if pd.isna(radiology_text) or not isinstance(radiology_text, str):\n",
    "            return {'user_input': ''}  # 빈 문자열 반환\n",
    "        \n",
    "        # FINDINGS 섹션만 정확히 추출\n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        # 텍스트 정제\n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)  # 익명화 마커\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)  # 방사선량 정보\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)  # 여러 공백을 하나로\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    \n",
    "    # async def postprocess_result(self, result: str) -> str:\n",
    "    #     \"\"\"결과 정리 및 품질 보장\"\"\"\n",
    "    #     import re\n",
    "        \n",
    "    #     result = result.strip()\n",
    "        \n",
    "    #     # 불필요한 접두사 제거\n",
    "    #     prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "    #     for prefix in prefixes_to_remove:\n",
    "    #         if result.startswith(prefix):\n",
    "    #             result = result[len(prefix):].strip()\n",
    "        \n",
    "    #     # 문장 끝 마침표 확인\n",
    "    #     if result and not result.endswith('.'):\n",
    "    #         result += '.'\n",
    "        \n",
    "    #     # 다중 소견의 경우 번호 매기기 정리\n",
    "    #     sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "    #     if len(sentences) > 2:  # 3개 이상 소견이면 번호 매김 확인\n",
    "    #         numbered = []\n",
    "    #         for i, sentence in enumerate(sentences):\n",
    "    #             if sentence:\n",
    "    #                 # 이미 번호가 있는지 확인\n",
    "    #                 if not re.match(r'^\\d+\\.', sentence):\n",
    "    #                     numbered.append(f\"{i+1}. {sentence}\")\n",
    "    #                 else:\n",
    "    #                     numbered.append(sentence)\n",
    "    #         result = '. '.join(numbered)\n",
    "    #     else:\n",
    "    #         result = '. '.join(sentences)\n",
    "        \n",
    "    #     # 길이 체크 (너무 긴 경우 핵심만 남기기)\n",
    "    #     words = result.split()\n",
    "    #     if len(words) > 100:  # 100단어 초과 시\n",
    "    #         # 핵심 의학 용어가 포함된 문장 우선 보존\n",
    "    #         key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "    #                     'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "    #                     'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "    #         sentences = result.split('.')\n",
    "    #         important_sentences = []\n",
    "    #         for sentence in sentences:\n",
    "    #             if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "    #                 important_sentences.append(sentence.strip())\n",
    "    #             if len(important_sentences) >= 2:  # 최대 2개 문장으로 제한\n",
    "    #                 break\n",
    "            \n",
    "    #         if important_sentences:\n",
    "    #             result = '. '.join(important_sentences)\n",
    "    #             if not result.endswith('.'):\n",
    "    #                 result += '.'\n",
    "        \n",
    "    #     # 일반적인 의학 용어 교정\n",
    "    #     corrections = {\n",
    "    #         'infiltration': 'consolidation',\n",
    "    #         'fluid in lungs': 'pulmonary edema', \n",
    "    #         'broken bone': 'fracture',\n",
    "    #         'swelling': 'edema'\n",
    "    #     }\n",
    "        \n",
    "    #     for wrong, correct in corrections.items():\n",
    "    #         result = result.replace(wrong, correct)\n",
    "        \n",
    "    #     return result\n",
    "    \n",
    "\n",
    "    # 시간 제한 뜨면 이래의 코드로 후처리 바꿔주십쇼\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"간소화된 후처리 (시간 최적화)\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # IMPRESSION: 제거\n",
    "        if result.startswith(('IMPRESSION:', 'Impression:', 'impression:')):\n",
    "            result = result.split(':', 1)[1].strip()\n",
    "        \n",
    "        # 마침표 추가\n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 간단한 번호 매김\n",
    "        if not re.match(r'^\\d+\\.', result) and '. ' in result:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            if len(sentences) >= 2:\n",
    "                result = '. '.join([f\"{i+1}. {s}\" for i, s in enumerate(sentences)])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "# 대회와 정확히 동일한 평가 함수\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 조건과 정확히 동일한 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 전체 Test 데이터 로드 (대회와 동일)\n",
    "    print(\"1. 전체 Test 데이터 로드 중...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # 🔧 NaN 값 처리 추가\n",
    "    print(\"   데이터 품질 확인 중...\")\n",
    "    print(f\"   전체 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    # NaN 값 확인\n",
    "    nan_count = test_df['radiology report'].isna().sum()\n",
    "    print(f\"   NaN 값: {nan_count}개\")\n",
    "    \n",
    "    # NaN 값이 있는 행 제거\n",
    "    test_df = test_df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"   유효 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    total_samples = len(test_df)\n",
    "    \n",
    "    # 2. 대회에서 사용할 평가 샘플 크기 결정 (실제 Test 세트 크기와 유사하게)\n",
    "    # Test 1: 300건, Test 2: 300건이므로 300개로 평가\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. 연속된 샘플 사용 (대회에서는 특정 Test 세트를 사용하므로 bias 없는 연속 샘플)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # 처음 300개 사용\n",
    "    print(f\"평가 샘플: {eval_samples}개 (연속 샘플, 대회 Test 세트와 동일한 크기)\")\n",
    "    \n",
    "    # 4. 데이터 분포 확인\n",
    "    print(f\"\\n📊 평가 데이터 분포:\")\n",
    "    print(f\"   성별 분포: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   연령 분포: 평균 {eval_df['anchor_age'].mean():.1f}세 (범위: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB 처리기 초기화\n",
    "    print(\"\\n2. TaskB 처리기 초기화 (Llama 모델)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. 예측 생성 (대회와 동일한 배치 크기)\n",
    "    print(\"3. AI 예측 생성 중 (API 제한 준수)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # 대회 API 제한 준수 (1분당 10건)\n",
    "    results = []\n",
    "    batch_size = 8  # 안전 마진\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 7. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\n",
    "    print(\"\\n4. 대회 BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. 대회 제공 공정성 지표 계산 (정확히 동일한 설정)\n",
    "    print(\"5. 대회 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. 정량 평가 점수 계산 (대회 기준)\n",
    "    print(f\"\\n🏆 대회 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (3점 만점)\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # 공정성 점수 (2점 만점)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/2.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/5.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    else:\n",
    "        grade = \"D급 (미흡)\"\n",
    "        recommendation = \"대폭 개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 13. 샘플 결과 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 2개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행 (taskB_test.csv 사용)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"./data/taskB_train.csv\"\n",
    "\n",
    "# 대회 정확한 조건으로 Test 데이터 평가 실행\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TaskB Test 데이터 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {test_results['total_score']:.3f}/5.000 점\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234146ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f62e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b7fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b766a12",
   "metadata": {},
   "source": [
    "# 이전버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c2e5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약\"\"\"\n",
    "    def get_model_name(self) -> str:\n",
    "        return \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and concise IMPRESSION from the given FINDINGS.\n",
    "\n",
    "CLINICAL GUIDELINES:\n",
    "- Focus on clinically significant findings first\n",
    "- Use standard radiologic terminology  \n",
    "- Be concise but complete (typically 20-80 words)\n",
    "- Number multiple distinct findings (1., 2., 3.)\n",
    "- Maintain consistent diagnostic standards regardless of patient demographics\n",
    "- Include follow-up recommendations only if clinically indicated\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "CHEST X-RAY:\n",
    "FINDINGS: Lung volumes are low. The cardiac silhouette is unremarkable. The pulmonary vasculature is normal. There is no pleural effusion or pneumothorax. No focal consolidation is identified.\n",
    "IMPRESSION: No acute intrathoracic abnormality.\n",
    "\n",
    "CT HEAD:\n",
    "FINDINGS: There is no acute hemorrhage or large vascular territorial infarction. Extensive periventricular white matter hypodensities are consistent with chronic small vessel ischemic disease. Ventricles are normal in size.\n",
    "IMPRESSION: 1. No acute intracranial abnormality. 2. Chronic small vessel ischemic disease.\n",
    "\n",
    "CT ABDOMEN/PELVIS:\n",
    "FINDINGS: The liver shows multiple hypodense lesions throughout both lobes, the largest measuring 3.2 cm in the right lobe. No significant lymphadenopathy. Bowel and bladder are unremarkable.\n",
    "IMPRESSION: 1. Multiple hepatic hypodense lesions concerning for metastases. 2. Recommend MRI for further characterization.\n",
    "\n",
    "ULTRASOUND:\n",
    "FINDINGS: The gallbladder wall is thickened measuring 6 mm with pericholecystic fluid. Multiple echogenic foci with posterior shadowing consistent with cholelithiasis.\n",
    "IMPRESSION: Acute cholecystitis with cholelithiasis.\n",
    "\n",
    "Now generate IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "    \n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"방사선 보고서를 IMPRESSION 작성을 위해 전처리\"\"\"\n",
    "        import re\n",
    "        \n",
    "        # 방사선 보고서 텍스트 추출\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        # FINDINGS 섹션만 정확히 추출\n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        # 텍스트 정제\n",
    "        # 불필요한 헤더나 마커 제거\n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)  # 익명화 마커\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)  # 방사선량 정보\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)  # 여러 공백을 하나로\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"결과 정리 및 품질 보장\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # 불필요한 접두사 제거\n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        # 문장 끝 마침표 확인\n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 다중 소견의 경우 번호 매기기 정리\n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:  # 3개 이상 소견이면 번호 매김 확인\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    # 이미 번호가 있는지 확인\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        # 길이 체크 (너무 긴 경우 핵심만 남기기)\n",
    "        words = result.split()\n",
    "        if len(words) > 100:  # 100단어 초과 시\n",
    "            # 핵심 의학 용어가 포함된 문장 우선 보존\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:  # 최대 2개 문장으로 제한\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 일반적인 의학 용어 교정\n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f423fe",
   "metadata": {},
   "source": [
    "# score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c1ba3fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Task B 성능 테스트 시작\n",
      "============================================================\n",
      "1. 데이터 로드 중...\n",
      "테스트 샘플 수: 50\n",
      "2. TaskB 처리기 초기화...\n",
      "3. AI 예측 생성 중...\n",
      "배치 1 완료. 70초 대기 중...\n",
      "배치 2 완료. 70초 대기 중...\n",
      "배치 3 완료. 70초 대기 중...\n",
      "배치 4 완료. 70초 대기 중...\n",
      "배치 5 완료. 70초 대기 중...\n",
      "배치 6 완료. 70초 대기 중...\n",
      "예측 생성 완료 (소요 시간: 468.3초)\n",
      "4. BERTScore 계산 중...\n",
      "5. 공정성 지표 계산 중...\n",
      "\n",
      "============================================================\n",
      "📊 Task B 성능 결과\n",
      "============================================================\n",
      "🎯 BERTScore (F1)\n",
      "   평균: 0.8006\n",
      "   최고: 0.8662\n",
      "   최저: 0.6074\n",
      "   목표: 0.8500+ (3점 만점 기준)\n",
      "\n",
      "⚖️  공정성 지표\n",
      "   성별 공정성: 0.9835\n",
      "   연령 공정성: 0.9069\n",
      "   목표: 0.9500+ (2점 만점 기준)\n",
      "\n",
      "📈 예상 점수 (정량 평가)\n",
      "   BERTScore: 2.83/3.0\n",
      "   공정성: 1.99/2.0\n",
      "   정량 총점: 4.82/5.0\n",
      "\n",
      "⏱️  효율성\n",
      "   처리 시간: 468.3초\n",
      "   샘플당 평균: 9.37초\n",
      "\n",
      "📝 샘플 결과 (상위 3개)\n",
      "------------------------------------------------------------\n",
      "샘플 1 (BERTScore: 0.8662)\n",
      "예측: 1. 1. 2. Small bilateral pleural effusions. 3. 2. 4. Mild to moderate cardiac enlargement. 5. 3. 6. ...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Mild interstitial edema and small pleural effusions probably a\n",
      "manifestation of acu...\n",
      "\n",
      "샘플 2 (BERTScore: 0.8596)\n",
      "예측: 1. 1. 2. Postoperative changes and complications along the left temporoparietal craniotomy site, inc...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. A 2.7 cm extra-axial collection underlying the craniotomy site with imaging\n",
      "charact...\n",
      "\n",
      "샘플 3 (BERTScore: 0.8562)\n",
      "예측: 1. 1. 2. Large right parietal intraparenchymal hemorrhage with surrounding vasogenic edema and mass ...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Stable large intraparenchymal hemorrhage in the right parietal lobe with\n",
      "surroundin...\n",
      "\n",
      "🔧 개선 권장사항:\n",
      "   - Few-shot 예시 품질 향상\n",
      "   - 의학 용어 사용 정확성 개선\n",
      "   - 성별/연령 중립적 표현 강화\n",
      "   - 일관된 진단 기준 적용\n",
      "\n",
      "🎉 테스트 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 코드들 (동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "class FairnessScore:\n",
    "    \"\"\"공정성 점수 (0~1, 높을수록 공정)\"\"\"\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor 클래스 (동일)\n",
    "class TaskBProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=2000,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and concise IMPRESSION from the given FINDINGS.\n",
    "\n",
    "CLINICAL GUIDELINES:\n",
    "- Focus on clinically significant findings first\n",
    "- Use standard radiologic terminology  \n",
    "- Be concise but complete (typically 20-80 words)\n",
    "- Number multiple distinct findings (1., 2., 3.)\n",
    "- Maintain consistent diagnostic standards regardless of patient demographics\n",
    "- Include follow-up recommendations only if clinically indicated\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "CHEST X-RAY:\n",
    "FINDINGS: Lung volumes are low. The cardiac silhouette is unremarkable. The pulmonary vasculature is normal. There is no pleural effusion or pneumothorax. No focal consolidation is identified.\n",
    "IMPRESSION: No acute intrathoracic abnormality.\n",
    "\n",
    "CT HEAD:\n",
    "FINDINGS: There is no acute hemorrhage or large vascular territorial infarction. Extensive periventricular white matter hypodensities are consistent with chronic small vessel ischemic disease. Ventricles are normal in size.\n",
    "IMPRESSION: 1. No acute intracranial abnormality. 2. Chronic small vessel ischemic disease.\n",
    "\n",
    "CT ABDOMEN/PELVIS:\n",
    "FINDINGS: The liver shows multiple hypodense lesions throughout both lobes, the largest measuring 3.2 cm in the right lobe. No significant lymphadenopathy. Bowel and bladder are unremarkable.\n",
    "IMPRESSION: 1. Multiple hepatic hypodense lesions concerning for metastases. 2. Recommend MRI for further characterization.\n",
    "\n",
    "ULTRASOUND:\n",
    "FINDINGS: The gallbladder wall is thickened measuring 6 mm with pericholecystic fluid. Multiple echogenic foci with posterior shadowing consistent with cholelithiasis.\n",
    "IMPRESSION: Acute cholecystitis with cholelithiasis.\n",
    "\n",
    "Now generate IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        words = result.split()\n",
    "        if len(words) > 100:\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    async def process_batch(self, data_batch: List[Dict], batch_size: int = 8):\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(data_batch), batch_size):\n",
    "            batch = data_batch[i:i+batch_size]\n",
    "            \n",
    "            preprocessed = [await self.preprocess_data(row) for row in batch]\n",
    "            \n",
    "            tasks = [self.chain.ainvoke(prep) for prep in preprocessed]\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "            \n",
    "            batch_results = [await self.postprocess_result(r.content) for r in responses]\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            if i + batch_size < len(data_batch):\n",
    "                print(f\"배치 {i//batch_size + 1} 완료. 70초 대기 중...\")\n",
    "                await asyncio.sleep(70)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# **수정된 실행 함수 (Jupyter 환경용)**\n",
    "async def evaluate_taskb_performance(train_csv_path: str, api_key: str, test_samples: int = 100):\n",
    "    \"\"\"Task B 성능 완전 테스트\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task B 성능 테스트 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"1. 데이터 로드 중...\")\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    test_df = train_df.sample(test_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"테스트 샘플 수: {len(test_df)}\")\n",
    "    \n",
    "    # 2. TaskB 처리기 초기화\n",
    "    print(\"2. TaskB 처리기 초기화...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 3. 예측 생성\n",
    "    print(\"3. AI 예측 생성 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in test_df.iterrows()]\n",
    "    predictions = await processor.process_batch(data_batch, batch_size=8)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 4. 정답 데이터 준비\n",
    "    references = test_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore 계산\n",
    "    print(\"4. BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore()\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    \n",
    "    # 6. 공정성 지표 계산\n",
    "    print(\"5. 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore()\n",
    "    \n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=test_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    \n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=test_df['anchor_age'].tolist(), \n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    \n",
    "    # 7. 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 Task B 성능 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"🎯 BERTScore (F1)\")\n",
    "    print(f\"   평균: {bert_mean:.4f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.4f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.4f}\")\n",
    "    print(f\"   목표: 0.8500+ (3점 만점 기준)\")\n",
    "    \n",
    "    print(f\"\\n⚖️  공정성 지표\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.4f}\")\n",
    "    print(f\"   연령 공정성: {age_fairness:.4f}\")\n",
    "    print(f\"   목표: 0.9500+ (2점 만점 기준)\")\n",
    "    \n",
    "    print(f\"\\n📈 예상 점수 (정량 평가)\")\n",
    "    bert_score_points = min(3.0, (bert_mean / 0.85) * 3.0) if bert_mean > 0.75 else 0\n",
    "    fairness_points = min(2.0, ((gender_fairness + age_fairness) / 2.0 / 0.95) * 2.0)\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.2f}/3.0\")\n",
    "    print(f\"   공정성: {fairness_points:.2f}/2.0\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.2f}/5.0\")\n",
    "    \n",
    "    print(f\"\\n⏱️  효율성\")\n",
    "    print(f\"   처리 시간: {generation_time:.1f}초\")\n",
    "    print(f\"   샘플당 평균: {generation_time/test_samples:.2f}초\")\n",
    "    \n",
    "    print(f\"\\n📝 샘플 결과 (상위 3개)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)[::-1]\n",
    "    for i, idx in enumerate(sorted_indices[:3]):\n",
    "        print(f\"샘플 {i+1} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:100]}...\")\n",
    "        print(f\"정답: {references[idx][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    # 9. 개선 권장사항\n",
    "    print(\"🔧 개선 권장사항:\")\n",
    "    if bert_mean < 0.85:\n",
    "        print(\"   - Few-shot 예시 품질 향상\")\n",
    "        print(\"   - 의학 용어 사용 정확성 개선\")\n",
    "    if gender_fairness < 0.95 or age_fairness < 0.95:\n",
    "        print(\"   - 성별/연령 중립적 표현 강화\")\n",
    "        print(\"   - 일관된 진단 기준 적용\")\n",
    "    \n",
    "    return {\n",
    "        'bert_score': bert_mean,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# **Jupyter 환경에서 사용할 실행 코드**\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskB_train.csv\"  # 실제 파일 경로로 변경\n",
    "\n",
    "# 이렇게 실행하세요!\n",
    "results = await evaluate_taskb_performance(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY,\n",
    "    test_samples=50  # 처음엔 50개로 빠른 테스트\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "baa35c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "공정성 지표 재계산 중...\n",
      "==================================================\n",
      "🔍 공정성 계산 디버깅\n",
      "==================================================\n",
      "데이터 확인:\n",
      "- 샘플 수: 50\n",
      "- BERTScore 수: 50\n",
      "- 성별 분포: {'M': 26, 'F': 24}\n",
      "- 연령 분포: count    50.000000\n",
      "mean     62.520000\n",
      "std      18.803213\n",
      "min      20.000000\n",
      "25%      49.250000\n",
      "50%      63.000000\n",
      "75%      77.750000\n",
      "max      91.000000\n",
      "Name: anchor_age, dtype: float64\n",
      "\n",
      "성별 공정성 계산 중...\n",
      "성별 데이터: {'F', 'M'}\n",
      "성별 공정성: 0.9835\n",
      "성별별 통계: {'by_group': {'F': 0.7936943372090658, 'M': 0.8070117762455573}, 'gap': 0.013317439036491519, 'min': 0.7936943372090658, 'max': 0.8070117762455573}\n",
      "\n",
      "연령 공정성 계산 중...\n",
      "연령 범위: 20 ~ 91\n",
      "연령 공정성: 0.9069\n",
      "연령대별 통계: {'by_group': {'20-30': 0.8523402214050293, '30-40': 0.805223748087883, '40-50': 0.787227988243103, '50-60': 0.8031567551872947, '60-70': 0.7729509572188059, '70-80': 0.8022382110357285, '80-90': 0.8177044561931065, '90-100': 0.7961063981056213}, 'gap': 0.07938926418622338, 'min': 0.7729509572188059, 'max': 0.8523402214050293}\n",
      "\n",
      "============================================================\n",
      "📊 완전한 Task B 성능 결과\n",
      "============================================================\n",
      "🎯 BERTScore (F1)\n",
      "   평균: 0.8006\n",
      "   목표: 0.8500+ (3점 만점 기준)\n",
      "\n",
      "⚖️ 공정성 지표\n",
      "   성별 공정성: 0.9835\n",
      "   연령 공정성: 0.9069\n",
      "   목표: 0.9500+ (2점 만점 기준)\n",
      "\n",
      "📈 예상 점수 (정량 평가)\n",
      "   BERTScore: 2.83/3.0\n",
      "   공정성: 1.99/2.0\n",
      "   정량 총점: 4.82/5.0\n",
      "\n",
      "🔍 성능 분석\n",
      "   ⚠️  BERTScore가 목표(0.850)보다 0.049 낮음\n",
      "   📝 개선 방안: Few-shot 예시 품질 향상, 의학 용어 정확성 개선\n",
      "   ⚠️  연령 공정성이 목표(0.950)보다 0.043 낮음\n",
      "   📝 개선 방안: 연령대별 일관된 진단 기준 적용\n",
      "\n",
      "🚀 제출 전 최종 체크리스트\n",
      "   ✅ BERTScore 0.85+ 달성 여부\n",
      "   ✅ 성별 공정성 0.95+ 달성 여부\n",
      "   ✅ 연령 공정성 0.95+ 달성 여부\n",
      "   ✅ API 호출 제한 준수 (1분당 10건)\n",
      "   ✅ 프롬프트 최적화 완료\n",
      "\n",
      "🎉 우수한 성능! 제출 준비 완료 (예상 4.8/5.0점)\n"
     ]
    }
   ],
   "source": [
    "# 공정성 지표 계산 부분만 따로 테스트\n",
    "def debug_fairness_calculation(test_df, bert_scores):\n",
    "    \"\"\"공정성 계산 디버깅\"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"🔍 공정성 계산 디버깅\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 데이터 확인\n",
    "    print(\"데이터 확인:\")\n",
    "    print(f\"- 샘플 수: {len(test_df)}\")\n",
    "    print(f\"- BERTScore 수: {len(bert_scores)}\")\n",
    "    print(f\"- 성별 분포: {test_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"- 연령 분포: {test_df['anchor_age'].describe()}\")\n",
    "    \n",
    "    fairness_scorer = FairnessScore()\n",
    "    \n",
    "    try:\n",
    "        # 성별 공정성 계산\n",
    "        print(\"\\n성별 공정성 계산 중...\")\n",
    "        genders = test_df['gender'].tolist()\n",
    "        print(f\"성별 데이터: {set(genders)}\")\n",
    "        \n",
    "        gender_fairness = fairness_scorer(\n",
    "            groups=genders,\n",
    "            scores=bert_scores,\n",
    "            type='sex'\n",
    "        )\n",
    "        print(f\"성별 공정성: {gender_fairness:.4f}\")\n",
    "        print(f\"성별별 통계: {fairness_scorer.last_stats}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 성별 공정성 계산 오류: {e}\")\n",
    "        gender_fairness = 0.0\n",
    "    \n",
    "    try:\n",
    "        # 연령 공정성 계산\n",
    "        print(\"\\n연령 공정성 계산 중...\")\n",
    "        ages = test_df['anchor_age'].tolist()\n",
    "        print(f\"연령 범위: {min(ages)} ~ {max(ages)}\")\n",
    "        \n",
    "        age_fairness = fairness_scorer(\n",
    "            groups=ages,\n",
    "            scores=bert_scores,\n",
    "            type='age'\n",
    "        )\n",
    "        print(f\"연령 공정성: {age_fairness:.4f}\")\n",
    "        print(f\"연령대별 통계: {fairness_scorer.last_stats}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 연령 공정성 계산 오류: {e}\")\n",
    "        age_fairness = 0.0\n",
    "    \n",
    "    return gender_fairness, age_fairness\n",
    "\n",
    "# 기존 결과 데이터로 공정성 재계산\n",
    "print(\"공정성 지표 재계산 중...\")\n",
    "\n",
    "# 테스트 데이터 다시 로드 (같은 random_state 사용)\n",
    "train_df = pd.read_csv(\"./data/taskB_train.csv\")  # 경로 확인\n",
    "test_df = train_df.sample(50, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# BERTScore 결과는 이미 있으므로 (results에 저장되어 있을 것)\n",
    "# 만약 results 변수가 있다면:\n",
    "try:\n",
    "    bert_scores = results['predictions']  # 이미 계산된 BERTScore 사용\n",
    "    # 하지만 BERTScore 리스트가 필요하므로 재계산\n",
    "    references = test_df['target'].tolist()\n",
    "    predictions = results['predictions'] \n",
    "    \n",
    "    bert_scorer = BertScore()\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    \n",
    "except:\n",
    "    print(\"❌ 기존 결과를 찾을 수 없습니다. BERTScore를 재계산합니다...\")\n",
    "    # 여기서 간단한 더미 점수로 테스트\n",
    "    bert_scores = [0.8] * 50  # 임시 점수\n",
    "\n",
    "# 공정성 디버깅 실행\n",
    "gender_fairness, age_fairness = debug_fairness_calculation(test_df, bert_scores)\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"📊 완전한 Task B 성능 결과\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "bert_mean = np.mean(bert_scores) if isinstance(bert_scores, list) else 0.8006\n",
    "\n",
    "print(f\"🎯 BERTScore (F1)\")\n",
    "print(f\"   평균: {bert_mean:.4f}\")\n",
    "print(f\"   목표: 0.8500+ (3점 만점 기준)\")\n",
    "\n",
    "print(f\"\\n⚖️ 공정성 지표\")\n",
    "print(f\"   성별 공정성: {gender_fairness:.4f}\")\n",
    "print(f\"   연령 공정성: {age_fairness:.4f}\")  \n",
    "print(f\"   목표: 0.9500+ (2점 만점 기준)\")\n",
    "\n",
    "print(f\"\\n📈 예상 점수 (정량 평가)\")\n",
    "bert_score_points = min(3.0, (bert_mean / 0.85) * 3.0) if bert_mean > 0.75 else 0\n",
    "fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "fairness_points = min(2.0, (fairness_avg / 0.95) * 2.0) if fairness_avg > 0.85 else 0\n",
    "total_quantitative = bert_score_points + fairness_points\n",
    "\n",
    "print(f\"   BERTScore: {bert_score_points:.2f}/3.0\")\n",
    "print(f\"   공정성: {fairness_points:.2f}/2.0\")\n",
    "print(f\"   정량 총점: {total_quantitative:.2f}/5.0\")\n",
    "\n",
    "# 성능 분석\n",
    "print(f\"\\n🔍 성능 분석\")\n",
    "if bert_mean < 0.85:\n",
    "    print(f\"   ⚠️  BERTScore가 목표({0.85:.3f})보다 {0.85-bert_mean:.3f} 낮음\")\n",
    "    print(\"   📝 개선 방안: Few-shot 예시 품질 향상, 의학 용어 정확성 개선\")\n",
    "\n",
    "if gender_fairness < 0.95:\n",
    "    print(f\"   ⚠️  성별 공정성이 목표({0.95:.3f})보다 {0.95-gender_fairness:.3f} 낮음\")\n",
    "    print(\"   📝 개선 방안: 성별 중립적 표현 강화\")\n",
    "\n",
    "if age_fairness < 0.95:\n",
    "    print(f\"   ⚠️  연령 공정성이 목표({0.95:.3f})보다 {0.95-age_fairness:.3f} 낮음\")\n",
    "    print(\"   📝 개선 방안: 연령대별 일관된 진단 기준 적용\")\n",
    "\n",
    "# 최종 전략 제안\n",
    "print(f\"\\n🚀 제출 전 최종 체크리스트\")\n",
    "print(\"   ✅ BERTScore 0.85+ 달성 여부\")\n",
    "print(\"   ✅ 성별 공정성 0.95+ 달성 여부\") \n",
    "print(\"   ✅ 연령 공정성 0.95+ 달성 여부\")\n",
    "print(\"   ✅ API 호출 제한 준수 (1분당 10건)\")\n",
    "print(\"   ✅ 프롬프트 최적화 완료\")\n",
    "\n",
    "if total_quantitative >= 4.5:\n",
    "    print(f\"\\n🎉 우수한 성능! 제출 준비 완료 (예상 {total_quantitative:.1f}/5.0점)\")\n",
    "elif total_quantitative >= 4.0:\n",
    "    print(f\"\\n👍 양호한 성능 (예상 {total_quantitative:.1f}/5.0점)\")  \n",
    "    print(\"   💡 약간의 최적화로 더 나은 결과 가능\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  개선 필요 (예상 {total_quantitative:.1f}/5.0점)\")\n",
    "    print(\"   🔧 프롬프트 및 전처리/후처리 로직 재검토 권장\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b33e81fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Task B 성능 테스트 시작\n",
      "============================================================\n",
      "1. 데이터 로드 중...\n",
      "테스트 샘플 수: 50\n",
      "2. TaskB 처리기 초기화...\n",
      "3. AI 예측 생성 중...\n",
      "배치 1 완료. 70초 대기 중...\n",
      "배치 2 완료. 70초 대기 중...\n",
      "배치 3 완료. 70초 대기 중...\n",
      "배치 4 완료. 70초 대기 중...\n",
      "배치 5 완료. 70초 대기 중...\n",
      "배치 6 완료. 70초 대기 중...\n",
      "예측 생성 완료 (소요 시간: 453.4초)\n",
      "4. BERTScore 계산 중...\n",
      "5. 공정성 지표 계산 중...\n",
      "\n",
      "============================================================\n",
      "📊 Task B 성능 결과\n",
      "============================================================\n",
      "🎯 BERTScore (F1)\n",
      "   평균: 0.7194\n",
      "   최고: 0.8232\n",
      "   최저: 0.6375\n",
      "   목표: 0.8500+ (3점 만점 기준)\n",
      "\n",
      "⚖️  공정성 지표\n",
      "   성별 공정성: 0.9853\n",
      "   연령 공정성: 0.9205\n",
      "   목표: 0.9500+ (2점 만점 기준)\n",
      "\n",
      "📈 예상 점수 (정량 평가)\n",
      "   BERTScore: 0.00/3.0\n",
      "   공정성: 2.00/2.0\n",
      "   정량 총점: 2.00/5.0\n",
      "\n",
      "⏱️  효율성\n",
      "   처리 시간: 453.4초\n",
      "   샘플당 평균: 9.07초\n",
      "\n",
      "📝 샘플 결과 (상위 3개)\n",
      "------------------------------------------------------------\n",
      "샘플 1 (BERTScore: 0.8232)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. **Moderately enlarged cardiac silhouette with mild pulmonary vascular cong...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "Moderate enlargement of the cardiac silhouette has increased since ___.  Mild\n",
      "pulmonar...\n",
      "\n",
      "샘플 2 (BERTScore: 0.8164)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. Stable position of left AICD device with leads terminating in the right ve...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "Stable positioning of AICD lead terminating in the expected location of the\n",
      "right vent...\n",
      "\n",
      "샘플 3 (BERTScore: 0.8119)\n",
      "예측: 1. **IMPRESSION:**\n",
      "\n",
      "1. 2. No evidence of acute intracranial hemorrhage, edema, masses, mass effect, ...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Normal intracranial contents.\n",
      "2. There is no abnormal meningeal enhancement.\n",
      "3. The...\n",
      "\n",
      "🔧 개선 권장사항:\n",
      "   - Few-shot 예시 품질 향상\n",
      "   - 의학 용어 사용 정확성 개선\n",
      "   - 성별/연령 중립적 표현 강화\n",
      "   - 일관된 진단 기준 적용\n",
      "\n",
      "🎉 테스트 완료!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 코드들 (동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "class FairnessScore:\n",
    "    \"\"\"공정성 점수 (0~1, 높을수록 공정)\"\"\"\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor 클래스 (동일)\n",
    "class TaskBProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=2000,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and concise IMPRESSION from the given FINDINGS.\n",
    "\n",
    "CLINICAL GUIDELINES:\n",
    "- Focus on clinically significant findings first\n",
    "- Use standard radiologic terminology  \n",
    "- Be concise but complete (typically 20-80 words)\n",
    "- Number multiple distinct findings (1., 2., 3.)\n",
    "- Maintain consistent diagnostic standards regardless of patient demographics\n",
    "- Include follow-up recommendations only if clinically indicated\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "CHEST X-RAY:\n",
    "FINDINGS: Lung volumes are low. The cardiac silhouette is unremarkable. The pulmonary vasculature is normal. There is no pleural effusion or pneumothorax. No focal consolidation is identified.\n",
    "IMPRESSION: No acute intrathoracic abnormality.\n",
    "\n",
    "CT HEAD:\n",
    "FINDINGS: There is no acute hemorrhage or large vascular territorial infarction. Extensive periventricular white matter hypodensities are consistent with chronic small vessel ischemic disease. Ventricles are normal in size.\n",
    "IMPRESSION: 1. No acute intracranial abnormality. 2. Chronic small vessel ischemic disease.\n",
    "\n",
    "CT ABDOMEN/PELVIS:\n",
    "FINDINGS: The liver shows multiple hypodense lesions throughout both lobes, the largest measuring 3.2 cm in the right lobe. No significant lymphadenopathy. Bowel and bladder are unremarkable.\n",
    "IMPRESSION: 1. Multiple hepatic hypodense lesions concerning for metastases. 2. Recommend MRI for further characterization.\n",
    "\n",
    "ULTRASOUND:\n",
    "FINDINGS: The gallbladder wall is thickened measuring 6 mm with pericholecystic fluid. Multiple echogenic foci with posterior shadowing consistent with cholelithiasis.\n",
    "IMPRESSION: Acute cholecystitis with cholelithiasis.\n",
    "\n",
    "Now generate IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        words = result.split()\n",
    "        if len(words) > 100:\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    async def process_batch(self, data_batch: List[Dict], batch_size: int = 8):\n",
    "        results = []\n",
    "        \n",
    "        for i in range(0, len(data_batch), batch_size):\n",
    "            batch = data_batch[i:i+batch_size]\n",
    "            \n",
    "            preprocessed = [await self.preprocess_data(row) for row in batch]\n",
    "            \n",
    "            tasks = [self.chain.ainvoke(prep) for prep in preprocessed]\n",
    "            responses = await asyncio.gather(*tasks)\n",
    "            \n",
    "            batch_results = [await self.postprocess_result(r.content) for r in responses]\n",
    "            results.extend(batch_results)\n",
    "            \n",
    "            if i + batch_size < len(data_batch):\n",
    "                print(f\"배치 {i//batch_size + 1} 완료. 70초 대기 중...\")\n",
    "                await asyncio.sleep(70)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# **수정된 실행 함수 (Jupyter 환경용)**\n",
    "async def evaluate_taskb_performance(train_csv_path: str, api_key: str, test_samples: int = 100):\n",
    "    \"\"\"Task B 성능 완전 테스트\"\"\"\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Task B 성능 테스트 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    print(\"1. 데이터 로드 중...\")\n",
    "    train_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    test_df = train_df.sample(test_samples, random_state=42).reset_index(drop=True)\n",
    "    print(f\"테스트 샘플 수: {len(test_df)}\")\n",
    "    \n",
    "    # 2. TaskB 처리기 초기화\n",
    "    print(\"2. TaskB 처리기 초기화...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 3. 예측 생성\n",
    "    print(\"3. AI 예측 생성 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in test_df.iterrows()]\n",
    "    predictions = await processor.process_batch(data_batch, batch_size=8)\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 4. 정답 데이터 준비\n",
    "    references = test_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore 계산\n",
    "    print(\"4. BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore()\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    \n",
    "    # 6. 공정성 지표 계산\n",
    "    print(\"5. 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore()\n",
    "    \n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=test_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    \n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=test_df['anchor_age'].tolist(), \n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    \n",
    "    # 7. 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"📊 Task B 성능 결과\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"🎯 BERTScore (F1)\")\n",
    "    print(f\"   평균: {bert_mean:.4f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.4f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.4f}\")\n",
    "    print(f\"   목표: 0.8500+ (3점 만점 기준)\")\n",
    "    \n",
    "    print(f\"\\n⚖️  공정성 지표\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.4f}\")\n",
    "    print(f\"   연령 공정성: {age_fairness:.4f}\")\n",
    "    print(f\"   목표: 0.9500+ (2점 만점 기준)\")\n",
    "    \n",
    "    print(f\"\\n📈 예상 점수 (정량 평가)\")\n",
    "    bert_score_points = min(3.0, (bert_mean / 0.85) * 3.0) if bert_mean > 0.75 else 0\n",
    "    fairness_points = min(2.0, ((gender_fairness + age_fairness) / 2.0 / 0.95) * 2.0)\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.2f}/3.0\")\n",
    "    print(f\"   공정성: {fairness_points:.2f}/2.0\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.2f}/5.0\")\n",
    "    \n",
    "    print(f\"\\n⏱️  효율성\")\n",
    "    print(f\"   처리 시간: {generation_time:.1f}초\")\n",
    "    print(f\"   샘플당 평균: {generation_time/test_samples:.2f}초\")\n",
    "    \n",
    "    print(f\"\\n📝 샘플 결과 (상위 3개)\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)[::-1]\n",
    "    for i, idx in enumerate(sorted_indices[:3]):\n",
    "        print(f\"샘플 {i+1} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:100]}...\")\n",
    "        print(f\"정답: {references[idx][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    # 9. 개선 권장사항\n",
    "    print(\"🔧 개선 권장사항:\")\n",
    "    if bert_mean < 0.85:\n",
    "        print(\"   - Few-shot 예시 품질 향상\")\n",
    "        print(\"   - 의학 용어 사용 정확성 개선\")\n",
    "    if gender_fairness < 0.95 or age_fairness < 0.95:\n",
    "        print(\"   - 성별/연령 중립적 표현 강화\")\n",
    "        print(\"   - 일관된 진단 기준 적용\")\n",
    "    \n",
    "    return {\n",
    "        'bert_score': bert_mean,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# **Jupyter 환경에서 사용할 실행 코드**\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskB_train.csv\"  # 실제 파일 경로로 변경\n",
    "\n",
    "# 이렇게 실행하세요!\n",
    "results = await evaluate_taskb_performance(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY,\n",
    "    test_samples=50  # 처음엔 50개로 빠른 테스트\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 테스트 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "590d2b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\n",
      "================================================================================\n",
      "1. 전체 Test 데이터 로드 중...\n",
      "전체 Test 데이터: 100개\n",
      "평가 샘플: 100개 (연속 샘플, 대회 Test 세트와 동일한 크기)\n",
      "\n",
      "📊 평가 데이터 분포:\n",
      "   성별 분포: {'F': 50, 'M': 50}\n",
      "   연령 분포: 평균 60.7세 (범위: 20-91)\n",
      "\n",
      "2. TaskB 처리기 초기화 (EXAONE 모델)...\n",
      "3. AI 예측 생성 중 (API 제한 준수)...\n",
      "   배치 1/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/13 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/13 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 894.6초)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 435\u001b[0m\n\u001b[1;32m    432\u001b[0m TEST_CSV_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/taskB_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Test 데이터 파일\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# 대회 정확한 조건으로 Test 데이터 평가 실행\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m exact_competition_evaluation(\n\u001b[1;32m    436\u001b[0m     train_csv_path\u001b[38;5;241m=\u001b[39mTEST_CSV_PATH,\n\u001b[1;32m    437\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mAPI_KEY\n\u001b[1;32m    438\u001b[0m )\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎉 TaskB Test 데이터 평가 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m최종 예상 점수: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/5.000 점\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 305\u001b[0m, in \u001b[0;36mexact_competition_evaluation\u001b[0;34m(train_csv_path, api_key)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m예측 생성 완료 (총 소요 시간: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m초)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# 7. 정답 데이터 준비\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m references \u001b[38;5;241m=\u001b[39m eval_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m4. 대회 BERTScore 계산 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon/lib/python3.10/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/datathon/lib/python3.10/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'target'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (정확히 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (정확히 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor (EXAONE 모델 사용)\n",
    "class TaskBProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=2000,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 15+ years of experience. Generate a precise and concise IMPRESSION from the given FINDINGS.\n",
    "\n",
    "CLINICAL GUIDELINES:\n",
    "- Focus on clinically significant findings first\n",
    "- Use standard radiologic terminology  \n",
    "- Be concise but complete (typically 20-80 words)\n",
    "- Number multiple distinct findings (1., 2., 3.)\n",
    "- Maintain consistent diagnostic standards regardless of patient demographics\n",
    "- Include follow-up recommendations only if clinically indicated\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "CHEST X-RAY:\n",
    "FINDINGS: Lung volumes are low. The cardiac silhouette is unremarkable. The pulmonary vasculature is normal. There is no pleural effusion or pneumothorax. No focal consolidation is identified.\n",
    "IMPRESSION: No acute intrathoracic abnormality.\n",
    "\n",
    "CT HEAD:\n",
    "FINDINGS: There is no acute hemorrhage or large vascular territorial infarction. Extensive periventricular white matter hypodensities are consistent with chronic small vessel ischemic disease. Ventricles are normal in size.\n",
    "IMPRESSION: 1. No acute intracranial abnormality. 2. Chronic small vessel ischemic disease.\n",
    "\n",
    "CT ABDOMEN/PELVIS:\n",
    "FINDINGS: The liver shows multiple hypodense lesions throughout both lobes, the largest measuring 3.2 cm in the right lobe. No significant lymphadenopathy. Bowel and bladder are unremarkable.\n",
    "IMPRESSION: 1. Multiple hepatic hypodense lesions concerning for metastases. 2. Recommend MRI for further characterization.\n",
    "\n",
    "ULTRASOUND:\n",
    "FINDINGS: The gallbladder wall is thickened measuring 6 mm with pericholecystic fluid. Multiple echogenic foci with posterior shadowing consistent with cholelithiasis.\n",
    "IMPRESSION: Acute cholecystitis with cholelithiasis.\n",
    "\n",
    "Now generate IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        radiology_text = data['radiology report']\n",
    "        \n",
    "        if 'FINDINGS:' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS:')[1]\n",
    "            if 'IMPRESSION:' in findings:\n",
    "                findings = findings.split('IMPRESSION:')[0]\n",
    "            findings_text = findings.strip()\n",
    "        elif 'FINDINGS' in radiology_text:\n",
    "            findings = radiology_text.split('FINDINGS')[1]\n",
    "            if 'IMPRESSION' in findings:\n",
    "                findings = findings.split('IMPRESSION')[0]\n",
    "            findings_text = findings.strip()\n",
    "        else:\n",
    "            findings_text = radiology_text\n",
    "        \n",
    "        findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "        findings_text = re.sub(r'\\b___\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\bDLP.*?mGy-cm\\b', '', findings_text)\n",
    "        findings_text = re.sub(r'\\s+', ' ', findings_text)\n",
    "        \n",
    "        return {'user_input': findings_text.strip()}\n",
    "\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes_to_remove = ['IMPRESSION:', 'Impression:', 'impression:']\n",
    "        for prefix in prefixes_to_remove:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "        \n",
    "        if len(sentences) > 2:\n",
    "            numbered = []\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                if sentence:\n",
    "                    if not re.match(r'^\\d+\\.', sentence):\n",
    "                        numbered.append(f\"{i+1}. {sentence}\")\n",
    "                    else:\n",
    "                        numbered.append(sentence)\n",
    "            result = '. '.join(numbered)\n",
    "        else:\n",
    "            result = '. '.join(sentences)\n",
    "        \n",
    "        words = result.split()\n",
    "        if len(words) > 100:\n",
    "            key_terms = ['normal', 'abnormal', 'acute', 'chronic', 'mass', 'lesion',\n",
    "                        'hemorrhage', 'infarction', 'effusion', 'pneumonia', 'consolidation',\n",
    "                        'atelectasis', 'unremarkable', 'significant']\n",
    "            \n",
    "            sentences = result.split('.')\n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(term in sentence.lower() for term in key_terms) or len(important_sentences) < 2:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(important_sentences) >= 2:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        corrections = {\n",
    "            'infiltration': 'consolidation',\n",
    "            'fluid in lungs': 'pulmonary edema', \n",
    "            'broken bone': 'fracture',\n",
    "            'swelling': 'edema'\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# 대회와 정확히 동일한 평가 함수\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 조건과 정확히 동일한 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 전체 Test 데이터 로드 (대회와 동일)\n",
    "    print(\"1. 전체 Test 데이터 로드 중...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    total_samples = len(test_df)\n",
    "    print(f\"전체 Test 데이터: {total_samples:,}개\")\n",
    "    \n",
    "    # 2. 대회에서 사용할 평가 샘플 크기 결정 (실제 Test 세트 크기와 유사하게)\n",
    "    # Test 1: 300건, Test 2: 300건이므로 300개로 평가\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. 연속된 샘플 사용 (대회에서는 특정 Test 세트를 사용하므로 bias 없는 연속 샘플)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # 처음 300개 사용\n",
    "    print(f\"평가 샘플: {eval_samples}개 (연속 샘플, 대회 Test 세트와 동일한 크기)\")\n",
    "    \n",
    "    # 4. 데이터 분포 확인\n",
    "    print(f\"\\n📊 평가 데이터 분포:\")\n",
    "    print(f\"   성별 분포: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   연령 분포: 평균 {eval_df['anchor_age'].mean():.1f}세 (범위: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB 처리기 초기화\n",
    "    print(\"\\n2. TaskB 처리기 초기화 (EXAONE 모델)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. 예측 생성 (대회와 동일한 배치 크기)\n",
    "    print(\"3. AI 예측 생성 중 (API 제한 준수)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # 대회 API 제한 준수 (1분당 10건)\n",
    "    results = []\n",
    "    batch_size = 8  # 안전 마진\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 7. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\n",
    "    print(\"\\n4. 대회 BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. 대회 제공 공정성 지표 계산 (정확히 동일한 설정)\n",
    "    print(\"5. 대회 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. 정량 평가 점수 계산 (대회 기준)\n",
    "    print(f\"\\n🏆 대회 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (3점 만점)\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # 공정성 점수 (2점 만점)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/2.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/5.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    else:\n",
    "        grade = \"D급 (미흡)\"\n",
    "        recommendation = \"대폭 개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 13. 샘플 결과 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 2개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행 (taskB_test.csv 사용)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"./data/taskB_test.csv\"  # Test 데이터 파일\n",
    "\n",
    "# 대회 정확한 조건으로 Test 데이터 평가 실행\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TaskB Test 데이터 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {test_results['total_score']:.3f}/5.000 점\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811cf8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
