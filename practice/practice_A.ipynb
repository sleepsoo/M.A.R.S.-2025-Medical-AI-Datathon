{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6972d7c6",
   "metadata": {},
   "source": [
    "## Few-shotìš© ë°ì´í„° ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011074f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ë°ì´í„°: 1000ê°œ\n",
      "ìœ íš¨ ë°ì´í„°: 1000ê°œ\n",
      "\n",
      "ðŸ“Š Task A ê¸°ë³¸ í†µê³„:\n",
      "Medical Record ê¸¸ì´: í‰ê·  6192ìž (ë²”ìœ„: 984-32945)\n",
      "Target ê¸¸ì´: í‰ê·  2228ìž (ë²”ìœ„: 76-14062)\n",
      "Medical Record ë‹¨ì–´ìˆ˜: í‰ê·  890ê°œ\n",
      "Target ë‹¨ì–´ìˆ˜: í‰ê·  346ê°œ\n",
      "\n",
      "=== Task A ê³ í’ˆì§ˆ ì˜ˆì‹œ ===\n",
      "=== ì˜ˆì‹œ 1 (ì ìˆ˜: 20) ===\n",
      "ìš”ì•½ ë‹¨ì–´ìˆ˜: 257, ì••ì¶•ë¹„: 0.354\n",
      "Brief Hospital Course: Ms. ___ was admitted to the gynecologic oncology service \n",
      "after undergoing diagnostic laparoscopy converted to exploratory \n",
      "laparotomy, total abdominal hysterectomy, bilateral \n",
      "salpingo-oophrectomy, o...\n",
      "--------------------------------------------------------------------------------\n",
      "=== ì˜ˆì‹œ 2 (ì ìˆ˜: 20) ===\n",
      "ìš”ì•½ ë‹¨ì–´ìˆ˜: 465, ì••ì¶•ë¹„: 0.271\n",
      "Brief Hospital Course: ___ y/o man with opioid use disorder, hepatitis C recently \n",
      "hospitalized ___ for MSSA tricuspid endocarditis who left AMA \n",
      "and self-discontinued antibiotics who presented to ___ \n",
      "with fever 103.2 and ...\n",
      "--------------------------------------------------------------------------------\n",
      "=== ì˜ˆì‹œ 3 (ì ìˆ˜: 19) ===\n",
      "ìš”ì•½ ë‹¨ì–´ìˆ˜: 316, ì••ì¶•ë¹„: 0.320\n",
      "Brief Hospital Course: He was admitted to the cardiology service and remained chest \n",
      "pain free. He underwent routine preoperative testing and \n",
      "evaluation. He developed early signs of gout flare in the right \n",
      "and left hallux...\n",
      "--------------------------------------------------------------------------------\n",
      "=== ì˜ˆì‹œ 4 (ì ìˆ˜: 19) ===\n",
      "ìš”ì•½ ë‹¨ì–´ìˆ˜: 336, ì••ì¶•ë¹„: 0.370\n",
      "Brief Hospital Course: ___ with history of severe AS and atrial fibrillation (on \n",
      "metoprolol and warfarin) admitted with several weeks of \n",
      "progressive bilateral lower extremity edema consistent with \n",
      "exacerbation of diastol...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Task A íŒ¨í„´ ë¶„ì„ ===\n",
      "admission_types: {'URGENT': {'count': 58, 'avg_target_words': np.float64(382.5344827586207)}, 'EW EMER.': {'count': 307, 'avg_target_words': np.float64(398.22801302931595)}, 'OBSERVATION ADMIT': {'count': 521, 'avg_target_words': np.float64(337.3282149712092)}, 'EU OBSERVATION': {'count': 86, 'avg_target_words': np.float64(215.9418604651163)}, 'DIRECT OBSERVATION': {'count': 18, 'avg_target_words': np.float64(269.72222222222223)}, 'DIRECT EMER.': {'count': 9, 'avg_target_words': np.float64(248.55555555555554)}, 'ELECTIVE': {'count': 1, 'avg_target_words': np.float64(153.0)}}\n",
      "age_groups: {'Senior': {'count': 426, 'avg_target_words': np.float64(337.85915492957747)}, 'Elderly': {'count': 352, 'avg_target_words': np.float64(367.25852272727275)}, 'Middle': {'count': 166, 'avg_target_words': np.float64(326.93975903614455)}, 'Young': {'count': 56, 'avg_target_words': np.float64(330.92857142857144)}}\n",
      "\n",
      "=== SIMPLE Task A ì˜ˆì‹œ ===\n",
      "í‰ê·  ìš”ì•½ ë‹¨ì–´ìˆ˜: 228\n",
      "ë³µìž¡ë„ ì ìˆ˜: 7.90\n",
      "\n",
      "=== MEDIUM Task A ì˜ˆì‹œ ===\n",
      "í‰ê·  ìš”ì•½ ë‹¨ì–´ìˆ˜: 246\n",
      "ë³µìž¡ë„ ì ìˆ˜: 9.42\n",
      "\n",
      "=== COMPLEX Task A ì˜ˆì‹œ ===\n",
      "í‰ê·  ìš”ì•½ ë‹¨ì–´ìˆ˜: 406\n",
      "ë³µìž¡ë„ ì ìˆ˜: 13.65\n",
      "\n",
      "=== êµ¬ì¡°í™”ëœ Task A ì˜ˆì‹œ ===\n",
      "     structure_score  target_words\n",
      "233                6           721\n",
      "316                6          1058\n",
      "619                6          1652\n",
      "632                6          1638\n",
      "767                6           338\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_taskA_comprehensive(train_csv_path: str):\n",
    "    \"\"\"Task A ì¢…í•© ë°ì´í„° ë¶„ì„ - ì—ëŸ¬ ì•ˆì „ ë²„ì „\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    print(f\"ì „ì²´ ë°ì´í„°: {len(df)}ê°œ\")\n",
    "    \n",
    "    # ê¸°ë³¸ ì •ì œ\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    print(f\"ìœ íš¨ ë°ì´í„°: {len(df)}ê°œ\")\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„\n",
    "    df['medical_record_length'] = df['medical record'].str.len()\n",
    "    df['target_length'] = df['target'].str.len()\n",
    "    df['medical_record_words'] = df['medical record'].str.split().str.len()\n",
    "    df['target_words'] = df['target'].str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Task A ê¸°ë³¸ í†µê³„:\")\n",
    "    print(f\"Medical Record ê¸¸ì´: í‰ê·  {df['medical_record_length'].mean():.0f}ìž (ë²”ìœ„: {df['medical_record_length'].min()}-{df['medical_record_length'].max()})\")\n",
    "    print(f\"Target ê¸¸ì´: í‰ê·  {df['target_length'].mean():.0f}ìž (ë²”ìœ„: {df['target_length'].min()}-{df['target_length'].max()})\")\n",
    "    print(f\"Medical Record ë‹¨ì–´ìˆ˜: í‰ê·  {df['medical_record_words'].mean():.0f}ê°œ\")\n",
    "    print(f\"Target ë‹¨ì–´ìˆ˜: í‰ê·  {df['target_words'].mean():.0f}ê°œ\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_taskA_quality_examples(df, top_n=15):\n",
    "    \"\"\"Task A ê³ í’ˆì§ˆ Brief Hospital Course ì˜ˆì‹œ ì¶”ì¶œ\"\"\"\n",
    "    \n",
    "    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "    df['quality_score'] = 0\n",
    "    \n",
    "    # 1. ì ì ˆí•œ ìš”ì•½ ê¸¸ì´ (200-800ë‹¨ì–´ê°€ ìµœì )\n",
    "    optimal_length = (df['target_words'] >= 150) & (df['target_words'] <= 600)\n",
    "    df.loc[optimal_length, 'quality_score'] += 5\n",
    "    \n",
    "    # 2. ì••ì¶•ìœ¨ (ì˜ë£Œ ê¸°ë¡ ëŒ€ë¹„ ì ì ˆí•œ ìš”ì•½)\n",
    "    df['compression_ratio'] = df['target_words'] / df['medical_record_words']\n",
    "    good_compression = (df['compression_ratio'] >= 0.1) & (df['compression_ratio'] <= 0.4)\n",
    "    df.loc[good_compression, 'quality_score'] += 3\n",
    "    \n",
    "    # 3. ì˜ë£Œ í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨\n",
    "    medical_keywords = ['diagnosis', 'treatment', 'admitted', 'discharged', 'improved', \n",
    "                       'stable', 'patient', 'hospital', 'condition', 'therapy', 'medication']\n",
    "    for keyword in medical_keywords:\n",
    "        df.loc[df['target'].str.lower().str.contains(keyword, na=False), 'quality_score'] += 1\n",
    "    \n",
    "    # 4. êµ¬ì¡°í™”ëœ í˜•íƒœ (ë¬¸ë‹¨ êµ¬ë¶„, ë²ˆí˜¸ ë§¤ê¹€)\n",
    "    structured = df['target'].str.contains(r'\\n\\n|\\d+\\.|#|\\*', na=False)\n",
    "    df.loc[structured, 'quality_score'] += 2\n",
    "    \n",
    "    # 5. ì˜ë£Œ ì„œë¹„ìŠ¤ ë° ë¶€ì„œ ì–¸ê¸‰\n",
    "    service_mentions = df['target'].str.contains('Service|Department|ICU|Emergency', na=False, case=False)\n",
    "    df.loc[service_mentions, 'quality_score'] += 2\n",
    "    \n",
    "    top_examples = df.nlargest(top_n, 'quality_score')\n",
    "    \n",
    "    return top_examples[['medical record', 'target', 'quality_score', \n",
    "                        'target_words', 'compression_ratio', 'medical_record_length']]\n",
    "\n",
    "def analyze_taskA_patterns(df):\n",
    "    \"\"\"Task A íŒ¨í„´ ë¶„ì„ (ìž…ì› ìœ í˜•, ë‚˜ì´ëŒ€, ì„±ë³„ë³„) - ì•ˆì „ ë²„ì „\"\"\"\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # ì»¬ëŸ¼ ì¡´ìž¬ í™•ì¸ í›„ ë¶„ì„\n",
    "    available_cols = df.columns.tolist()\n",
    "    \n",
    "    # ìž…ì› ìœ í˜•ë³„ ë¶„ì„\n",
    "    if 'admission_type' in available_cols:\n",
    "        try:\n",
    "            admission_analysis = {}\n",
    "            for adm_type in df['admission_type'].dropna().unique():\n",
    "                subset = df[df['admission_type'] == adm_type]\n",
    "                admission_analysis[adm_type] = {\n",
    "                    'count': len(subset),\n",
    "                    'avg_target_words': subset['target_words'].mean(),\n",
    "                }\n",
    "            patterns['admission_types'] = admission_analysis\n",
    "        except Exception as e:\n",
    "            print(f\"ìž…ì›ìœ í˜• ë¶„ì„ ì—ëŸ¬: {e}\")\n",
    "    \n",
    "    # ì—°ë ¹ëŒ€ë³„ ë¶„ì„\n",
    "    if 'anchor_age' in available_cols:\n",
    "        try:\n",
    "            # ì•ˆì „í•œ ì—°ë ¹ëŒ€ ë¶„í• \n",
    "            age_bins = [0, 30, 50, 70, 100]\n",
    "            age_labels = ['Young', 'Middle', 'Senior', 'Elderly']\n",
    "            df['age_group'] = pd.cut(df['anchor_age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "            \n",
    "            age_analysis = {}\n",
    "            for age_group in df['age_group'].dropna().unique():\n",
    "                subset = df[df['age_group'] == age_group]\n",
    "                age_analysis[str(age_group)] = {\n",
    "                    'count': len(subset),\n",
    "                    'avg_target_words': subset['target_words'].mean(),\n",
    "                }\n",
    "            patterns['age_groups'] = age_analysis\n",
    "        except Exception as e:\n",
    "            print(f\"ì—°ë ¹ëŒ€ ë¶„ì„ ì—ëŸ¬: {e}\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def extract_taskA_by_complexity(df, n_each=4):\n",
    "    \"\"\"ë³µìž¡ë„ë³„ Task A ì˜ˆì‹œ - ì•ˆì „ ë²„ì „\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # ë³µìž¡ë„ ê³„ì‚° ì‹œ 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€\n",
    "        safe_compression = df['compression_ratio'].replace([np.inf, -np.inf], np.nan).fillna(0.1)\n",
    "        \n",
    "        df['complexity_score'] = (\n",
    "            (df['medical_record_words'] / 500) * 2 +\n",
    "            (df['target_words'] / 100) * 1.5 +\n",
    "            (1 / safe_compression.clip(lower=0.01)) * 0.5  # ìµœì†Œê°’ ì œí•œ\n",
    "        )\n",
    "        \n",
    "        # 3ë¶„ìœ„ ë¶„í• \n",
    "        percentiles = df['complexity_score'].quantile([0.33, 0.67]).values\n",
    "        \n",
    "        simple = df[df['complexity_score'] <= percentiles[0]].copy()\n",
    "        medium = df[(df['complexity_score'] > percentiles[0]) & \n",
    "                   (df['complexity_score'] <= percentiles[1])].copy()\n",
    "        complex_cases = df[df['complexity_score'] > percentiles[1]].copy()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for category, subset in [('simple', simple), ('medium', medium), ('complex', complex_cases)]:\n",
    "            if len(subset) > 0:\n",
    "                # ì•ˆì „í•œ ì¸ë±ì‹±\n",
    "                subset = subset.reset_index(drop=True)\n",
    "                subset['category_quality'] = 0\n",
    "                \n",
    "                # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜\n",
    "                optimal_length = (subset['target_words'] >= 100) & (subset['target_words'] <= 800)\n",
    "                subset.loc[optimal_length, 'category_quality'] += 3\n",
    "                \n",
    "                good_compression = (subset['compression_ratio'] >= 0.05) & (subset['compression_ratio'] <= 0.5)\n",
    "                subset.loc[good_compression, 'category_quality'] += 2\n",
    "                \n",
    "                # ìƒìœ„ nê°œ ì„ íƒ\n",
    "                top_subset = subset.nlargest(min(n_each, len(subset)), 'category_quality')\n",
    "                results[category] = top_subset[['medical record', 'target', 'complexity_score', 'target_words']]\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ë³µìž¡ë„ ë¶„ì„ ì—ëŸ¬: {e}\")\n",
    "        return {}\n",
    "\n",
    "def analyze_common_structures(df, top_n=12):\n",
    "    \"\"\"Task A ì¼ë°˜ì ì¸ êµ¬ì¡° íŒ¨í„´ ë¶„ì„ - ì•ˆì „ ë²„ì „\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Brief Hospital Courseì˜ ì¼ë°˜ì ì¸ êµ¬ì¡° ìš”ì†Œë“¤\n",
    "        structure_elements = {\n",
    "            'chief_complaint': r'(?:chief complaint|presenting|admitted for)',\n",
    "            'diagnosis': r'(?:diagnosis|diagnosed with|found to have)',\n",
    "            'treatment': r'(?:treated with|therapy|medication|managed)',\n",
    "            'course': r'(?:hospital course|during admission|stay)',\n",
    "            'outcome': r'(?:improved|stable|discharged|condition)',\n",
    "            'follow_up': r'(?:follow.?up|outpatient|return)'\n",
    "        }\n",
    "        \n",
    "        # ê° êµ¬ì¡° ìš”ì†Œ í¬í•¨ ì—¬ë¶€ ì ìˆ˜í™”\n",
    "        df['structure_score'] = 0\n",
    "        for element, pattern in structure_elements.items():\n",
    "            try:\n",
    "                matches = df['target'].str.lower().str.contains(pattern, regex=True, na=False)\n",
    "                df.loc[matches, 'structure_score'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"êµ¬ì¡° ìš”ì†Œ '{element}' ë¶„ì„ ì—ëŸ¬: {e}\")\n",
    "        \n",
    "        # êµ¬ì¡°ê°€ ìž˜ ìž¡ížŒ ì˜ˆì‹œë“¤ ì„ ë³„\n",
    "        well_structured = df.nlargest(min(top_n, len(df)), 'structure_score')\n",
    "        \n",
    "        return well_structured[['medical record', 'target', 'structure_score', 'target_words']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"êµ¬ì¡° ë¶„ì„ ì—ëŸ¬: {e}\")\n",
    "        return df[['medical record', 'target', 'target_words']].head(top_n)\n",
    "\n",
    "# ì•ˆì „í•œ ì‹¤í–‰ ì½”ë“œ\n",
    "try:\n",
    "    # 1. ì „ì²´ ë°ì´í„° ë¶„ì„\n",
    "    df = analyze_taskA_comprehensive('./data/taskA_train.csv')\n",
    "\n",
    "    # 2. ê³ í’ˆì§ˆ ì˜ˆì‹œ ì¶”ì¶œ\n",
    "    high_quality_A = extract_taskA_quality_examples(df, top_n=12)\n",
    "    print(f\"\\n=== Task A ê³ í’ˆì§ˆ ì˜ˆì‹œ ===\")\n",
    "    for i, (_, row) in enumerate(high_quality_A.head(4).iterrows()):\n",
    "        print(f'=== ì˜ˆì‹œ {i+1} (ì ìˆ˜: {row[\"quality_score\"]}) ===')\n",
    "        print(f'ìš”ì•½ ë‹¨ì–´ìˆ˜: {row[\"target_words\"]}, ì••ì¶•ë¹„: {row[\"compression_ratio\"]:.3f}')\n",
    "        print(f'Brief Hospital Course: {row[\"target\"][:200]}...')\n",
    "        print('-' * 80)\n",
    "\n",
    "    # 3. íŒ¨í„´ ë¶„ì„\n",
    "    patterns_A = analyze_taskA_patterns(df)\n",
    "    print(f'\\n=== Task A íŒ¨í„´ ë¶„ì„ ===')\n",
    "    for pattern_type, data in patterns_A.items():\n",
    "        print(f'{pattern_type}: {data}')\n",
    "\n",
    "    # 4. ë³µìž¡ë„ë³„ ì˜ˆì‹œ\n",
    "    by_complexity_A = extract_taskA_by_complexity(df, n_each=3)\n",
    "    for complexity, examples in by_complexity_A.items():\n",
    "        print(f'\\n=== {complexity.upper()} Task A ì˜ˆì‹œ ===')\n",
    "        if len(examples) > 0:\n",
    "            print(f'í‰ê·  ìš”ì•½ ë‹¨ì–´ìˆ˜: {examples[\"target_words\"].mean():.0f}')\n",
    "            print(f'ë³µìž¡ë„ ì ìˆ˜: {examples[\"complexity_score\"].mean():.2f}')\n",
    "\n",
    "    # 5. êµ¬ì¡° íŒ¨í„´ ë¶„ì„\n",
    "    structured_examples = analyze_common_structures(df, top_n=8)\n",
    "    print(f'\\n=== êµ¬ì¡°í™”ëœ Task A ì˜ˆì‹œ ===')\n",
    "    print(structured_examples[['structure_score', 'target_words']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ì „ì²´ ì‹¤í–‰ ì—ëŸ¬: {e}\")\n",
    "    print(\"ë°ì´í„° íŒŒì¼ ê²½ë¡œì™€ ì»¬ëŸ¼ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32b693",
   "metadata": {},
   "source": [
    "## Datathon í´ëž˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b04e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Any, List, Dict\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì²˜ë¦¬ìš©\n",
    "from langevaluate.config import ModelConfig # LLM ì„¤ì •ìš©\n",
    "from langevaluate.llmfactory import LLMFactory  # LLM íŒ©í† ë¦¬ìš©\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "class DatathonProcessor(ABC):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°í†¤ìš© AI ì²˜ë¦¬ í†µí•© í´ëž˜ìŠ¤\n",
    "    ì¿¼ë¦¬, í‰ê°€, ìž„ë² ë”©ì„ ì¼ê´„ ì²˜ë¦¬í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "    ì‚¬ìš©ìžëŠ” ì´ í´ëž˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ íŠ¹ì • ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # LLM ì„¤ì • ìƒìˆ˜ë“¤\n",
    "    \n",
    "    DEFAULT_MODEL_CONFIG = {\n",
    "        'model_name': 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ',\n",
    "        'api_base': 'https://api.snubhai.org/api/v1/llm',\n",
    "        'max_tokens': 2000,\n",
    "        'seed': 777,\n",
    "        'temperature': 0,\n",
    "        'rpm': 10\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key : str,\n",
    "    ):\n",
    "        # ê¸°ë³¸ ì„¤ì • ë³µì‚¬\n",
    "        config = self.DEFAULT_MODEL_CONFIG.copy()\n",
    "        \n",
    "        # model_nameë§Œ í´ëž˜ìŠ¤ë³„ ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "        config['model_name'] = self.get_model_name()\n",
    "        \n",
    "        # LLM ì„¤ì • ìƒì„±\n",
    "        custom_config = ModelConfig(\n",
    "            model_name=config['model_name'],\n",
    "            api_base=config['api_base'],\n",
    "            api_key=api_key,\n",
    "            max_tokens=config['max_tokens'],\n",
    "            seed=config['seed'],\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "        self.llm = LLMFactory.create_llm(\n",
    "            custom_config, \n",
    "            temperature=config['temperature'], \n",
    "            rpm=config['rpm']\n",
    "        )\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "        # ê²°ê³¼ ì €ìž¥ì†Œ\n",
    "        self.results: List[str] = []\n",
    "        \n",
    "        # metric ì €ìž¥ì†Œ\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©í•  ëª¨ë¸ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        ìƒì† í´ëž˜ìŠ¤ì—ì„œ ì´ ë©”ì„œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ íŠ¹ì • ëª¨ë¸ì„ ì„¤ì •í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        return self.DEFAULT_MODEL_CONFIG['model_name']\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"ì‚¬ìš©ìžê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def postprocess_result(self, result: Any) -> str:\n",
    "        \"\"\"ë°ì´í„° í›„ì²˜ë¦¬ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def summarize(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ìž…ë ¥ê³¼ ë°°ì¹˜ ìž…ë ¥ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” í†µí•© ë©”ì„œë“œ\n",
    "        \"\"\"\n",
    "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        \n",
    "        preprocess_tasks = [self.preprocess_data(row) for _, row in data.iterrows()]\n",
    "        preprocessed_data = await tqdm_asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "        # ê°ê°ì„ ë³„ë„ì˜ coroutineìœ¼ë¡œ ì‹¤í–‰\n",
    "        tasks = [self.chain.ainvoke(vars) for vars in preprocessed_data]\n",
    "\n",
    "        # tqdm_asyncio.gatherë¡œ ë™ì‹œì— ì‹¤í–‰í•˜ë©° progress bar í‘œì‹œ\n",
    "        responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        postprocess_tasks = [self.postprocess_result(r.content) for r in responses]\n",
    "        results = await tqdm_asyncio.gather(*postprocess_tasks)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c9c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAProcessor(DatathonProcessor):\n",
    "    \"\"\"Task A: Brief Hospital Course ìž‘ì„±\"\"\"\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\"  # ì„±ëŠ¥ ìµœì í™”\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission â†’ Course â†’ Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:  # âœ… Dict ë°˜í™˜ (TaskB/Cì™€ ë™ì¼)\n",
    "        \"\"\"ì˜ë£Œ ê¸°ë¡ì„ Brief Hospital Course ìž‘ì„±ì„ ìœ„í•´ ì „ì²˜ë¦¬\"\"\"\n",
    "        import re\n",
    "        \n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        # NaN ì²˜ë¦¬\n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        # í•µì‹¬ ì˜ë£Œ ì •ë³´ ì¶”ì¶œ\n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History (ê°„ê²°)\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Physical Exam í•µì‹¬\n",
    "        if 'ADMISSION PHYSICAL EXAM:' in medical_record:\n",
    "            pe_match = re.search(r'ADMISSION PHYSICAL EXAM:\\s*(.*?)(?=DISCHARGE|Pertinent Results|$)',\n",
    "                               medical_record, re.DOTALL)\n",
    "            if pe_match:\n",
    "                pe = pe_match.group(1).strip()[:500]\n",
    "                processed_sections.append(f\"Physical Exam: {pe}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        # ìµœì¢… í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"Brief Hospital Course ê²°ê³¼ ìµœì í™”\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # \"BRIEF HOSPITAL COURSE:\" ì œê±°\n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # ë‹¨ì–´ ìˆ˜ ìµœì í™” (250-400 ëª©í‘œ)\n",
    "        words = result.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        if word_count > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # ì˜ë£Œ ìš©ì–´ í‘œì¤€í™”\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69534e42",
   "metadata": {},
   "source": [
    "## ìžì²´ í‰ê°€(EXAONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61d6313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ† Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ì‹œë®¬ë ˆì´ì…˜\n",
      "================================================================================\n",
      "1. ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "í‰ê°€ ìƒ˜í”Œ: 300ê°œ\n",
      "\n",
      "ðŸ“Š ë°ì´í„° ë¶„í¬:\n",
      "ì„±ë³„: {'M': 155, 'F': 145}\n",
      "ì—°ë ¹: í‰ê·  63.4ì„¸\n",
      "\n",
      "2. TaskA ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (EXAONE ëª¨ë¸)...\n",
      "3. Brief Hospital Course ìƒì„± ì¤‘...\n",
      "   ë°°ì¹˜ 1/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 2/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 3/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 4/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 5/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 6/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 7/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 8/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 9/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 10/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 11/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 12/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 13/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 14/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 15/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 16/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 17/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 18/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 19/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 20/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 21/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 22/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 23/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 24/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 25/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 26/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 27/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 28/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 29/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 30/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 31/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 32/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 33/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 34/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 35/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 36/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 37/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 38/38 ì²˜ë¦¬ ì¤‘...\n",
      "ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: 4946.3ì´ˆ)\n",
      "\n",
      "4. BERTScore ê³„ì‚° ì¤‘...\n",
      "5. ê³µì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ê²°ê³¼\n",
      "================================================================================\n",
      "ðŸ“Š BERTScore (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
      "   í‰ê· : 0.759906\n",
      "   í‘œì¤€íŽ¸ì°¨: 0.015933\n",
      "   ìµœê³ : 0.806447\n",
      "   ìµœì €: 0.669932\n",
      "   ì¤‘ì•™ê°’: 0.760030\n",
      "\n",
      "âš–ï¸ ê³µì •ì„± ì§€í‘œ (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
      "   ì„±ë³„ ê³µì •ì„±: 0.996993\n",
      "   ì„±ë³„ë³„ ì„±ëŠ¥: {'F': 0.761089000619691, 'M': 0.7588002897077991}\n",
      "   ì„±ë³„ ê²©ì°¨: 0.002289\n",
      "   \n",
      "   ì—°ë ¹ ê³µì •ì„±: 0.973249\n",
      "   ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥: {'10-20': 0.7758827805519104, '20-30': 0.7611913470660939, '30-40': 0.761976420879364, '40-50': 0.762029534036463, '50-60': 0.7610530118147533, '60-70': 0.7606103059271692, '70-80': 0.758561955947502, '80-90': 0.7551269863927087, '90-100': 0.7618942826986312}\n",
      "   ì—°ë ¹ ê²©ì°¨: 0.020756\n",
      "\n",
      "ðŸ† Task A ì •ëŸ‰ í‰ê°€ ì ìˆ˜\n",
      "   BERTScore: 8.940/10.000 ì \n",
      "   ê³µì •ì„± ì§€í‘œ: 6.000/6.000 ì \n",
      "   ì •ëŸ‰ ì´ì : 14.940/16.000 ì \n",
      "   ì •ëŸ‰ ë‹¬ì„±ë¥ : 93.4%\n",
      "\n",
      "ðŸŽ–ï¸ ì„±ëŠ¥ ë“±ê¸‰\n",
      "   ë“±ê¸‰: Sê¸‰ (ìµœìš°ìˆ˜)\n",
      "   ê¶Œìž¥ì‚¬í•­: ì¦‰ì‹œ ì œì¶œ ê¶Œìž¥\n",
      "\n",
      "ðŸ“ ì˜ˆì¸¡ í’ˆì§ˆ ìƒ˜í”Œ (ìƒìœ„/í•˜ìœ„ ê° 3ê°œ)\n",
      "--------------------------------------------------------------------------------\n",
      "ðŸ† ìµœê³  ì„±ëŠ¥ ìƒ˜í”Œ:\n",
      "ìƒ˜í”Œ 203 (BERTScore: 0.8064)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Admission:**\n",
      "Ms. [REDACTED], a [REDACTED] with a known history of severe aortic stenosis (AS) and atrial fibrillation managed with metoprolol and warfarin, was admitted t...\n",
      "ì •ë‹µ: ___ with history of severe AS and atrial fibrillation (on \n",
      "metoprolol and warfarin) admitted with several weeks of \n",
      "progressive bilateral lower extremity edema consistent with \n",
      "exacerbation of diastol...\n",
      "\n",
      "ìƒ˜í”Œ 250 (BERTScore: 0.8055)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "[REDACTED] male, [REDACTED] years old, with a history of myelodysplastic syndrome (MDS) currently managed on filgrastim/jakaf...\n",
      "ì •ë‹µ: Information for Outpatient Providers: ___ male with \n",
      "history of myelodysplastic syndrome currently being managed on \n",
      "filgrastim/jakafi also with secondary hemochromatosis on exjade \n",
      "and known prior sp...\n",
      "\n",
      "ìƒ˜í”Œ 189 (BERTScore: 0.8053)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Admission:**\n",
      "Ms. [REDACTED], a [REDACTED] year old female with a past medical history including atrial tachycardia diagnosed [REDACTED] years ago and asymptomatic mitral ...\n",
      "ì •ë‹µ: Ms. ___ is a ___ year old F w/ PMH atrial tachycardia on \n",
      "Verapamil XR and Metoprolol XR who presented with palpitations, \n",
      "found to be in atrial tachycardia with episodes of bradycardia \n",
      "___ overdrive...\n",
      "\n",
      "âš ï¸ ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ:\n",
      "ìƒ˜í”Œ 39 (BERTScore: 0.6699)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 68-year-old male with a history of chronic obstructive pulmonary disease (COPD) and hypertension, was admi...\n",
      "ì •ë‹µ: Discharge Worksheet - Key Information for Outpatient \n",
      "___, MD on ___ @ 1608 ...\n",
      "\n",
      "ìƒ˜í”Œ 47 (BERTScore: 0.7005)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 65-year-old male with a history of coronary artery disease (CAD), multiple sclerosis, diabetes mellitus, d...\n",
      "ì •ë‹µ: BRIEF SUMMARY\n",
      "==============\n",
      "___ with CAD s/p CABG and PCI, HFrEF (35-40%) presenting with \n",
      "fall and SDH/SAH/TBI, course complicated by newly reduced EF \n",
      "28%, hypoxic respiratory failure requiring mul...\n",
      "\n",
      "ìƒ˜í”Œ 78 (BERTScore: 0.7008)\n",
      "ì˜ˆì¸¡: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 68-year-old male with a history of hypertension and mild congestive heart failure, was admitted to the Med...\n",
      "ì •ë‹µ: TRANSITIONAL ISSUES:\n",
      "====================\n",
      "[] Please follow up Lyme serologies \n",
      "[] Repeat TFTs in 6 weeks, patient's TSH elevated to 5.8 during \n",
      "this admission\n",
      "\n",
      "DISCHARGE WEIGHT: 179.67 lbs\n",
      "DISCHARGE C...\n",
      "\n",
      "\n",
      "ðŸŽ‰ Task A ë¦¬ë”ë³´ë“œ í‰ê°€ ì™„ë£Œ!\n",
      "ìµœì¢… ì˜ˆìƒ ì ìˆ˜: 14.940/16.000 ì \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import re\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ BertScore í´ëž˜ìŠ¤ (100% ë™ì¼)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ FairnessScore í´ëž˜ìŠ¤ (100% ë™ì¼)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskA Processor (ì•žì„œ ìž‘ì„±í•œ ìµœì í™” ë²„ì „)\n",
    "class TaskAProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            # model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=1500,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission â†’ Course â†’ Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # ë‹¨ì–´ ìˆ˜ ìµœì í™”\n",
    "        words = result.split()\n",
    "        if len(words) > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # ì˜ë£Œ ìš©ì–´ í‘œì¤€í™”\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ë¦¬ë”ë³´ë“œ 100% ë™ì¼ í‰ê°€ í•¨ìˆ˜\n",
    "async def exact_taskA_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"ëŒ€íšŒ ë¦¬ë”ë³´ë“œì™€ ì •í™•ížˆ ë™ì¼í•œ Task A í‰ê°€\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ† Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "    print(\"1. ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    \n",
    "    eval_samples = min(300, len(df))\n",
    "    eval_df = df.iloc[:eval_samples].copy()\n",
    "    print(f\"í‰ê°€ ìƒ˜í”Œ: {eval_samples}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š ë°ì´í„° ë¶„í¬:\")\n",
    "    print(f\"ì„±ë³„: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"ì—°ë ¹: í‰ê·  {eval_df['anchor_age'].mean():.1f}ì„¸\")\n",
    "    \n",
    "    # 2. TaskA ì²˜ë¦¬ê¸° ì´ˆê¸°í™”\n",
    "    print(\"\\n2. TaskA ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (EXAONE ëª¨ë¸)...\")\n",
    "    processor = TaskAProcessor(api_key)\n",
    "    \n",
    "    # 3. ì˜ˆì¸¡ ìƒì„±\n",
    "    print(\"3. Brief Hospital Course ìƒì„± ì¤‘...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'medical record': row['medical record']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   ë°°ì¹˜ {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API í˜¸ì¶œ\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # í›„ì²˜ë¦¬\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API ì œí•œ ì¤€ìˆ˜\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {generation_time:.1f}ì´ˆ)\")\n",
    "    \n",
    "    # 4. ì •ë‹µ ë°ì´í„° ì¤€ë¹„\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore ê³„ì‚° (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
    "    print(\"\\n4. BERTScore ê³„ì‚° ì¤‘...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 6. ê³µì •ì„± ì§€í‘œ ê³„ì‚° (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
    "    print(\"5. ê³µì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # ì„±ë³„ ê³µì •ì„±\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # ì—°ë ¹ ê³µì •ì„±\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 7. ëŒ€íšŒ ì •í™•í•œ ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸŽ¯ Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ê²°ê³¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"ðŸ“Š BERTScore (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   í‰ê· : {bert_mean:.6f}\")\n",
    "    print(f\"   í‘œì¤€íŽ¸ì°¨: {bert_std:.6f}\")\n",
    "    print(f\"   ìµœê³ : {max(bert_scores):.6f}\")\n",
    "    print(f\"   ìµœì €: {min(bert_scores):.6f}\")\n",
    "    print(f\"   ì¤‘ì•™ê°’: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ ê³µì •ì„± ì§€í‘œ (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   ì„±ë³„ ê³µì •ì„±: {gender_fairness:.6f}\")\n",
    "    print(f\"   ì„±ë³„ë³„ ì„±ëŠ¥: {gender_stats['by_group']}\")\n",
    "    print(f\"   ì„±ë³„ ê²©ì°¨: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   ì—°ë ¹ ê³µì •ì„±: {age_fairness:.6f}\")\n",
    "    print(f\"   ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥: {age_stats['by_group']}\")\n",
    "    print(f\"   ì—°ë ¹ ê²©ì°¨: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 8. ì •ëŸ‰ í‰ê°€ ì ìˆ˜ ê³„ì‚° (Task AëŠ” 16ì  ë§Œì )\n",
    "    print(f\"\\nðŸ† Task A ì •ëŸ‰ í‰ê°€ ì ìˆ˜\")\n",
    "    \n",
    "    # BERTScore ì ìˆ˜ (10ì  ë§Œì  - 16ì ì˜ 5/8)\n",
    "    bert_score_points = min(10.0, max(0.0, (bert_mean / 0.85) * 10.0))  # ëª©í‘œ 0.85\n",
    "    \n",
    "    # ê³µì •ì„± ì ìˆ˜ (6ì  ë§Œì  - 16ì ì˜ 3/8)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(6.0, max(0.0, (fairness_avg / 0.95) * 6.0))\n",
    "    \n",
    "    # ì´ì \n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/10.000 ì \")\n",
    "    print(f\"   ê³µì •ì„± ì§€í‘œ: {fairness_points:.3f}/6.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ì´ì : {total_quantitative:.3f}/16.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ë‹¬ì„±ë¥ : {total_quantitative/16.0*100:.1f}%\")\n",
    "    \n",
    "    # 9. ì„±ëŠ¥ ë“±ê¸‰ íŒì •\n",
    "    print(f\"\\nðŸŽ–ï¸ ì„±ëŠ¥ ë“±ê¸‰\")\n",
    "    if total_quantitative >= 14.0:\n",
    "        grade = \"Sê¸‰ (ìµœìš°ìˆ˜)\"\n",
    "        recommendation = \"ì¦‰ì‹œ ì œì¶œ ê¶Œìž¥\"\n",
    "    elif total_quantitative >= 12.0:\n",
    "        grade = \"Aê¸‰ (ìš°ìˆ˜)\"\n",
    "        recommendation = \"ì œì¶œ ê¶Œìž¥\"\n",
    "    elif total_quantitative >= 10.0:\n",
    "        grade = \"Bê¸‰ (ì–‘í˜¸)\"\n",
    "        recommendation = \"ì†Œí­ ê°œì„  í›„ ì œì¶œ\"\n",
    "    else:\n",
    "        grade = \"Cê¸‰ (ë³´í†µ)\"\n",
    "        recommendation = \"ê°œì„  í•„ìš”\"\n",
    "    \n",
    "    print(f\"   ë“±ê¸‰: {grade}\")\n",
    "    print(f\"   ê¶Œìž¥ì‚¬í•­: {recommendation}\")\n",
    "    \n",
    "    # 10. ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„\n",
    "    print(f\"\\nðŸ“ ì˜ˆì¸¡ í’ˆì§ˆ ìƒ˜í”Œ (ìƒìœ„/í•˜ìœ„ ê° 3ê°œ)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"ðŸ† ìµœê³  ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:200]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"âš ï¸ ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:200]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# ì‹¤í–‰\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskA_train.csv\"\n",
    "\n",
    "# Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ì‹¤í–‰\n",
    "taskA_results = await exact_taskA_evaluation(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Task A ë¦¬ë”ë³´ë“œ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ì˜ˆìƒ ì ìˆ˜: {taskA_results['total_score']:.3f}/16.000 ì \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd48b5",
   "metadata": {},
   "source": [
    "## ìžì²´í‰ê°€(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import re\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ BertScore í´ëž˜ìŠ¤ (100% ë™ì¼)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ FairnessScore í´ëž˜ìŠ¤ (100% ë™ì¼)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskA Processor (ì•žì„œ ìž‘ì„±í•œ ìµœì í™” ë²„ì „)\n",
    "class TaskAProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            # model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=1500,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission â†’ Course â†’ Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # ë‹¨ì–´ ìˆ˜ ìµœì í™”\n",
    "        words = result.split()\n",
    "        if len(words) > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # ì˜ë£Œ ìš©ì–´ í‘œì¤€í™”\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# ë¦¬ë”ë³´ë“œ 100% ë™ì¼ í‰ê°€ í•¨ìˆ˜\n",
    "async def exact_taskA_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"ëŒ€íšŒ ë¦¬ë”ë³´ë“œì™€ ì •í™•ížˆ ë™ì¼í•œ Task A í‰ê°€\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ðŸ† Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ì‹œë®¬ë ˆì´ì…˜\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "    print(\"1. ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    \n",
    "    eval_samples = min(300, len(df))\n",
    "    eval_df = df.iloc[:eval_samples].copy()\n",
    "    print(f\"í‰ê°€ ìƒ˜í”Œ: {eval_samples}ê°œ\")\n",
    "    \n",
    "    print(f\"\\nðŸ“Š ë°ì´í„° ë¶„í¬:\")\n",
    "    print(f\"ì„±ë³„: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"ì—°ë ¹: í‰ê·  {eval_df['anchor_age'].mean():.1f}ì„¸\")\n",
    "    \n",
    "    # 2. TaskA ì²˜ë¦¬ê¸° ì´ˆê¸°í™”\n",
    "    print(\"\\n2. TaskA ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (EXAONE ëª¨ë¸)...\")\n",
    "    processor = TaskAProcessor(api_key)\n",
    "    \n",
    "    # 3. ì˜ˆì¸¡ ìƒì„±\n",
    "    print(\"3. Brief Hospital Course ìƒì„± ì¤‘...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'medical record': row['medical record']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   ë°°ì¹˜ {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API í˜¸ì¶œ\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # í›„ì²˜ë¦¬\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API ì œí•œ ì¤€ìˆ˜\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {generation_time:.1f}ì´ˆ)\")\n",
    "    \n",
    "    # 4. ì •ë‹µ ë°ì´í„° ì¤€ë¹„\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore ê³„ì‚° (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
    "    print(\"\\n4. BERTScore ê³„ì‚° ì¤‘...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 6. ê³µì •ì„± ì§€í‘œ ê³„ì‚° (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
    "    print(\"5. ê³µì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # ì„±ë³„ ê³µì •ì„±\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # ì—°ë ¹ ê³µì •ì„±\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 7. ëŒ€íšŒ ì •í™•í•œ ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸŽ¯ Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ê²°ê³¼\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"ðŸ“Š BERTScore (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   í‰ê· : {bert_mean:.6f}\")\n",
    "    print(f\"   í‘œì¤€íŽ¸ì°¨: {bert_std:.6f}\")\n",
    "    print(f\"   ìµœê³ : {max(bert_scores):.6f}\")\n",
    "    print(f\"   ìµœì €: {min(bert_scores):.6f}\")\n",
    "    print(f\"   ì¤‘ì•™ê°’: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ ê³µì •ì„± ì§€í‘œ (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   ì„±ë³„ ê³µì •ì„±: {gender_fairness:.6f}\")\n",
    "    print(f\"   ì„±ë³„ë³„ ì„±ëŠ¥: {gender_stats['by_group']}\")\n",
    "    print(f\"   ì„±ë³„ ê²©ì°¨: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   ì—°ë ¹ ê³µì •ì„±: {age_fairness:.6f}\")\n",
    "    print(f\"   ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥: {age_stats['by_group']}\")\n",
    "    print(f\"   ì—°ë ¹ ê²©ì°¨: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 8. ì •ëŸ‰ í‰ê°€ ì ìˆ˜ ê³„ì‚° (Task AëŠ” 16ì  ë§Œì )\n",
    "    print(f\"\\nðŸ† Task A ì •ëŸ‰ í‰ê°€ ì ìˆ˜\")\n",
    "    \n",
    "    # BERTScore ì ìˆ˜ (10ì  ë§Œì  - 16ì ì˜ 5/8)\n",
    "    bert_score_points = min(10.0, max(0.0, (bert_mean / 0.85) * 10.0))  # ëª©í‘œ 0.85\n",
    "    \n",
    "    # ê³µì •ì„± ì ìˆ˜ (6ì  ë§Œì  - 16ì ì˜ 3/8)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(6.0, max(0.0, (fairness_avg / 0.95) * 6.0))\n",
    "    \n",
    "    # ì´ì \n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/10.000 ì \")\n",
    "    print(f\"   ê³µì •ì„± ì§€í‘œ: {fairness_points:.3f}/6.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ì´ì : {total_quantitative:.3f}/16.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ë‹¬ì„±ë¥ : {total_quantitative/16.0*100:.1f}%\")\n",
    "    \n",
    "    # 9. ì„±ëŠ¥ ë“±ê¸‰ íŒì •\n",
    "    print(f\"\\nðŸŽ–ï¸ ì„±ëŠ¥ ë“±ê¸‰\")\n",
    "    if total_quantitative >= 14.0:\n",
    "        grade = \"Sê¸‰ (ìµœìš°ìˆ˜)\"\n",
    "        recommendation = \"ì¦‰ì‹œ ì œì¶œ ê¶Œìž¥\"\n",
    "    elif total_quantitative >= 12.0:\n",
    "        grade = \"Aê¸‰ (ìš°ìˆ˜)\"\n",
    "        recommendation = \"ì œì¶œ ê¶Œìž¥\"\n",
    "    elif total_quantitative >= 10.0:\n",
    "        grade = \"Bê¸‰ (ì–‘í˜¸)\"\n",
    "        recommendation = \"ì†Œí­ ê°œì„  í›„ ì œì¶œ\"\n",
    "    else:\n",
    "        grade = \"Cê¸‰ (ë³´í†µ)\"\n",
    "        recommendation = \"ê°œì„  í•„ìš”\"\n",
    "    \n",
    "    print(f\"   ë“±ê¸‰: {grade}\")\n",
    "    print(f\"   ê¶Œìž¥ì‚¬í•­: {recommendation}\")\n",
    "    \n",
    "    # 10. ì˜ˆì¸¡ ìƒ˜í”Œ ë¶„ì„\n",
    "    print(f\"\\nðŸ“ ì˜ˆì¸¡ í’ˆì§ˆ ìƒ˜í”Œ (ìƒìœ„/í•˜ìœ„ ê° 3ê°œ)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"ðŸ† ìµœê³  ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:200]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"âš ï¸ ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:200]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# ì‹¤í–‰\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskA_train.csv\"\n",
    "\n",
    "# Task A ë¦¬ë”ë³´ë“œ ì •í™• í‰ê°€ ì‹¤í–‰\n",
    "taskA_results = await exact_taskA_evaluation(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Task A ë¦¬ë”ë³´ë“œ í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ì˜ˆìƒ ì ìˆ˜: {taskA_results['total_score']:.3f}/16.000 ì \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
