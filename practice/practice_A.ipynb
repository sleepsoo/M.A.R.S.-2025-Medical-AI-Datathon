{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6972d7c6",
   "metadata": {},
   "source": [
    "## Few-shot용 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "011074f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 데이터: 1000개\n",
      "유효 데이터: 1000개\n",
      "\n",
      "📊 Task A 기본 통계:\n",
      "Medical Record 길이: 평균 6192자 (범위: 984-32945)\n",
      "Target 길이: 평균 2228자 (범위: 76-14062)\n",
      "Medical Record 단어수: 평균 890개\n",
      "Target 단어수: 평균 346개\n",
      "\n",
      "=== Task A 고품질 예시 ===\n",
      "=== 예시 1 (점수: 20) ===\n",
      "요약 단어수: 257, 압축비: 0.354\n",
      "Brief Hospital Course: Ms. ___ was admitted to the gynecologic oncology service \n",
      "after undergoing diagnostic laparoscopy converted to exploratory \n",
      "laparotomy, total abdominal hysterectomy, bilateral \n",
      "salpingo-oophrectomy, o...\n",
      "--------------------------------------------------------------------------------\n",
      "=== 예시 2 (점수: 20) ===\n",
      "요약 단어수: 465, 압축비: 0.271\n",
      "Brief Hospital Course: ___ y/o man with opioid use disorder, hepatitis C recently \n",
      "hospitalized ___ for MSSA tricuspid endocarditis who left AMA \n",
      "and self-discontinued antibiotics who presented to ___ \n",
      "with fever 103.2 and ...\n",
      "--------------------------------------------------------------------------------\n",
      "=== 예시 3 (점수: 19) ===\n",
      "요약 단어수: 316, 압축비: 0.320\n",
      "Brief Hospital Course: He was admitted to the cardiology service and remained chest \n",
      "pain free. He underwent routine preoperative testing and \n",
      "evaluation. He developed early signs of gout flare in the right \n",
      "and left hallux...\n",
      "--------------------------------------------------------------------------------\n",
      "=== 예시 4 (점수: 19) ===\n",
      "요약 단어수: 336, 압축비: 0.370\n",
      "Brief Hospital Course: ___ with history of severe AS and atrial fibrillation (on \n",
      "metoprolol and warfarin) admitted with several weeks of \n",
      "progressive bilateral lower extremity edema consistent with \n",
      "exacerbation of diastol...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Task A 패턴 분석 ===\n",
      "admission_types: {'URGENT': {'count': 58, 'avg_target_words': np.float64(382.5344827586207)}, 'EW EMER.': {'count': 307, 'avg_target_words': np.float64(398.22801302931595)}, 'OBSERVATION ADMIT': {'count': 521, 'avg_target_words': np.float64(337.3282149712092)}, 'EU OBSERVATION': {'count': 86, 'avg_target_words': np.float64(215.9418604651163)}, 'DIRECT OBSERVATION': {'count': 18, 'avg_target_words': np.float64(269.72222222222223)}, 'DIRECT EMER.': {'count': 9, 'avg_target_words': np.float64(248.55555555555554)}, 'ELECTIVE': {'count': 1, 'avg_target_words': np.float64(153.0)}}\n",
      "age_groups: {'Senior': {'count': 426, 'avg_target_words': np.float64(337.85915492957747)}, 'Elderly': {'count': 352, 'avg_target_words': np.float64(367.25852272727275)}, 'Middle': {'count': 166, 'avg_target_words': np.float64(326.93975903614455)}, 'Young': {'count': 56, 'avg_target_words': np.float64(330.92857142857144)}}\n",
      "\n",
      "=== SIMPLE Task A 예시 ===\n",
      "평균 요약 단어수: 228\n",
      "복잡도 점수: 7.90\n",
      "\n",
      "=== MEDIUM Task A 예시 ===\n",
      "평균 요약 단어수: 246\n",
      "복잡도 점수: 9.42\n",
      "\n",
      "=== COMPLEX Task A 예시 ===\n",
      "평균 요약 단어수: 406\n",
      "복잡도 점수: 13.65\n",
      "\n",
      "=== 구조화된 Task A 예시 ===\n",
      "     structure_score  target_words\n",
      "233                6           721\n",
      "316                6          1058\n",
      "619                6          1652\n",
      "632                6          1638\n",
      "767                6           338\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_taskA_comprehensive(train_csv_path: str):\n",
    "    \"\"\"Task A 종합 데이터 분석 - 에러 안전 버전\"\"\"\n",
    "    \n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    print(f\"전체 데이터: {len(df)}개\")\n",
    "    \n",
    "    # 기본 정제\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    print(f\"유효 데이터: {len(df)}개\")\n",
    "    \n",
    "    # 텍스트 길이 분석\n",
    "    df['medical_record_length'] = df['medical record'].str.len()\n",
    "    df['target_length'] = df['target'].str.len()\n",
    "    df['medical_record_words'] = df['medical record'].str.split().str.len()\n",
    "    df['target_words'] = df['target'].str.split().str.len()\n",
    "    \n",
    "    print(f\"\\n📊 Task A 기본 통계:\")\n",
    "    print(f\"Medical Record 길이: 평균 {df['medical_record_length'].mean():.0f}자 (범위: {df['medical_record_length'].min()}-{df['medical_record_length'].max()})\")\n",
    "    print(f\"Target 길이: 평균 {df['target_length'].mean():.0f}자 (범위: {df['target_length'].min()}-{df['target_length'].max()})\")\n",
    "    print(f\"Medical Record 단어수: 평균 {df['medical_record_words'].mean():.0f}개\")\n",
    "    print(f\"Target 단어수: 평균 {df['target_words'].mean():.0f}개\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_taskA_quality_examples(df, top_n=15):\n",
    "    \"\"\"Task A 고품질 Brief Hospital Course 예시 추출\"\"\"\n",
    "    \n",
    "    # 품질 점수 계산\n",
    "    df['quality_score'] = 0\n",
    "    \n",
    "    # 1. 적절한 요약 길이 (200-800단어가 최적)\n",
    "    optimal_length = (df['target_words'] >= 150) & (df['target_words'] <= 600)\n",
    "    df.loc[optimal_length, 'quality_score'] += 5\n",
    "    \n",
    "    # 2. 압축율 (의료 기록 대비 적절한 요약)\n",
    "    df['compression_ratio'] = df['target_words'] / df['medical_record_words']\n",
    "    good_compression = (df['compression_ratio'] >= 0.1) & (df['compression_ratio'] <= 0.4)\n",
    "    df.loc[good_compression, 'quality_score'] += 3\n",
    "    \n",
    "    # 3. 의료 핵심 키워드 포함\n",
    "    medical_keywords = ['diagnosis', 'treatment', 'admitted', 'discharged', 'improved', \n",
    "                       'stable', 'patient', 'hospital', 'condition', 'therapy', 'medication']\n",
    "    for keyword in medical_keywords:\n",
    "        df.loc[df['target'].str.lower().str.contains(keyword, na=False), 'quality_score'] += 1\n",
    "    \n",
    "    # 4. 구조화된 형태 (문단 구분, 번호 매김)\n",
    "    structured = df['target'].str.contains(r'\\n\\n|\\d+\\.|#|\\*', na=False)\n",
    "    df.loc[structured, 'quality_score'] += 2\n",
    "    \n",
    "    # 5. 의료 서비스 및 부서 언급\n",
    "    service_mentions = df['target'].str.contains('Service|Department|ICU|Emergency', na=False, case=False)\n",
    "    df.loc[service_mentions, 'quality_score'] += 2\n",
    "    \n",
    "    top_examples = df.nlargest(top_n, 'quality_score')\n",
    "    \n",
    "    return top_examples[['medical record', 'target', 'quality_score', \n",
    "                        'target_words', 'compression_ratio', 'medical_record_length']]\n",
    "\n",
    "def analyze_taskA_patterns(df):\n",
    "    \"\"\"Task A 패턴 분석 (입원 유형, 나이대, 성별별) - 안전 버전\"\"\"\n",
    "    \n",
    "    patterns = {}\n",
    "    \n",
    "    # 컬럼 존재 확인 후 분석\n",
    "    available_cols = df.columns.tolist()\n",
    "    \n",
    "    # 입원 유형별 분석\n",
    "    if 'admission_type' in available_cols:\n",
    "        try:\n",
    "            admission_analysis = {}\n",
    "            for adm_type in df['admission_type'].dropna().unique():\n",
    "                subset = df[df['admission_type'] == adm_type]\n",
    "                admission_analysis[adm_type] = {\n",
    "                    'count': len(subset),\n",
    "                    'avg_target_words': subset['target_words'].mean(),\n",
    "                }\n",
    "            patterns['admission_types'] = admission_analysis\n",
    "        except Exception as e:\n",
    "            print(f\"입원유형 분석 에러: {e}\")\n",
    "    \n",
    "    # 연령대별 분석\n",
    "    if 'anchor_age' in available_cols:\n",
    "        try:\n",
    "            # 안전한 연령대 분할\n",
    "            age_bins = [0, 30, 50, 70, 100]\n",
    "            age_labels = ['Young', 'Middle', 'Senior', 'Elderly']\n",
    "            df['age_group'] = pd.cut(df['anchor_age'], bins=age_bins, labels=age_labels, include_lowest=True)\n",
    "            \n",
    "            age_analysis = {}\n",
    "            for age_group in df['age_group'].dropna().unique():\n",
    "                subset = df[df['age_group'] == age_group]\n",
    "                age_analysis[str(age_group)] = {\n",
    "                    'count': len(subset),\n",
    "                    'avg_target_words': subset['target_words'].mean(),\n",
    "                }\n",
    "            patterns['age_groups'] = age_analysis\n",
    "        except Exception as e:\n",
    "            print(f\"연령대 분석 에러: {e}\")\n",
    "    \n",
    "    return patterns\n",
    "\n",
    "def extract_taskA_by_complexity(df, n_each=4):\n",
    "    \"\"\"복잡도별 Task A 예시 - 안전 버전\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 복잡도 계산 시 0으로 나누기 방지\n",
    "        safe_compression = df['compression_ratio'].replace([np.inf, -np.inf], np.nan).fillna(0.1)\n",
    "        \n",
    "        df['complexity_score'] = (\n",
    "            (df['medical_record_words'] / 500) * 2 +\n",
    "            (df['target_words'] / 100) * 1.5 +\n",
    "            (1 / safe_compression.clip(lower=0.01)) * 0.5  # 최소값 제한\n",
    "        )\n",
    "        \n",
    "        # 3분위 분할\n",
    "        percentiles = df['complexity_score'].quantile([0.33, 0.67]).values\n",
    "        \n",
    "        simple = df[df['complexity_score'] <= percentiles[0]].copy()\n",
    "        medium = df[(df['complexity_score'] > percentiles[0]) & \n",
    "                   (df['complexity_score'] <= percentiles[1])].copy()\n",
    "        complex_cases = df[df['complexity_score'] > percentiles[1]].copy()\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for category, subset in [('simple', simple), ('medium', medium), ('complex', complex_cases)]:\n",
    "            if len(subset) > 0:\n",
    "                # 안전한 인덱싱\n",
    "                subset = subset.reset_index(drop=True)\n",
    "                subset['category_quality'] = 0\n",
    "                \n",
    "                # 기본 품질 점수\n",
    "                optimal_length = (subset['target_words'] >= 100) & (subset['target_words'] <= 800)\n",
    "                subset.loc[optimal_length, 'category_quality'] += 3\n",
    "                \n",
    "                good_compression = (subset['compression_ratio'] >= 0.05) & (subset['compression_ratio'] <= 0.5)\n",
    "                subset.loc[good_compression, 'category_quality'] += 2\n",
    "                \n",
    "                # 상위 n개 선택\n",
    "                top_subset = subset.nlargest(min(n_each, len(subset)), 'category_quality')\n",
    "                results[category] = top_subset[['medical record', 'target', 'complexity_score', 'target_words']]\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"복잡도 분석 에러: {e}\")\n",
    "        return {}\n",
    "\n",
    "def analyze_common_structures(df, top_n=12):\n",
    "    \"\"\"Task A 일반적인 구조 패턴 분석 - 안전 버전\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Brief Hospital Course의 일반적인 구조 요소들\n",
    "        structure_elements = {\n",
    "            'chief_complaint': r'(?:chief complaint|presenting|admitted for)',\n",
    "            'diagnosis': r'(?:diagnosis|diagnosed with|found to have)',\n",
    "            'treatment': r'(?:treated with|therapy|medication|managed)',\n",
    "            'course': r'(?:hospital course|during admission|stay)',\n",
    "            'outcome': r'(?:improved|stable|discharged|condition)',\n",
    "            'follow_up': r'(?:follow.?up|outpatient|return)'\n",
    "        }\n",
    "        \n",
    "        # 각 구조 요소 포함 여부 점수화\n",
    "        df['structure_score'] = 0\n",
    "        for element, pattern in structure_elements.items():\n",
    "            try:\n",
    "                matches = df['target'].str.lower().str.contains(pattern, regex=True, na=False)\n",
    "                df.loc[matches, 'structure_score'] += 1\n",
    "            except Exception as e:\n",
    "                print(f\"구조 요소 '{element}' 분석 에러: {e}\")\n",
    "        \n",
    "        # 구조가 잘 잡힌 예시들 선별\n",
    "        well_structured = df.nlargest(min(top_n, len(df)), 'structure_score')\n",
    "        \n",
    "        return well_structured[['medical record', 'target', 'structure_score', 'target_words']]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"구조 분석 에러: {e}\")\n",
    "        return df[['medical record', 'target', 'target_words']].head(top_n)\n",
    "\n",
    "# 안전한 실행 코드\n",
    "try:\n",
    "    # 1. 전체 데이터 분석\n",
    "    df = analyze_taskA_comprehensive('./data/taskA_train.csv')\n",
    "\n",
    "    # 2. 고품질 예시 추출\n",
    "    high_quality_A = extract_taskA_quality_examples(df, top_n=12)\n",
    "    print(f\"\\n=== Task A 고품질 예시 ===\")\n",
    "    for i, (_, row) in enumerate(high_quality_A.head(4).iterrows()):\n",
    "        print(f'=== 예시 {i+1} (점수: {row[\"quality_score\"]}) ===')\n",
    "        print(f'요약 단어수: {row[\"target_words\"]}, 압축비: {row[\"compression_ratio\"]:.3f}')\n",
    "        print(f'Brief Hospital Course: {row[\"target\"][:200]}...')\n",
    "        print('-' * 80)\n",
    "\n",
    "    # 3. 패턴 분석\n",
    "    patterns_A = analyze_taskA_patterns(df)\n",
    "    print(f'\\n=== Task A 패턴 분석 ===')\n",
    "    for pattern_type, data in patterns_A.items():\n",
    "        print(f'{pattern_type}: {data}')\n",
    "\n",
    "    # 4. 복잡도별 예시\n",
    "    by_complexity_A = extract_taskA_by_complexity(df, n_each=3)\n",
    "    for complexity, examples in by_complexity_A.items():\n",
    "        print(f'\\n=== {complexity.upper()} Task A 예시 ===')\n",
    "        if len(examples) > 0:\n",
    "            print(f'평균 요약 단어수: {examples[\"target_words\"].mean():.0f}')\n",
    "            print(f'복잡도 점수: {examples[\"complexity_score\"].mean():.2f}')\n",
    "\n",
    "    # 5. 구조 패턴 분석\n",
    "    structured_examples = analyze_common_structures(df, top_n=8)\n",
    "    print(f'\\n=== 구조화된 Task A 예시 ===')\n",
    "    print(structured_examples[['structure_score', 'target_words']].head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"전체 실행 에러: {e}\")\n",
    "    print(\"데이터 파일 경로와 컬럼명을 확인해주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a32b693",
   "metadata": {},
   "source": [
    "## Datathon 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26b04e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Any, List, Dict\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate  # 프롬프트 템플릿 처리용\n",
    "from langevaluate.config import ModelConfig # LLM 설정용\n",
    "from langevaluate.llmfactory import LLMFactory  # LLM 팩토리용\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "class DatathonProcessor(ABC):\n",
    "    \"\"\"\n",
    "    데이터톤용 AI 처리 통합 클래스\n",
    "    쿼리, 평가, 임베딩을 일괄 처리할 수 있습니다.\n",
    "    사용자는 이 클래스를 상속받아 특정 메서드만 구현하면 됩니다.\n",
    "    \"\"\"\n",
    "    # LLM 설정 상수들\n",
    "    \n",
    "    DEFAULT_MODEL_CONFIG = {\n",
    "        'model_name': 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ',\n",
    "        'api_base': 'https://api.snubhai.org/api/v1/llm',\n",
    "        'max_tokens': 2000,\n",
    "        'seed': 777,\n",
    "        'temperature': 0,\n",
    "        'rpm': 10\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key : str,\n",
    "    ):\n",
    "        # 기본 설정 복사\n",
    "        config = self.DEFAULT_MODEL_CONFIG.copy()\n",
    "        \n",
    "        # model_name만 클래스별 설정으로 업데이트\n",
    "        config['model_name'] = self.get_model_name()\n",
    "        \n",
    "        # LLM 설정 생성\n",
    "        custom_config = ModelConfig(\n",
    "            model_name=config['model_name'],\n",
    "            api_base=config['api_base'],\n",
    "            api_key=api_key,\n",
    "            max_tokens=config['max_tokens'],\n",
    "            seed=config['seed'],\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        # LLM 인스턴스 생성\n",
    "        self.llm = LLMFactory.create_llm(\n",
    "            custom_config, \n",
    "            temperature=config['temperature'], \n",
    "            rpm=config['rpm']\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 템플릿 설정\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "        # 결과 저장소\n",
    "        self.results: List[str] = []\n",
    "        \n",
    "        # metric 저장소\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        사용할 모델명을 반환합니다.\n",
    "        상속 클래스에서 이 메서드를 오버라이드하여 특정 모델을 설정할 수 있습니다.\n",
    "        \"\"\"\n",
    "        return self.DEFAULT_MODEL_CONFIG['model_name']\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"데이터 전처리 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"사용자가 구현해야 하는 프롬프트 템플릿 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def postprocess_result(self, result: Any) -> str:\n",
    "        \"\"\"데이터 후처리 메서드\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def summarize(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        단일 입력과 배치 입력을 모두 처리하는 통합 메서드\n",
    "        \"\"\"\n",
    "        # 데이터 전처리\n",
    "        \n",
    "        preprocess_tasks = [self.preprocess_data(row) for _, row in data.iterrows()]\n",
    "        preprocessed_data = await tqdm_asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "        # 각각을 별도의 coroutine으로 실행\n",
    "        tasks = [self.chain.ainvoke(vars) for vars in preprocessed_data]\n",
    "\n",
    "        # tqdm_asyncio.gather로 동시에 실행하며 progress bar 표시\n",
    "        responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        postprocess_tasks = [self.postprocess_result(r.content) for r in responses]\n",
    "        results = await tqdm_asyncio.gather(*postprocess_tasks)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77c9c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskAProcessor(DatathonProcessor):\n",
    "    \"\"\"Task A: Brief Hospital Course 작성\"\"\"\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\"  # 성능 최적화\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission → Course → Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:  # ✅ Dict 반환 (TaskB/C와 동일)\n",
    "        \"\"\"의료 기록을 Brief Hospital Course 작성을 위해 전처리\"\"\"\n",
    "        import re\n",
    "        \n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        # NaN 처리\n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        # 핵심 의료 정보 추출\n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History (간결)\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Physical Exam 핵심\n",
    "        if 'ADMISSION PHYSICAL EXAM:' in medical_record:\n",
    "            pe_match = re.search(r'ADMISSION PHYSICAL EXAM:\\s*(.*?)(?=DISCHARGE|Pertinent Results|$)',\n",
    "                               medical_record, re.DOTALL)\n",
    "            if pe_match:\n",
    "                pe = pe_match.group(1).strip()[:500]\n",
    "                processed_sections.append(f\"Physical Exam: {pe}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        # 최종 텍스트 구성\n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"Brief Hospital Course 결과 최적화\"\"\"\n",
    "        import re\n",
    "        \n",
    "        result = result.strip()\n",
    "        \n",
    "        # \"BRIEF HOSPITAL COURSE:\" 제거\n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 단어 수 최적화 (250-400 목표)\n",
    "        words = result.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        if word_count > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 의료 용어 표준화\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69534e42",
   "metadata": {},
   "source": [
    "## 자체 평가(EXAONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61d6313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 Task A 리더보드 정확 평가 시뮬레이션\n",
      "================================================================================\n",
      "1. 데이터 로드 중...\n",
      "평가 샘플: 300개\n",
      "\n",
      "📊 데이터 분포:\n",
      "성별: {'M': 155, 'F': 145}\n",
      "연령: 평균 63.4세\n",
      "\n",
      "2. TaskA 처리기 초기화 (EXAONE 모델)...\n",
      "3. Brief Hospital Course 생성 중...\n",
      "   배치 1/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 14/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 15/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 16/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 17/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 18/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 19/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 20/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 21/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 22/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 23/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 24/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 25/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 26/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 27/38 처리 중...\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 500 - {'success': False, 'error': 'InternalServerError', 'message': 'An unexpected error occurred'}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 28/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 29/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 30/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 31/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 32/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 33/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 34/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 35/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 36/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 37/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 38/38 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 4946.3초)\n",
      "\n",
      "4. BERTScore 계산 중...\n",
      "5. 공정성 지표 계산 중...\n",
      "\n",
      "================================================================================\n",
      "🎯 Task A 리더보드 정확 평가 결과\n",
      "================================================================================\n",
      "📊 BERTScore (대회 공식 계산)\n",
      "   평균: 0.759906\n",
      "   표준편차: 0.015933\n",
      "   최고: 0.806447\n",
      "   최저: 0.669932\n",
      "   중앙값: 0.760030\n",
      "\n",
      "⚖️ 공정성 지표 (대회 공식 계산)\n",
      "   성별 공정성: 0.996993\n",
      "   성별별 성능: {'F': 0.761089000619691, 'M': 0.7588002897077991}\n",
      "   성별 격차: 0.002289\n",
      "   \n",
      "   연령 공정성: 0.973249\n",
      "   연령대별 성능: {'10-20': 0.7758827805519104, '20-30': 0.7611913470660939, '30-40': 0.761976420879364, '40-50': 0.762029534036463, '50-60': 0.7610530118147533, '60-70': 0.7606103059271692, '70-80': 0.758561955947502, '80-90': 0.7551269863927087, '90-100': 0.7618942826986312}\n",
      "   연령 격차: 0.020756\n",
      "\n",
      "🏆 Task A 정량 평가 점수\n",
      "   BERTScore: 8.940/10.000 점\n",
      "   공정성 지표: 6.000/6.000 점\n",
      "   정량 총점: 14.940/16.000 점\n",
      "   정량 달성률: 93.4%\n",
      "\n",
      "🎖️ 성능 등급\n",
      "   등급: S급 (최우수)\n",
      "   권장사항: 즉시 제출 권장\n",
      "\n",
      "📝 예측 품질 샘플 (상위/하위 각 3개)\n",
      "--------------------------------------------------------------------------------\n",
      "🏆 최고 성능 샘플:\n",
      "샘플 203 (BERTScore: 0.8064)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Admission:**\n",
      "Ms. [REDACTED], a [REDACTED] with a known history of severe aortic stenosis (AS) and atrial fibrillation managed with metoprolol and warfarin, was admitted t...\n",
      "정답: ___ with history of severe AS and atrial fibrillation (on \n",
      "metoprolol and warfarin) admitted with several weeks of \n",
      "progressive bilateral lower extremity edema consistent with \n",
      "exacerbation of diastol...\n",
      "\n",
      "샘플 250 (BERTScore: 0.8055)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "[REDACTED] male, [REDACTED] years old, with a history of myelodysplastic syndrome (MDS) currently managed on filgrastim/jakaf...\n",
      "정답: Information for Outpatient Providers: ___ male with \n",
      "history of myelodysplastic syndrome currently being managed on \n",
      "filgrastim/jakafi also with secondary hemochromatosis on exjade \n",
      "and known prior sp...\n",
      "\n",
      "샘플 189 (BERTScore: 0.8053)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Admission:**\n",
      "Ms. [REDACTED], a [REDACTED] year old female with a past medical history including atrial tachycardia diagnosed [REDACTED] years ago and asymptomatic mitral ...\n",
      "정답: Ms. ___ is a ___ year old F w/ PMH atrial tachycardia on \n",
      "Verapamil XR and Metoprolol XR who presented with palpitations, \n",
      "found to be in atrial tachycardia with episodes of bradycardia \n",
      "___ overdrive...\n",
      "\n",
      "⚠️ 최저 성능 샘플:\n",
      "샘플 39 (BERTScore: 0.6699)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 68-year-old male with a history of chronic obstructive pulmonary disease (COPD) and hypertension, was admi...\n",
      "정답: Discharge Worksheet - Key Information for Outpatient \n",
      "___, MD on ___ @ 1608 ...\n",
      "\n",
      "샘플 47 (BERTScore: 0.7005)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 65-year-old male with a history of coronary artery disease (CAD), multiple sclerosis, diabetes mellitus, d...\n",
      "정답: BRIEF SUMMARY\n",
      "==============\n",
      "___ with CAD s/p CABG and PCI, HFrEF (35-40%) presenting with \n",
      "fall and SDH/SAH/TBI, course complicated by newly reduced EF \n",
      "28%, hypoxic respiratory failure requiring mul...\n",
      "\n",
      "샘플 78 (BERTScore: 0.7008)\n",
      "예측: **BRIEF HOSPITAL COURSE:**\n",
      "\n",
      "**Patient Presentation and Admission Reason:**\n",
      "Mr. [Last Name], a 68-year-old male with a history of hypertension and mild congestive heart failure, was admitted to the Med...\n",
      "정답: TRANSITIONAL ISSUES:\n",
      "====================\n",
      "[] Please follow up Lyme serologies \n",
      "[] Repeat TFTs in 6 weeks, patient's TSH elevated to 5.8 during \n",
      "this admission\n",
      "\n",
      "DISCHARGE WEIGHT: 179.67 lbs\n",
      "DISCHARGE C...\n",
      "\n",
      "\n",
      "🎉 Task A 리더보드 평가 완료!\n",
      "최종 예상 점수: 14.940/16.000 점\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (100% 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (100% 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskA Processor (앞서 작성한 최적화 버전)\n",
    "class TaskAProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            # model_name=\"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=1500,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission → Course → Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 단어 수 최적화\n",
    "        words = result.split()\n",
    "        if len(words) > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 의료 용어 표준화\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# 리더보드 100% 동일 평가 함수\n",
    "async def exact_taskA_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 리더보드와 정확히 동일한 Task A 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 Task A 리더보드 정확 평가 시뮬레이션\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    print(\"1. 데이터 로드 중...\")\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    \n",
    "    eval_samples = min(300, len(df))\n",
    "    eval_df = df.iloc[:eval_samples].copy()\n",
    "    print(f\"평가 샘플: {eval_samples}개\")\n",
    "    \n",
    "    print(f\"\\n📊 데이터 분포:\")\n",
    "    print(f\"성별: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"연령: 평균 {eval_df['anchor_age'].mean():.1f}세\")\n",
    "    \n",
    "    # 2. TaskA 처리기 초기화\n",
    "    print(\"\\n2. TaskA 처리기 초기화 (EXAONE 모델)...\")\n",
    "    processor = TaskAProcessor(api_key)\n",
    "    \n",
    "    # 3. 예측 생성\n",
    "    print(\"3. Brief Hospital Course 생성 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'medical record': row['medical record']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 4. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore 계산 (대회 공식 계산)\n",
    "    print(\"\\n4. BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 6. 공정성 지표 계산 (대회 공식 계산)\n",
    "    print(\"5. 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 7. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 Task A 리더보드 정확 평가 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 8. 정량 평가 점수 계산 (Task A는 16점 만점)\n",
    "    print(f\"\\n🏆 Task A 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (10점 만점 - 16점의 5/8)\n",
    "    bert_score_points = min(10.0, max(0.0, (bert_mean / 0.85) * 10.0))  # 목표 0.85\n",
    "    \n",
    "    # 공정성 점수 (6점 만점 - 16점의 3/8)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(6.0, max(0.0, (fairness_avg / 0.95) * 6.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/10.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/6.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/16.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/16.0*100:.1f}%\")\n",
    "    \n",
    "    # 9. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 14.0:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 12.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 10.0:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    else:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 10. 예측 샘플 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 3개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:200]}...\")\n",
    "        print(f\"정답: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:200]}...\")\n",
    "        print(f\"정답: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskA_train.csv\"\n",
    "\n",
    "# Task A 리더보드 정확 평가 실행\n",
    "taskA_results = await exact_taskA_evaluation(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 Task A 리더보드 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {taskA_results['total_score']:.3f}/16.000 점\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd48b5",
   "metadata": {},
   "source": [
    "## 자체평가(Llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628a55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "import time\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (100% 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (100% 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskA Processor (앞서 작성한 최적화 버전)\n",
    "class TaskAProcessor:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        \n",
    "        config = ModelConfig(\n",
    "            # model_name=\"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\",\n",
    "            model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            api_base=\"https://api.snubhai.org/api/v1/llm\",\n",
    "            api_key=api_key,\n",
    "            max_tokens=1500,\n",
    "            seed=777,\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        self.llm = LLMFactory.create_llm(config, temperature=0, rpm=10)\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a senior attending physician with 15+ years of experience writing comprehensive Brief Hospital Course summaries. Create a professional, chronologically structured summary that captures the essential medical narrative.\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Write 250-400 words (optimal length based on successful cases)\n",
    "- Maintain chronological flow: Admission → Course → Outcome\n",
    "- Use precise medical terminology consistently\n",
    "- Include key diagnostic findings, treatments, and patient responses\n",
    "- Maintain professional tone regardless of patient demographics\n",
    "- Focus on clinically significant events and interventions\n",
    "\n",
    "STRUCTURE METHODOLOGY:\n",
    "1. Opening: Patient presentation and admission reason\n",
    "2. Initial Assessment: Key findings, diagnostics, initial diagnosis\n",
    "3. Hospital Course: Chronological treatment progression, complications\n",
    "4. Clinical Response: Patient improvement/deterioration, interventions\n",
    "5. Discharge Planning: Final status, disposition, follow-up needs\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "MEDICAL RECORD: [Complex gynecologic oncology case...]\n",
    "BRIEF HOSPITAL COURSE: Ms. ___ was admitted to the gynecologic oncology service after undergoing diagnostic laparoscopy converted to exploratory laparotomy, total abdominal hysterectomy, bilateral salpingo-oophrectomy, omentectomy, pelvic and para-aortic lymph node dissection, and tumor debulking for Stage IIIC ovarian carcinoma. Her postoperative course was complicated by prolonged ileus requiring nasogastric decompression and total parenteral nutrition. She developed a wound infection on postoperative day 5 treated with antibiotics and wound care. Patient was discharged home on postoperative day 8 in stable condition with visiting nurse services arranged.\n",
    "\n",
    "MEDICAL RECORD: [Cardiac case with preoperative evaluation...]\n",
    "BRIEF HOSPITAL COURSE: He was admitted to the cardiology service and remained chest pain free. He underwent routine preoperative testing and evaluation. He developed early signs of gout flare in the right and left hallux which was treated with colchicine and responded well. His cardiac catheterization revealed severe three-vessel coronary artery disease requiring surgical revascularization. He was medically optimized and discharged home after 3 days in stable condition with cardiothoracic surgery follow-up scheduled.\n",
    "\n",
    "Now create a Brief Hospital Course for:\n",
    "\n",
    "MEDICAL RECORD: {user_input}\n",
    "\n",
    "BRIEF HOSPITAL COURSE:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        medical_record = data['medical record']\n",
    "        \n",
    "        if pd.isna(medical_record) or not isinstance(medical_record, str):\n",
    "            return {'user_input': ''}\n",
    "        \n",
    "        processed_sections = []\n",
    "        \n",
    "        # Chief Complaint & Service\n",
    "        if 'Chief Complaint:' in medical_record:\n",
    "            cc_match = re.search(r'Chief Complaint:\\s*([^\\n]+)', medical_record)\n",
    "            if cc_match:\n",
    "                processed_sections.append(f\"Chief Complaint: {cc_match.group(1).strip()}\")\n",
    "        \n",
    "        if 'Service:' in medical_record:\n",
    "            service_match = re.search(r'Service:\\s*([^\\n]+)', medical_record)\n",
    "            if service_match:\n",
    "                processed_sections.append(f\"Service: {service_match.group(1).strip()}\")\n",
    "        \n",
    "        # History of Present Illness\n",
    "        if 'History of Present Illness:' in medical_record:\n",
    "            hpi_match = re.search(r'History of Present Illness:\\s*(.*?)(?=\\n\\n|\\nPast Medical|Physical Exam|$)', \n",
    "                                medical_record, re.DOTALL)\n",
    "            if hpi_match:\n",
    "                hpi = hpi_match.group(1).strip()[:800]\n",
    "                processed_sections.append(f\"History: {hpi}\")\n",
    "        \n",
    "        # Major Procedures\n",
    "        if 'Major Surgical or Invasive Procedure:' in medical_record:\n",
    "            proc_match = re.search(r'Major Surgical or Invasive Procedure:\\s*(.*?)(?=\\n\\n|History of Present|$)', \n",
    "                                 medical_record, re.DOTALL)\n",
    "            if proc_match:\n",
    "                proc = proc_match.group(1).strip()\n",
    "                if proc.lower() not in ['none', 'none.']:\n",
    "                    processed_sections.append(f\"Major Procedures: {proc}\")\n",
    "        \n",
    "        # Past Medical History\n",
    "        if 'Past Medical History:' in medical_record:\n",
    "            pmh_match = re.search(r'Past Medical History:\\s*(.*?)(?=\\n\\n|PAST SURGICAL|Social History|$)',\n",
    "                                medical_record, re.DOTALL)\n",
    "            if pmh_match:\n",
    "                pmh = pmh_match.group(1).strip()[:400]\n",
    "                processed_sections.append(f\"Past Medical History: {pmh}\")\n",
    "        \n",
    "        # Imaging IMPRESSION\n",
    "        impressions = re.findall(r'IMPRESSION:\\s*(.*?)(?=\\n\\n|\\n[A-Z_]|\\Z)', medical_record, re.DOTALL)\n",
    "        if impressions:\n",
    "            for i, imp in enumerate(impressions[:2]):\n",
    "                processed_sections.append(f\"Imaging {i+1}: {imp.strip()[:200]}\")\n",
    "        \n",
    "        processed_text = '\\n\\n'.join(processed_sections)\n",
    "        processed_text = re.sub(r'___+', '[REDACTED]', processed_text)\n",
    "        processed_text = re.sub(r'\\s+', ' ', processed_text)\n",
    "        processed_text = processed_text[:3000]\n",
    "        \n",
    "        return {'user_input': processed_text.strip()}\n",
    "    \n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        result = result.strip()\n",
    "        \n",
    "        prefixes = ['BRIEF HOSPITAL COURSE:', 'Brief Hospital Course:', 'brief hospital course:']\n",
    "        for prefix in prefixes:\n",
    "            if result.startswith(prefix):\n",
    "                result = result[len(prefix):].strip()\n",
    "        \n",
    "        if result and not result.endswith('.'):\n",
    "            result += '.'\n",
    "        \n",
    "        # 단어 수 최적화\n",
    "        words = result.split()\n",
    "        if len(words) > 450:\n",
    "            sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "            important_keywords = ['admitted', 'diagnosis', 'treated', 'underwent', 'developed', \n",
    "                                'improved', 'discharged', 'course', 'complication']\n",
    "            \n",
    "            important_sentences = []\n",
    "            for sentence in sentences:\n",
    "                if any(keyword in sentence.lower() for keyword in important_keywords) or len(important_sentences) < 3:\n",
    "                    important_sentences.append(sentence.strip())\n",
    "                if len(' '.join(important_sentences).split()) >= 400:\n",
    "                    break\n",
    "            \n",
    "            if important_sentences:\n",
    "                result = '. '.join(important_sentences)\n",
    "                if not result.endswith('.'):\n",
    "                    result += '.'\n",
    "        \n",
    "        # 의료 용어 표준화\n",
    "        medical_corrections = {\n",
    "            'pt ': 'patient ',\n",
    "            'w/ ': 'with ',\n",
    "            'w/o ': 'without ',\n",
    "            'h/o ': 'history of '\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in medical_corrections.items():\n",
    "            result = result.replace(wrong, correct)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# 리더보드 100% 동일 평가 함수\n",
    "async def exact_taskA_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 리더보드와 정확히 동일한 Task A 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 Task A 리더보드 정확 평가 시뮬레이션\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 데이터 로드 및 전처리\n",
    "    print(\"1. 데이터 로드 중...\")\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    df = df.dropna(subset=['medical record', 'target'])\n",
    "    \n",
    "    eval_samples = min(300, len(df))\n",
    "    eval_df = df.iloc[:eval_samples].copy()\n",
    "    print(f\"평가 샘플: {eval_samples}개\")\n",
    "    \n",
    "    print(f\"\\n📊 데이터 분포:\")\n",
    "    print(f\"성별: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"연령: 평균 {eval_df['anchor_age'].mean():.1f}세\")\n",
    "    \n",
    "    # 2. TaskA 처리기 초기화\n",
    "    print(\"\\n2. TaskA 처리기 초기화 (EXAONE 모델)...\")\n",
    "    processor = TaskAProcessor(api_key)\n",
    "    \n",
    "    # 3. 예측 생성\n",
    "    print(\"3. Brief Hospital Course 생성 중...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'medical record': row['medical record']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    results = []\n",
    "    batch_size = 8\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 4. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 5. BERTScore 계산 (대회 공식 계산)\n",
    "    print(\"\\n4. BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 6. 공정성 지표 계산 (대회 공식 계산)\n",
    "    print(\"5. 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 7. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 Task A 리더보드 정확 평가 결과\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 8. 정량 평가 점수 계산 (Task A는 16점 만점)\n",
    "    print(f\"\\n🏆 Task A 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (10점 만점 - 16점의 5/8)\n",
    "    bert_score_points = min(10.0, max(0.0, (bert_mean / 0.85) * 10.0))  # 목표 0.85\n",
    "    \n",
    "    # 공정성 점수 (6점 만점 - 16점의 3/8)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(6.0, max(0.0, (fairness_avg / 0.95) * 6.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/10.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/6.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/16.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/16.0*100:.1f}%\")\n",
    "    \n",
    "    # 9. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 14.0:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 12.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 10.0:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    else:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 10. 예측 샘플 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 3개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:200]}...\")\n",
    "        print(f\"정답: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(3):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:200]}...\")\n",
    "        print(f\"정답: {references[idx][:200]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TRAIN_CSV_PATH = \"./data/taskA_train.csv\"\n",
    "\n",
    "# Task A 리더보드 정확 평가 실행\n",
    "taskA_results = await exact_taskA_evaluation(\n",
    "    train_csv_path=TRAIN_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 Task A 리더보드 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {taskA_results['total_score']:.3f}/16.000 점\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
