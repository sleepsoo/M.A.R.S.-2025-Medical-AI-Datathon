{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dbcbe5",
   "metadata": {},
   "source": [
    "## Datathon 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ae4a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datathon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Any, List, Dict\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate  # 프롬프트 템플릿 처리용\n",
    "from langevaluate.config import ModelConfig # LLM 설정용\n",
    "from langevaluate.llmfactory import LLMFactory  # LLM 팩토리용\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "class DatathonProcessor(ABC):\n",
    "    \"\"\"\n",
    "    데이터톤용 AI 처리 통합 클래스\n",
    "    쿼리, 평가, 임베딩을 일괄 처리할 수 있습니다.\n",
    "    사용자는 이 클래스를 상속받아 특정 메서드만 구현하면 됩니다.\n",
    "    \"\"\"\n",
    "    # LLM 설정 상수들\n",
    "    \n",
    "    DEFAULT_MODEL_CONFIG = {\n",
    "        'model_name': 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ',\n",
    "        'api_base': 'https://api.snubhai.org/api/v1/llm',\n",
    "        'max_tokens': 2000,\n",
    "        'seed': 777,\n",
    "        'temperature': 0,\n",
    "        'rpm': 10\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key : str,\n",
    "    ):\n",
    "        # 기본 설정 복사\n",
    "        config = self.DEFAULT_MODEL_CONFIG.copy()\n",
    "        \n",
    "        # model_name만 클래스별 설정으로 업데이트\n",
    "        config['model_name'] = self.get_model_name()\n",
    "        \n",
    "        # LLM 설정 생성\n",
    "        custom_config = ModelConfig(\n",
    "            model_name=config['model_name'],\n",
    "            api_base=config['api_base'],\n",
    "            api_key=api_key,\n",
    "            max_tokens=config['max_tokens'],\n",
    "            seed=config['seed'],\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        # LLM 인스턴스 생성\n",
    "        self.llm = LLMFactory.create_llm(\n",
    "            custom_config, \n",
    "            temperature=config['temperature'], \n",
    "            rpm=config['rpm']\n",
    "        )\n",
    "        \n",
    "        # 프롬프트 템플릿 설정\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "        # 결과 저장소\n",
    "        self.results: List[str] = []\n",
    "        \n",
    "        # metric 저장소\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        사용할 모델명을 반환합니다.\n",
    "        상속 클래스에서 이 메서드를 오버라이드하여 특정 모델을 설정할 수 있습니다.\n",
    "        \"\"\"\n",
    "        return self.DEFAULT_MODEL_CONFIG['model_name']\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"데이터 전처리 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"사용자가 구현해야 하는 프롬프트 템플릿 메서드\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def postprocess_result(self, result: Any) -> str:\n",
    "        \"\"\"데이터 후처리 메서드\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def summarize(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        단일 입력과 배치 입력을 모두 처리하는 통합 메서드\n",
    "        \"\"\"\n",
    "        # 데이터 전처리\n",
    "        \n",
    "        preprocess_tasks = [self.preprocess_data(row) for _, row in data.iterrows()]\n",
    "        preprocessed_data = await tqdm_asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "        # 각각을 별도의 coroutine으로 실행\n",
    "        tasks = [self.chain.ainvoke(vars) for vars in preprocessed_data]\n",
    "\n",
    "        # tqdm_asyncio.gather로 동시에 실행하며 progress bar 표시\n",
    "        responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        postprocess_tasks = [self.postprocess_result(r.content) for r in responses]\n",
    "        results = await tqdm_asyncio.gather(*postprocess_tasks)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec04224",
   "metadata": {},
   "source": [
    "## 자체평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46790532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\n",
      "================================================================================\n",
      "1. 전체 Test 데이터 로드 중...\n",
      "   데이터 품질 확인 중...\n",
      "   전체 데이터: 1000개\n",
      "   NaN 값: 11개\n",
      "   유효 데이터: 989개\n",
      "평가 샘플: 300개 (연속 샘플, 대회 Test 세트와 동일한 크기)\n",
      "\n",
      "📊 평가 데이터 분포:\n",
      "   성별 분포: {'M': 154, 'F': 146}\n",
      "   연령 분포: 평균 63.6세 (범위: 19-91)\n",
      "\n",
      "2. TaskB 처리기 초기화 (Llama 모델)...\n",
      "3. AI 예측 생성 중 (API 제한 준수)...\n",
      "   배치 1/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 2/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 3/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 4/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 5/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 6/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 7/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 8/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 9/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 10/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 11/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 12/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 13/38 처리 중...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 14/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 15/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 16/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 17/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 18/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 19/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 20/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 21/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 22/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 23/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 24/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 25/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 26/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 27/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 28/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 29/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 30/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 31/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 32/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 33/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 34/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 35/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 36/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 37/38 처리 중...\n",
      "   API 제한 준수를 위해 70초 대기...\n",
      "   배치 38/38 처리 중...\n",
      "예측 생성 완료 (총 소요 시간: 5916.5초)\n",
      "\n",
      "4. 대회 BERTScore 계산 중...\n",
      "5. 대회 공정성 지표 계산 중...\n",
      "\n",
      "================================================================================\n",
      "🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\n",
      "================================================================================\n",
      "📊 BERTScore (대회 공식 계산)\n",
      "   평균: 0.791272\n",
      "   표준편차: 0.036367\n",
      "   최고: 0.930865\n",
      "   최저: 0.680474\n",
      "   중앙값: 0.790075\n",
      "\n",
      "⚖️ 공정성 지표 (대회 공식 계산)\n",
      "   성별 공정성: 0.999362\n",
      "   성별별 성능: {'F': 0.7910128080681579, 'M': 0.7915177983897073}\n",
      "   성별 격차: 0.000505\n",
      "   \n",
      "   연령 공정성: 0.974506\n",
      "   연령대별 성능: {'10-20': 0.8021212220191956, '20-30': 0.7926706110729891, '30-40': 0.7836787660916646, '40-50': 0.7966337258165533, '50-60': 0.7993657265679311, '60-70': 0.7821916560052147, '70-80': 0.7968791496753692, '80-90': 0.7921639124552409, '90-100': 0.7816723436117172}\n",
      "   연령 격차: 0.020449\n",
      "\n",
      "🏆 대회 정량 평가 점수\n",
      "   BERTScore: 2.793/3.000 점\n",
      "   공정성 지표: 2.000/2.000 점\n",
      "   정량 총점: 4.793/5.000 점\n",
      "   정량 달성률: 95.9%\n",
      "\n",
      "🎖️ 성능 등급\n",
      "   등급: S급 (최우수)\n",
      "   권장사항: 즉시 제출 권장\n",
      "\n",
      "📝 예측 품질 샘플 (상위/하위 각 2개)\n",
      "--------------------------------------------------------------------------------\n",
      "🏆 최고 성능 샘플:\n",
      "샘플 255 (BERTScore: 0.9309)\n",
      "예측: 1. Right atrial central venous catheter placement noted. 2. moderate cardiomegaly. 3. Pulmonary vascular congestion with...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Focal consolidation in the right lower lobe likely represents pneumonia.\n",
      "2. Moderate cardiomegaly with ...\n",
      "\n",
      "샘플 162 (BERTScore: 0.8998)\n",
      "예측: 1. Acute right-sided subdural hematoma (13 mm maximal thickness) with effacement of right cerebral sulci and mass effect...\n",
      "정답: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Acute right-sided subdural hematoma measuring up to 13 mm associated with 7\n",
      "mm of leftward shift of nor...\n",
      "\n",
      "⚠️ 최저 성능 샘플:\n",
      "샘플 96 (BERTScore: 0.6805)\n",
      "예측: **Retrocardiac opacity** (likely pleural thickening or small effusion. **Cardiomegaly** noted, though mediastinal silhou...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "샘플 48 (BERTScore: 0.6811)\n",
      "예측: 1. **IMPRESSION:** 1. 2. **Minimal retrocardiac streaky atelectasis** 2. 3. no pneumonia 3. 4. No large effusion or pneu...\n",
      "정답: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "\n",
      "🎉 TaskB Test 데이터 평가 완료!\n",
      "최종 예상 점수: 4.793/5.000 점\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# 대회 제공 BertScore 클래스 (정확히 동일)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# 대회 제공 FairnessScore 클래스 (정확히 동일)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor \n",
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression 요약 - 극한 최적화\"\"\"\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\"\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 20+ years of experience creating diagnostic impressions. Generate a precise, clinically actionable IMPRESSION that maximizes diagnostic clarity and conciseness.\n",
    "\n",
    "CRITICAL OPTIMIZATION FOR EVALUATION:\n",
    "- QUALITY: Capture ALL key findings with complete accuracy - miss nothing important\n",
    "- CLINICAL CLARITY: Use precise radiological terminology that clinicians can act upon immediately  \n",
    "- CONCISENESS: Zero redundancy, no verbose phrases, every word essential for diagnosis\n",
    "- ACCURACY: Only document what is explicitly stated - absolutely no assumptions or hallucinations\n",
    "\n",
    "OPTIMAL IMPRESSION STRUCTURE:\n",
    "1. Primary/acute findings first (most clinically significant)\n",
    "2. Secondary findings (supportive/chronic conditions)  \n",
    "3. Explicit negatives for critical differentials when mentioned\n",
    "4. Numbered format for multiple distinct findings (1., 2., 3.)\n",
    "\n",
    "CONCISENESS OPTIMIZATION EXAMPLES:\n",
    "\n",
    "VERBOSE: \"There appears to be some degree of mild enlargement of the cardiac silhouette that suggests possible cardiomegaly\"\n",
    "CONCISE: \"Mild cardiomegaly\"\n",
    "\n",
    "VERBOSE: \"The findings are consistent with and compatible with pneumonia involving the right lower lobe\"\n",
    "CONCISE: \"Right lower lobe pneumonia\"\n",
    "\n",
    "VERBOSE: \"No evidence of any acute fractures or significant osseous abnormalities are identified\"\n",
    "CONCISE: \"No acute fractures\"\n",
    "\n",
    "EXEMPLAR CASES FOR MAXIMUM SCORES:\n",
    "\n",
    "CT CHEST HIGH-COMPLEXITY:\n",
    "FINDINGS: Multiple bilateral pulmonary nodules, the largest measuring 2.3 cm in the right upper lobe with spiculated margins and adjacent pleural thickening. Moderate right pleural effusion with loculations. Mild mediastinal lymphadenopathy with nodes up to 1.2 cm. Left lower lobe consolidation with air bronchograms. No pericardial effusion.\n",
    "IMPRESSION: 1. Right upper lobe spiculated nodule (2.3 cm) concerning for malignancy with pleural involvement.\n",
    "2. Loculated right pleural effusion.  \n",
    "3. Mediastinal lymphadenopathy.\n",
    "4. Left lower lobe pneumonia.\n",
    "\n",
    "CHEST X-RAY OPTIMIZATION:\n",
    "FINDINGS: Heart size is enlarged. There is bilateral lower lobe airspace opacification consistent with consolidation or atelectasis. Small bilateral pleural effusions are present. The mediastinal contours appear normal. No pneumothorax is evident.\n",
    "IMPRESSION: 1. Cardiomegaly with bilateral lower lobe consolidation.\n",
    "2. Small bilateral pleural effusions.\n",
    "\n",
    "MR HEAD PRECISION:\n",
    "FINDINGS: There is a 4.2 cm enhancing extra-axial mass centered in the right frontal region with adjacent dural thickening and enhancement. Mild surrounding vasogenic edema extends into the frontal white matter with 3 mm of leftward midline shift. No restricted diffusion or hemorrhage.\n",
    "IMPRESSION: 1. Right frontal extra-axial enhancing mass (4.2 cm) consistent with meningioma.\n",
    "2. Mild mass effect with 3 mm leftward midline shift.\n",
    "3. No acute infarction or hemorrhage.\n",
    "\n",
    "Now generate optimal IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"방사선 보고서 전처리 - 정확도 및 공정성 최적화\"\"\"\n",
    "        import re\n",
    "        import pandas as pd\n",
    "\n",
    "        try:\n",
    "            radiology_text = data.get('radiology report', '')\n",
    "\n",
    "            if pd.isna(radiology_text) or not isinstance(radiology_text, str) or not radiology_text.strip():\n",
    "                return {'user_input': 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "            # Enhanced FINDINGS extraction with multiple fallback strategies\n",
    "            findings_text = radiology_text\n",
    "            \n",
    "            # Primary extraction - FINDINGS section\n",
    "            if 'FINDINGS:' in radiology_text:\n",
    "                findings_section = radiology_text.split('FINDINGS:')[1]\n",
    "                if 'IMPRESSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION:')[0]\n",
    "                elif 'CONCLUSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('CONCLUSION:')[0]\n",
    "                elif 'ASSESSMENT:' in findings_section:\n",
    "                    findings_section = findings_section.split('ASSESSMENT:')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "            \n",
    "            # Fallback - less specific FINDINGS\n",
    "            elif 'FINDINGS' in radiology_text and len(findings_text) == len(radiology_text):\n",
    "                findings_section = radiology_text.split('FINDINGS')[1]\n",
    "                if 'IMPRESSION' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION')[0]\n",
    "                elif 'CONCLUSION' in findings_section:\n",
    "                    findings_section = findings_section.split('CONCLUSION')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "            \n",
    "            # Alternative section names\n",
    "            elif 'INTERPRETATION:' in radiology_text:\n",
    "                findings_section = radiology_text.split('INTERPRETATION:')[1]\n",
    "                if 'IMPRESSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION:')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "\n",
    "            # Aggressive text cleaning for accuracy\n",
    "            findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "            findings_text = re.sub(r'\\b___+\\b', '[REDACTED]', findings_text)  # Preserve redacted info pattern\n",
    "            findings_text = re.sub(r'\\[\\*+[^\\]]*\\*+\\]', '[REDACTED]', findings_text)  # Remove bracketed redactions\n",
    "            findings_text = re.sub(r'\\s+', ' ', findings_text)  # Normalize whitespace\n",
    "            findings_text = findings_text.strip()\n",
    "\n",
    "            # Quality control - ensure substantial findings content\n",
    "            if len(findings_text) < 20:\n",
    "                # Try to extract from full text if FINDINGS section too short\n",
    "                sentences = re.split(r'[.!?]+', radiology_text)\n",
    "                medical_sentences = []\n",
    "                \n",
    "                medical_terms = [\n",
    "                    'normal', 'abnormal', 'mass', 'lesion', 'consolidation', 'effusion',\n",
    "                    'edema', 'hemorrhage', 'fracture', 'dislocation', 'stenosis', \n",
    "                    'dilatation', 'enhancement', 'atelectasis', 'pneumonia', 'cardiomegaly',\n",
    "                    'opacity', 'density', 'nodule', 'calcification'\n",
    "                ]\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if len(sentence) > 15 and any(term in sentence.lower() for term in medical_terms):\n",
    "                        medical_sentences.append(sentence)\n",
    "                        if len(' '.join(medical_sentences)) > 400:\n",
    "                            break\n",
    "                \n",
    "                if medical_sentences:\n",
    "                    findings_text = '. '.join(medical_sentences)\n",
    "\n",
    "            return {'user_input': findings_text if findings_text else 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Robust fallback\n",
    "            fallback_text = str(data.get('radiology report', ''))\n",
    "            if fallback_text.strip():\n",
    "                # Extract first meaningful sentence as fallback\n",
    "                sentences = re.split(r'[.!?]+', fallback_text)\n",
    "                for sentence in sentences[:3]:\n",
    "                    if len(sentence.strip()) > 20:\n",
    "                        return {'user_input': sentence.strip()}\n",
    "            return {'user_input': 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"후처리 최적화 - Conciseness 및 Clinical Clarity 강화\"\"\"\n",
    "        import re\n",
    "\n",
    "        try:\n",
    "            if not result or not isinstance(result, str):\n",
    "                return \"No acute abnormalities identified.\"\n",
    "\n",
    "            result = result.strip()\n",
    "\n",
    "            # Remove impression prefixes\n",
    "            impression_prefixes = [\n",
    "                'IMPRESSION:', 'Impression:', 'impression:', 'OPTIMAL IMPRESSION:',\n",
    "                'CONCLUSION:', 'ASSESSMENT:', 'SUMMARY:'\n",
    "            ]\n",
    "            for prefix in impression_prefixes:\n",
    "                if result.startswith(prefix):\n",
    "                    result = result[len(prefix):].strip()\n",
    "                    break\n",
    "\n",
    "            if not result:\n",
    "                return \"No acute abnormalities identified.\"\n",
    "\n",
    "            # Ensure proper sentence ending\n",
    "            if not result.endswith('.'):\n",
    "                result += '.'\n",
    "\n",
    "            # CONCISENESS OPTIMIZATION - Remove verbose phrases\n",
    "            conciseness_replacements = {\n",
    "                # Verbose medical expressions to concise equivalents\n",
    "                r'there (?:is|are) evidence of': '',\n",
    "                r'findings (?:are )?consistent with(?:\\s+and\\s+compatible\\s+with)?': '',\n",
    "                r'(?:appears to|seems to) (?:be|demonstrate|show)': '',\n",
    "                r'compatible with(?:\\s+a\\s+diagnosis\\s+of)?': '',\n",
    "                r'suggestive of(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'concerning for(?:\\s+the\\s+possibility\\s+of)?': 'concerning for',\n",
    "                r'no evidence of(?:\\s+any)?': 'no',\n",
    "                r'there is no(?:\\s+evidence\\s+of)?': 'no',\n",
    "                r'demonstrates?(?:\\s+evidence\\s+of)?': '',\n",
    "                r'shows?(?:\\s+signs\\s+of)?': '',\n",
    "                r'reveals?(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'indicates?(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'mild(?:\\s+degree\\s+of)?': 'mild',\n",
    "                r'moderate(?:\\s+degree\\s+of)?': 'moderate',\n",
    "                r'severe(?:\\s+degree\\s+of)?': 'severe',\n",
    "                r'small(?:\\s+amount\\s+of)?': 'small',\n",
    "                r'large(?:\\s+amount\\s+of)?': 'large',\n",
    "                r'(?:some\\s+)?degree\\s+of\\s+': '',\n",
    "                r'(?:a\\s+)?finding\\s+of\\s+': '',\n",
    "                r'presence\\s+of\\s+': '',\n",
    "                r'(?:most\\s+)?likely\\s+represents?': 'likely',\n",
    "                r'probably\\s+represents?': 'probably',\n",
    "                r'possibly\\s+represents?': 'possibly',\n",
    "            }\n",
    "\n",
    "            for pattern, replacement in conciseness_replacements.items():\n",
    "                result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
    "\n",
    "            # Clean up extra spaces and punctuation\n",
    "            result = re.sub(r'\\s+', ' ', result)\n",
    "            result = re.sub(r'\\s+([,.])', r'\\1', result)  # Remove space before punctuation\n",
    "            result = re.sub(r'([,.])([A-Z])', r'\\1 \\2', result)  # Add space after punctuation before capital\n",
    "            \n",
    "            # CLINICAL CLARITY - Standardize medical terminology\n",
    "            medical_standardizations = {\n",
    "                r'cardiomegaly': 'cardiomegaly',\n",
    "                r'pulmonary edema': 'pulmonary edema', \n",
    "                r'pleural effusion': 'pleural effusion',\n",
    "                r'pneumothorax': 'pneumothorax',\n",
    "                r'consolidation': 'consolidation',\n",
    "                r'atelectasis': 'atelectasis',\n",
    "                r'lymphadenopathy': 'lymphadenopathy',\n",
    "                r'hepatomegaly': 'hepatomegaly',\n",
    "                r'splenomegaly': 'splenomegaly'\n",
    "            }\n",
    "\n",
    "            # NUMBERING OPTIMIZATION for multiple findings\n",
    "            try:\n",
    "                if not result.startswith(('1.', '2.', '3.')) and ('. ' in result or ';' in result or ',' in result):\n",
    "                    # Split by various delimiters\n",
    "                    if ';' in result:\n",
    "                        sentences = [s.strip() for s in result.split(';') if s.strip()]\n",
    "                    elif '.' in result and len(result.split('.')) > 2:\n",
    "                        sentences = [s.strip() for s in result.split('.') if s.strip() and len(s.strip()) > 5]\n",
    "                    else:\n",
    "                        sentences = [s.strip() for s in result.split(',') if s.strip() and len(s.strip()) > 10]\n",
    "                    \n",
    "                    # Only number if we have 2+ substantial findings\n",
    "                    if len(sentences) >= 2 and all(len(s) > 8 for s in sentences):\n",
    "                        numbered_sentences = []\n",
    "                        for i, sentence in enumerate(sentences):\n",
    "                            if sentence and not sentence.endswith('.'):\n",
    "                                sentence += '.'\n",
    "                            numbered_sentences.append(f\"{i+1}. {sentence}\")\n",
    "                        \n",
    "                        if numbered_sentences:\n",
    "                            result = ' '.join(numbered_sentences)\n",
    "            except Exception:\n",
    "                pass  # Keep original if numbering fails\n",
    "\n",
    "            # Final length optimization for conciseness\n",
    "            words = result.split()\n",
    "            if len(words) > 50:  # If too verbose, prioritize key findings\n",
    "                sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "                if sentences:\n",
    "                    # Priority scoring for medical relevance\n",
    "                    priority_terms = [\n",
    "                        'fracture', 'mass', 'tumor', 'hemorrhage', 'infarction', 'pneumonia',\n",
    "                        'effusion', 'pneumothorax', 'cardiomegaly', 'consolidation', 'embolism',\n",
    "                        'stenosis', 'occlusion', 'aneurysm', 'dissection', 'malignancy'\n",
    "                    ]\n",
    "                    \n",
    "                    scored_sentences = []\n",
    "                    for sentence in sentences:\n",
    "                        score = sum(2 for term in priority_terms if term in sentence.lower())\n",
    "                        score += len([w for w in sentence.split() if len(w) > 6])  # Medical terms tend to be longer\n",
    "                        scored_sentences.append((sentence, score))\n",
    "                    \n",
    "                    # Keep highest scoring sentences within word limit\n",
    "                    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "                    final_sentences = []\n",
    "                    current_words = 0\n",
    "                    \n",
    "                    for sentence, score in scored_sentences:\n",
    "                        sentence_words = len(sentence.split())\n",
    "                        if current_words + sentence_words <= 45:\n",
    "                            final_sentences.append(sentence)\n",
    "                            current_words += sentence_words\n",
    "                        if current_words >= 25:  # Ensure minimum content\n",
    "                            break\n",
    "                    \n",
    "                    if final_sentences:\n",
    "                        result = '. '.join(final_sentences)\n",
    "                        if not result.endswith('.'):\n",
    "                            result += '.'\n",
    "\n",
    "            # Final cleanup\n",
    "            result = result.strip()\n",
    "            if not result:\n",
    "                return \"No acute abnormalities identified.\"\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return \"No acute abnormalities identified.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 대회와 정확히 동일한 평가 함수\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"대회 조건과 정확히 동일한 평가\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"🏆 대회 정확한 평가 조건 시뮬레이션 - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. 전체 Test 데이터 로드 (대회와 동일)\n",
    "    print(\"1. 전체 Test 데이터 로드 중...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # 🔧 NaN 값 처리 추가\n",
    "    print(\"   데이터 품질 확인 중...\")\n",
    "    print(f\"   전체 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    # NaN 값 확인\n",
    "    nan_count = test_df['radiology report'].isna().sum()\n",
    "    print(f\"   NaN 값: {nan_count}개\")\n",
    "    \n",
    "    # NaN 값이 있는 행 제거\n",
    "    test_df = test_df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"   유효 데이터: {len(test_df)}개\")\n",
    "    \n",
    "    total_samples = len(test_df)\n",
    "    \n",
    "    # 2. 대회에서 사용할 평가 샘플 크기 결정 (실제 Test 세트 크기와 유사하게)\n",
    "    # Test 1: 300건, Test 2: 300건이므로 300개로 평가\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. 연속된 샘플 사용 (대회에서는 특정 Test 세트를 사용하므로 bias 없는 연속 샘플)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # 처음 300개 사용\n",
    "    print(f\"평가 샘플: {eval_samples}개 (연속 샘플, 대회 Test 세트와 동일한 크기)\")\n",
    "    \n",
    "    # 4. 데이터 분포 확인\n",
    "    print(f\"\\n📊 평가 데이터 분포:\")\n",
    "    print(f\"   성별 분포: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   연령 분포: 평균 {eval_df['anchor_age'].mean():.1f}세 (범위: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB 처리기 초기화\n",
    "    print(\"\\n2. TaskB 처리기 초기화 (Llama 모델)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. 예측 생성 (대회와 동일한 배치 크기)\n",
    "    print(\"3. AI 예측 생성 중 (API 제한 준수)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # 대회 API 제한 준수 (1분당 10건)\n",
    "    results = []\n",
    "    batch_size = 8  # 안전 마진\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   배치 {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} 처리 중...\")\n",
    "        \n",
    "        # 전처리\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API 호출\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # 후처리\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API 제한 준수\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API 제한 준수를 위해 70초 대기...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"예측 생성 완료 (총 소요 시간: {generation_time:.1f}초)\")\n",
    "    \n",
    "    # 7. 정답 데이터 준비\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. 대회 제공 BERTScore 계산 (정확히 동일한 설정)\n",
    "    print(\"\\n4. 대회 BERTScore 계산 중...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. 대회 제공 공정성 지표 계산 (정확히 동일한 설정)\n",
    "    print(\"5. 대회 공정성 지표 계산 중...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # 성별 공정성\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 연령 공정성\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. 대회 정확한 결과 출력\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 대회 정확한 평가 결과 - Task B (Test 데이터)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"📊 BERTScore (대회 공식 계산)\")\n",
    "    print(f\"   평균: {bert_mean:.6f}\")\n",
    "    print(f\"   표준편차: {bert_std:.6f}\")\n",
    "    print(f\"   최고: {max(bert_scores):.6f}\")\n",
    "    print(f\"   최저: {min(bert_scores):.6f}\")\n",
    "    print(f\"   중앙값: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️ 공정성 지표 (대회 공식 계산)\")\n",
    "    print(f\"   성별 공정성: {gender_fairness:.6f}\")\n",
    "    print(f\"   성별별 성능: {gender_stats['by_group']}\")\n",
    "    print(f\"   성별 격차: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   연령 공정성: {age_fairness:.6f}\")\n",
    "    print(f\"   연령대별 성능: {age_stats['by_group']}\")\n",
    "    print(f\"   연령 격차: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. 정량 평가 점수 계산 (대회 기준)\n",
    "    print(f\"\\n🏆 대회 정량 평가 점수\")\n",
    "    \n",
    "    # BERTScore 점수 (3점 만점)\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # 공정성 점수 (2점 만점)\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # 총점\n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 점\")\n",
    "    print(f\"   공정성 지표: {fairness_points:.3f}/2.000 점\")\n",
    "    print(f\"   정량 총점: {total_quantitative:.3f}/5.000 점\")\n",
    "    print(f\"   정량 달성률: {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. 성능 등급 판정\n",
    "    print(f\"\\n🎖️ 성능 등급\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"S급 (최우수)\"\n",
    "        recommendation = \"즉시 제출 권장\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"A급 (우수)\"\n",
    "        recommendation = \"제출 권장\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"B급 (양호)\"\n",
    "        recommendation = \"소폭 개선 후 제출\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"C급 (보통)\"\n",
    "        recommendation = \"개선 필요\"\n",
    "    else:\n",
    "        grade = \"D급 (미흡)\"\n",
    "        recommendation = \"대폭 개선 필요\"\n",
    "    \n",
    "    print(f\"   등급: {grade}\")\n",
    "    print(f\"   권장사항: {recommendation}\")\n",
    "    \n",
    "    # 13. 샘플 결과 분석\n",
    "    print(f\"\\n📝 예측 품질 샘플 (상위/하위 각 2개)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"🏆 최고 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"⚠️ 최저 성능 샘플:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"샘플 {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"예측: {predictions[idx][:120]}...\")\n",
    "        print(f\"정답: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# 실행 (taskB_test.csv 사용)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"../data/taskB_train.csv\"\n",
    "\n",
    "# 대회 정확한 조건으로 Test 데이터 평가 실행\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 TaskB Test 데이터 평가 완료!\")\n",
    "print(f\"최종 예상 점수: {test_results['total_score']:.3f}/5.000 점\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117434c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
