{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2dbcbe5",
   "metadata": {},
   "source": [
    "## Datathon í´ë˜ìŠ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62ae4a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/datathon/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from typing import Any, List, Dict\n",
    "from typing import Optional, Dict, Any, List, Union\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain.prompts import ChatPromptTemplate  # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì²˜ë¦¬ìš©\n",
    "from langevaluate.config import ModelConfig # LLM ì„¤ì •ìš©\n",
    "from langevaluate.llmfactory import LLMFactory  # LLM íŒ©í† ë¦¬ìš©\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "import asyncio\n",
    "\n",
    "class DatathonProcessor(ABC):\n",
    "    \"\"\"\n",
    "    ë°ì´í„°í†¤ìš© AI ì²˜ë¦¬ í†µí•© í´ë˜ìŠ¤\n",
    "    ì¿¼ë¦¬, í‰ê°€, ì„ë² ë”©ì„ ì¼ê´„ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "    ì‚¬ìš©ìëŠ” ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ íŠ¹ì • ë©”ì„œë“œë§Œ êµ¬í˜„í•˜ë©´ ë©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # LLM ì„¤ì • ìƒìˆ˜ë“¤\n",
    "    \n",
    "    DEFAULT_MODEL_CONFIG = {\n",
    "        'model_name': 'LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ',\n",
    "        'api_base': 'https://api.snubhai.org/api/v1/llm',\n",
    "        'max_tokens': 2000,\n",
    "        'seed': 777,\n",
    "        'temperature': 0,\n",
    "        'rpm': 10\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        api_key : str,\n",
    "    ):\n",
    "        # ê¸°ë³¸ ì„¤ì • ë³µì‚¬\n",
    "        config = self.DEFAULT_MODEL_CONFIG.copy()\n",
    "        \n",
    "        # model_nameë§Œ í´ë˜ìŠ¤ë³„ ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸\n",
    "        config['model_name'] = self.get_model_name()\n",
    "        \n",
    "        # LLM ì„¤ì • ìƒì„±\n",
    "        custom_config = ModelConfig(\n",
    "            model_name=config['model_name'],\n",
    "            api_base=config['api_base'],\n",
    "            api_key=api_key,\n",
    "            max_tokens=config['max_tokens'],\n",
    "            seed=config['seed'],\n",
    "            provider=\"openai\"\n",
    "        )\n",
    "        \n",
    "        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n",
    "        self.llm = LLMFactory.create_llm(\n",
    "            custom_config, \n",
    "            temperature=config['temperature'], \n",
    "            rpm=config['rpm']\n",
    "        )\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(self.get_prompt_template())\n",
    "        self.chain = self.prompt_template | self.llm\n",
    "\n",
    "        # ê²°ê³¼ ì €ì¥ì†Œ\n",
    "        self.results: List[str] = []\n",
    "        \n",
    "        # metric ì €ì¥ì†Œ\n",
    "        self.metrics: Dict[str, Any] = {}\n",
    "    \n",
    "        \n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"\n",
    "        ì‚¬ìš©í•  ëª¨ë¸ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "        ìƒì† í´ë˜ìŠ¤ì—ì„œ ì´ ë©”ì„œë“œë¥¼ ì˜¤ë²„ë¼ì´ë“œí•˜ì—¬ íŠ¹ì • ëª¨ë¸ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "        return self.DEFAULT_MODEL_CONFIG['model_name']\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_prompt_template(self) -> str:\n",
    "        \"\"\"ì‚¬ìš©ìê°€ êµ¬í˜„í•´ì•¼ í•˜ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def postprocess_result(self, result: Any) -> str:\n",
    "        \"\"\"ë°ì´í„° í›„ì²˜ë¦¬ ë©”ì„œë“œ\"\"\"\n",
    "        pass\n",
    "\n",
    "    async def summarize(\n",
    "        self, \n",
    "        data: pd.DataFrame\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        ë‹¨ì¼ ì…ë ¥ê³¼ ë°°ì¹˜ ì…ë ¥ì„ ëª¨ë‘ ì²˜ë¦¬í•˜ëŠ” í†µí•© ë©”ì„œë“œ\n",
    "        \"\"\"\n",
    "        # ë°ì´í„° ì „ì²˜ë¦¬\n",
    "        \n",
    "        preprocess_tasks = [self.preprocess_data(row) for _, row in data.iterrows()]\n",
    "        preprocessed_data = await tqdm_asyncio.gather(*preprocess_tasks)\n",
    "\n",
    "        # ê°ê°ì„ ë³„ë„ì˜ coroutineìœ¼ë¡œ ì‹¤í–‰\n",
    "        tasks = [self.chain.ainvoke(vars) for vars in preprocessed_data]\n",
    "\n",
    "        # tqdm_asyncio.gatherë¡œ ë™ì‹œì— ì‹¤í–‰í•˜ë©° progress bar í‘œì‹œ\n",
    "        responses = await tqdm_asyncio.gather(*tasks)\n",
    "\n",
    "        postprocess_tasks = [self.postprocess_result(r.content) for r in responses]\n",
    "        results = await tqdm_asyncio.gather(*postprocess_tasks)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec04224",
   "metadata": {},
   "source": [
    "## ìì²´í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46790532",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ† ëŒ€íšŒ ì •í™•í•œ í‰ê°€ ì¡°ê±´ ì‹œë®¬ë ˆì´ì…˜ - Task B\n",
      "================================================================================\n",
      "1. ì „ì²´ Test ë°ì´í„° ë¡œë“œ ì¤‘...\n",
      "   ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì¤‘...\n",
      "   ì „ì²´ ë°ì´í„°: 1000ê°œ\n",
      "   NaN ê°’: 11ê°œ\n",
      "   ìœ íš¨ ë°ì´í„°: 989ê°œ\n",
      "í‰ê°€ ìƒ˜í”Œ: 300ê°œ (ì—°ì† ìƒ˜í”Œ, ëŒ€íšŒ Test ì„¸íŠ¸ì™€ ë™ì¼í•œ í¬ê¸°)\n",
      "\n",
      "ğŸ“Š í‰ê°€ ë°ì´í„° ë¶„í¬:\n",
      "   ì„±ë³„ ë¶„í¬: {'M': 154, 'F': 146}\n",
      "   ì—°ë ¹ ë¶„í¬: í‰ê·  63.6ì„¸ (ë²”ìœ„: 19-91)\n",
      "\n",
      "2. TaskB ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (Llama ëª¨ë¸)...\n",
      "3. AI ì˜ˆì¸¡ ìƒì„± ì¤‘ (API ì œí•œ ì¤€ìˆ˜)...\n",
      "   ë°°ì¹˜ 1/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 2/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 3/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 2/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 3/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 4/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 5/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 6/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 7/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 8/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 9/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 10/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 11/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 12/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 13/38 ì²˜ë¦¬ ì¤‘...\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "API Error: Error code: 429 - {'error': {'message': 'Rate limit exceeded. Token bucket: 0.00/10.0 tokens. Wait 60s.', 'type': 'rate_limit_error', 'param': None, 'code': 'rate_limit_exceeded'}}, retry 1/3\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 14/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 15/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 16/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 17/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 18/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 19/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 20/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 21/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 22/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 23/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 24/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 25/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 26/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 27/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 28/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 29/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 30/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 31/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 32/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 33/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 34/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 35/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 36/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 37/38 ì²˜ë¦¬ ì¤‘...\n",
      "   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\n",
      "   ë°°ì¹˜ 38/38 ì²˜ë¦¬ ì¤‘...\n",
      "ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: 5916.5ì´ˆ)\n",
      "\n",
      "4. ëŒ€íšŒ BERTScore ê³„ì‚° ì¤‘...\n",
      "5. ëŒ€íšŒ ê³µì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\n",
      "\n",
      "================================================================================\n",
      "ğŸ¯ ëŒ€íšŒ ì •í™•í•œ í‰ê°€ ê²°ê³¼ - Task B (Test ë°ì´í„°)\n",
      "================================================================================\n",
      "ğŸ“Š BERTScore (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
      "   í‰ê· : 0.791272\n",
      "   í‘œì¤€í¸ì°¨: 0.036367\n",
      "   ìµœê³ : 0.930865\n",
      "   ìµœì €: 0.680474\n",
      "   ì¤‘ì•™ê°’: 0.790075\n",
      "\n",
      "âš–ï¸ ê³µì •ì„± ì§€í‘œ (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\n",
      "   ì„±ë³„ ê³µì •ì„±: 0.999362\n",
      "   ì„±ë³„ë³„ ì„±ëŠ¥: {'F': 0.7910128080681579, 'M': 0.7915177983897073}\n",
      "   ì„±ë³„ ê²©ì°¨: 0.000505\n",
      "   \n",
      "   ì—°ë ¹ ê³µì •ì„±: 0.974506\n",
      "   ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥: {'10-20': 0.8021212220191956, '20-30': 0.7926706110729891, '30-40': 0.7836787660916646, '40-50': 0.7966337258165533, '50-60': 0.7993657265679311, '60-70': 0.7821916560052147, '70-80': 0.7968791496753692, '80-90': 0.7921639124552409, '90-100': 0.7816723436117172}\n",
      "   ì—°ë ¹ ê²©ì°¨: 0.020449\n",
      "\n",
      "ğŸ† ëŒ€íšŒ ì •ëŸ‰ í‰ê°€ ì ìˆ˜\n",
      "   BERTScore: 2.793/3.000 ì \n",
      "   ê³µì •ì„± ì§€í‘œ: 2.000/2.000 ì \n",
      "   ì •ëŸ‰ ì´ì : 4.793/5.000 ì \n",
      "   ì •ëŸ‰ ë‹¬ì„±ë¥ : 95.9%\n",
      "\n",
      "ğŸ–ï¸ ì„±ëŠ¥ ë“±ê¸‰\n",
      "   ë“±ê¸‰: Sê¸‰ (ìµœìš°ìˆ˜)\n",
      "   ê¶Œì¥ì‚¬í•­: ì¦‰ì‹œ ì œì¶œ ê¶Œì¥\n",
      "\n",
      "ğŸ“ ì˜ˆì¸¡ í’ˆì§ˆ ìƒ˜í”Œ (ìƒìœ„/í•˜ìœ„ ê° 2ê°œ)\n",
      "--------------------------------------------------------------------------------\n",
      "ğŸ† ìµœê³  ì„±ëŠ¥ ìƒ˜í”Œ:\n",
      "ìƒ˜í”Œ 255 (BERTScore: 0.9309)\n",
      "ì˜ˆì¸¡: 1. Right atrial central venous catheter placement noted. 2. moderate cardiomegaly. 3. Pulmonary vascular congestion with...\n",
      "ì •ë‹µ: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Focal consolidation in the right lower lobe likely represents pneumonia.\n",
      "2. Moderate cardiomegaly with ...\n",
      "\n",
      "ìƒ˜í”Œ 162 (BERTScore: 0.8998)\n",
      "ì˜ˆì¸¡: 1. Acute right-sided subdural hematoma (13 mm maximal thickness) with effacement of right cerebral sulci and mass effect...\n",
      "ì •ë‹µ: IMPRESSION:\n",
      "\n",
      "\n",
      "1. Acute right-sided subdural hematoma measuring up to 13 mm associated with 7\n",
      "mm of leftward shift of nor...\n",
      "\n",
      "âš ï¸ ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ:\n",
      "ìƒ˜í”Œ 96 (BERTScore: 0.6805)\n",
      "ì˜ˆì¸¡: **Retrocardiac opacity** (likely pleural thickening or small effusion. **Cardiomegaly** noted, though mediastinal silhou...\n",
      "ì •ë‹µ: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "ìƒ˜í”Œ 48 (BERTScore: 0.6811)\n",
      "ì˜ˆì¸¡: 1. **IMPRESSION:** 1. 2. **Minimal retrocardiac streaky atelectasis** 2. 3. no pneumonia 3. 4. No large effusion or pneu...\n",
      "ì •ë‹µ: IMPRESSION: \n",
      "\n",
      "As above....\n",
      "\n",
      "\n",
      "ğŸ‰ TaskB Test ë°ì´í„° í‰ê°€ ì™„ë£Œ!\n",
      "ìµœì¢… ì˜ˆìƒ ì ìˆ˜: 4.793/5.000 ì \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "from typing import Any, Dict, List\n",
    "from bert_score import BERTScorer\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langevaluate.config import ModelConfig\n",
    "from langevaluate.llmfactory import LLMFactory\n",
    "import time\n",
    "import re\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ BertScore í´ë˜ìŠ¤ (ì •í™•íˆ ë™ì¼)\n",
    "class BertScore:\n",
    "    def __init__(self, model_type=\"distilbert-base-uncased\", batch_size=16):\n",
    "        with torch.no_grad():\n",
    "            self.bert_scorer = BERTScorer(\n",
    "                model_type=model_type,\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "\n",
    "    def __call__(self, refs, hyps):\n",
    "        p, r, f = self.bert_scorer.score(\n",
    "            cands=hyps,\n",
    "            refs=refs,\n",
    "            verbose=False,\n",
    "            batch_size=8,\n",
    "        )\n",
    "        return f.tolist()\n",
    "\n",
    "# ëŒ€íšŒ ì œê³µ FairnessScore í´ë˜ìŠ¤ (ì •í™•íˆ ë™ì¼)\n",
    "class FairnessScore:\n",
    "    def __init__(self, bin_width: int = 10, min_samples_per_group: int = 1):\n",
    "        self.bin_width = int(bin_width)\n",
    "        self.min_samples_per_group = int(min_samples_per_group)\n",
    "        self.last_stats = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_1d(a) -> np.ndarray:\n",
    "        a = np.asarray(a)\n",
    "        if a.ndim == 2 and a.shape[1] == 1:\n",
    "            a = a[:, 0]\n",
    "        if a.ndim != 1:\n",
    "            raise ValueError(\"Input must be 1D or (N,1) shaped.\")\n",
    "        return a\n",
    "\n",
    "    def _bin_ages(self, ages) -> np.ndarray:\n",
    "        a = self._ensure_1d(ages).astype(float)\n",
    "        if np.any(np.isnan(a)):\n",
    "            raise ValueError(\"ages contain NaN.\")\n",
    "        if self.bin_width <= 0:\n",
    "            raise ValueError(\"bin_width must be positive.\")\n",
    "        starts = (np.floor(a / self.bin_width) * self.bin_width).astype(int)\n",
    "        ends = starts + self.bin_width\n",
    "        labels = np.array([f\"{s:d}-{e:d}\" for s, e in zip(starts, ends)], dtype=object)\n",
    "        return labels\n",
    "\n",
    "    def _groups_from_type(self, groups, type: str) -> np.ndarray:\n",
    "        t = (type or \"sex\").lower()\n",
    "        if t not in (\"sex\", \"age\"):\n",
    "            raise ValueError(\"type must be 'sex' or 'age'.\")\n",
    "        if t == \"sex\":\n",
    "            g = self._ensure_1d(groups)\n",
    "            return g\n",
    "        else:\n",
    "            return self._bin_ages(groups)\n",
    "\n",
    "    def __call__(self, groups, scores, type: str = \"sex\", sample_weight=None) -> float:\n",
    "        g = self._groups_from_type(groups, type=type)\n",
    "        s = self._ensure_1d(scores).astype(float)\n",
    "        if s.shape[0] != g.shape[0]:\n",
    "            raise ValueError(\"groups and scores must have the same length.\")\n",
    "\n",
    "        if sample_weight is None:\n",
    "            w = np.ones_like(s, dtype=float)\n",
    "        else:\n",
    "            w = self._ensure_1d(sample_weight).astype(float)\n",
    "            if w.shape[0] != s.shape[0]:\n",
    "                raise ValueError(\"sample_weight length must match scores.\")\n",
    "\n",
    "        s = np.clip(s, 0.0, 1.0)\n",
    "\n",
    "        uniq = np.unique(g)\n",
    "        means = []\n",
    "        by_group = {}\n",
    "        for grp in uniq:\n",
    "            mask = (g == grp)\n",
    "            if np.sum(mask) < self.min_samples_per_group:\n",
    "                continue\n",
    "            denom = np.sum(w[mask])\n",
    "            if denom <= 0:\n",
    "                continue\n",
    "            m = float(np.average(s[mask], weights=w[mask]))\n",
    "            means.append(m)\n",
    "            by_group[str(grp)] = m\n",
    "\n",
    "        if len(means) <= 1:\n",
    "            self.last_stats = {\"by_group\": by_group, \"gap\": 0.0, \"min\": None, \"max\": None}\n",
    "            return 1.0\n",
    "\n",
    "        max_m = float(np.max(means))\n",
    "        min_m = float(np.min(means))\n",
    "        fairness = 1.0 if max_m == 0.0 else float(min_m / max_m)\n",
    "        fairness = float(np.clip(fairness, 0.0, 1.0))\n",
    "\n",
    "        self.last_stats = {\"by_group\": by_group, \"gap\": max_m - min_m, \"min\": min_m, \"max\": max_m}\n",
    "        return fairness\n",
    "\n",
    "# TaskB Processor \n",
    "class TaskBProcessor(DatathonProcessor):\n",
    "    \"\"\"Task B: Radiology Impression ìš”ì•½ - ê·¹í•œ ìµœì í™”\"\"\"\n",
    "    \n",
    "    def get_model_name(self) -> str:\n",
    "        return \"LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct-AWQ\"\n",
    "    \n",
    "    def get_prompt_template(self) -> str:\n",
    "        return \"\"\"You are a board-certified radiologist with 20+ years of experience creating diagnostic impressions. Generate a precise, clinically actionable IMPRESSION that maximizes diagnostic clarity and conciseness.\n",
    "\n",
    "CRITICAL OPTIMIZATION FOR EVALUATION:\n",
    "- QUALITY: Capture ALL key findings with complete accuracy - miss nothing important\n",
    "- CLINICAL CLARITY: Use precise radiological terminology that clinicians can act upon immediately  \n",
    "- CONCISENESS: Zero redundancy, no verbose phrases, every word essential for diagnosis\n",
    "- ACCURACY: Only document what is explicitly stated - absolutely no assumptions or hallucinations\n",
    "\n",
    "OPTIMAL IMPRESSION STRUCTURE:\n",
    "1. Primary/acute findings first (most clinically significant)\n",
    "2. Secondary findings (supportive/chronic conditions)  \n",
    "3. Explicit negatives for critical differentials when mentioned\n",
    "4. Numbered format for multiple distinct findings (1., 2., 3.)\n",
    "\n",
    "CONCISENESS OPTIMIZATION EXAMPLES:\n",
    "\n",
    "VERBOSE: \"There appears to be some degree of mild enlargement of the cardiac silhouette that suggests possible cardiomegaly\"\n",
    "CONCISE: \"Mild cardiomegaly\"\n",
    "\n",
    "VERBOSE: \"The findings are consistent with and compatible with pneumonia involving the right lower lobe\"\n",
    "CONCISE: \"Right lower lobe pneumonia\"\n",
    "\n",
    "VERBOSE: \"No evidence of any acute fractures or significant osseous abnormalities are identified\"\n",
    "CONCISE: \"No acute fractures\"\n",
    "\n",
    "EXEMPLAR CASES FOR MAXIMUM SCORES:\n",
    "\n",
    "CT CHEST HIGH-COMPLEXITY:\n",
    "FINDINGS: Multiple bilateral pulmonary nodules, the largest measuring 2.3 cm in the right upper lobe with spiculated margins and adjacent pleural thickening. Moderate right pleural effusion with loculations. Mild mediastinal lymphadenopathy with nodes up to 1.2 cm. Left lower lobe consolidation with air bronchograms. No pericardial effusion.\n",
    "IMPRESSION: 1. Right upper lobe spiculated nodule (2.3 cm) concerning for malignancy with pleural involvement.\n",
    "2. Loculated right pleural effusion.  \n",
    "3. Mediastinal lymphadenopathy.\n",
    "4. Left lower lobe pneumonia.\n",
    "\n",
    "CHEST X-RAY OPTIMIZATION:\n",
    "FINDINGS: Heart size is enlarged. There is bilateral lower lobe airspace opacification consistent with consolidation or atelectasis. Small bilateral pleural effusions are present. The mediastinal contours appear normal. No pneumothorax is evident.\n",
    "IMPRESSION: 1. Cardiomegaly with bilateral lower lobe consolidation.\n",
    "2. Small bilateral pleural effusions.\n",
    "\n",
    "MR HEAD PRECISION:\n",
    "FINDINGS: There is a 4.2 cm enhancing extra-axial mass centered in the right frontal region with adjacent dural thickening and enhancement. Mild surrounding vasogenic edema extends into the frontal white matter with 3 mm of leftward midline shift. No restricted diffusion or hemorrhage.\n",
    "IMPRESSION: 1. Right frontal extra-axial enhancing mass (4.2 cm) consistent with meningioma.\n",
    "2. Mild mass effect with 3 mm leftward midline shift.\n",
    "3. No acute infarction or hemorrhage.\n",
    "\n",
    "Now generate optimal IMPRESSION for:\n",
    "FINDINGS: {user_input}\n",
    "IMPRESSION:\"\"\"\n",
    "\n",
    "    async def preprocess_data(self, data: Any) -> Dict[str, Any]:\n",
    "        \"\"\"ë°©ì‚¬ì„  ë³´ê³ ì„œ ì „ì²˜ë¦¬ - ì •í™•ë„ ë° ê³µì •ì„± ìµœì í™”\"\"\"\n",
    "        import re\n",
    "        import pandas as pd\n",
    "\n",
    "        try:\n",
    "            radiology_text = data.get('radiology report', '')\n",
    "\n",
    "            if pd.isna(radiology_text) or not isinstance(radiology_text, str) or not radiology_text.strip():\n",
    "                return {'user_input': 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "            # Enhanced FINDINGS extraction with multiple fallback strategies\n",
    "            findings_text = radiology_text\n",
    "            \n",
    "            # Primary extraction - FINDINGS section\n",
    "            if 'FINDINGS:' in radiology_text:\n",
    "                findings_section = radiology_text.split('FINDINGS:')[1]\n",
    "                if 'IMPRESSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION:')[0]\n",
    "                elif 'CONCLUSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('CONCLUSION:')[0]\n",
    "                elif 'ASSESSMENT:' in findings_section:\n",
    "                    findings_section = findings_section.split('ASSESSMENT:')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "            \n",
    "            # Fallback - less specific FINDINGS\n",
    "            elif 'FINDINGS' in radiology_text and len(findings_text) == len(radiology_text):\n",
    "                findings_section = radiology_text.split('FINDINGS')[1]\n",
    "                if 'IMPRESSION' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION')[0]\n",
    "                elif 'CONCLUSION' in findings_section:\n",
    "                    findings_section = findings_section.split('CONCLUSION')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "            \n",
    "            # Alternative section names\n",
    "            elif 'INTERPRETATION:' in radiology_text:\n",
    "                findings_section = radiology_text.split('INTERPRETATION:')[1]\n",
    "                if 'IMPRESSION:' in findings_section:\n",
    "                    findings_section = findings_section.split('IMPRESSION:')[0]\n",
    "                findings_text = findings_section.strip()\n",
    "\n",
    "            # Aggressive text cleaning for accuracy\n",
    "            findings_text = re.sub(r'^[:\\s]*', '', findings_text)\n",
    "            findings_text = re.sub(r'\\b___+\\b', '[REDACTED]', findings_text)  # Preserve redacted info pattern\n",
    "            findings_text = re.sub(r'\\[\\*+[^\\]]*\\*+\\]', '[REDACTED]', findings_text)  # Remove bracketed redactions\n",
    "            findings_text = re.sub(r'\\s+', ' ', findings_text)  # Normalize whitespace\n",
    "            findings_text = findings_text.strip()\n",
    "\n",
    "            # Quality control - ensure substantial findings content\n",
    "            if len(findings_text) < 20:\n",
    "                # Try to extract from full text if FINDINGS section too short\n",
    "                sentences = re.split(r'[.!?]+', radiology_text)\n",
    "                medical_sentences = []\n",
    "                \n",
    "                medical_terms = [\n",
    "                    'normal', 'abnormal', 'mass', 'lesion', 'consolidation', 'effusion',\n",
    "                    'edema', 'hemorrhage', 'fracture', 'dislocation', 'stenosis', \n",
    "                    'dilatation', 'enhancement', 'atelectasis', 'pneumonia', 'cardiomegaly',\n",
    "                    'opacity', 'density', 'nodule', 'calcification'\n",
    "                ]\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence = sentence.strip()\n",
    "                    if len(sentence) > 15 and any(term in sentence.lower() for term in medical_terms):\n",
    "                        medical_sentences.append(sentence)\n",
    "                        if len(' '.join(medical_sentences)) > 400:\n",
    "                            break\n",
    "                \n",
    "                if medical_sentences:\n",
    "                    findings_text = '. '.join(medical_sentences)\n",
    "\n",
    "            return {'user_input': findings_text if findings_text else 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Robust fallback\n",
    "            fallback_text = str(data.get('radiology report', ''))\n",
    "            if fallback_text.strip():\n",
    "                # Extract first meaningful sentence as fallback\n",
    "                sentences = re.split(r'[.!?]+', fallback_text)\n",
    "                for sentence in sentences[:3]:\n",
    "                    if len(sentence.strip()) > 20:\n",
    "                        return {'user_input': sentence.strip()}\n",
    "            return {'user_input': 'Normal examination without acute abnormalities.'}\n",
    "\n",
    "    async def postprocess_result(self, result: str) -> str:\n",
    "        \"\"\"í›„ì²˜ë¦¬ ìµœì í™” - Conciseness ë° Clinical Clarity ê°•í™”\"\"\"\n",
    "        import re\n",
    "\n",
    "        try:\n",
    "            if not result or not isinstance(result, str):\n",
    "                return \"No acute abnormalities identified.\"\n",
    "\n",
    "            result = result.strip()\n",
    "\n",
    "            # Remove impression prefixes\n",
    "            impression_prefixes = [\n",
    "                'IMPRESSION:', 'Impression:', 'impression:', 'OPTIMAL IMPRESSION:',\n",
    "                'CONCLUSION:', 'ASSESSMENT:', 'SUMMARY:'\n",
    "            ]\n",
    "            for prefix in impression_prefixes:\n",
    "                if result.startswith(prefix):\n",
    "                    result = result[len(prefix):].strip()\n",
    "                    break\n",
    "\n",
    "            if not result:\n",
    "                return \"No acute abnormalities identified.\"\n",
    "\n",
    "            # Ensure proper sentence ending\n",
    "            if not result.endswith('.'):\n",
    "                result += '.'\n",
    "\n",
    "            # CONCISENESS OPTIMIZATION - Remove verbose phrases\n",
    "            conciseness_replacements = {\n",
    "                # Verbose medical expressions to concise equivalents\n",
    "                r'there (?:is|are) evidence of': '',\n",
    "                r'findings (?:are )?consistent with(?:\\s+and\\s+compatible\\s+with)?': '',\n",
    "                r'(?:appears to|seems to) (?:be|demonstrate|show)': '',\n",
    "                r'compatible with(?:\\s+a\\s+diagnosis\\s+of)?': '',\n",
    "                r'suggestive of(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'concerning for(?:\\s+the\\s+possibility\\s+of)?': 'concerning for',\n",
    "                r'no evidence of(?:\\s+any)?': 'no',\n",
    "                r'there is no(?:\\s+evidence\\s+of)?': 'no',\n",
    "                r'demonstrates?(?:\\s+evidence\\s+of)?': '',\n",
    "                r'shows?(?:\\s+signs\\s+of)?': '',\n",
    "                r'reveals?(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'indicates?(?:\\s+the\\s+presence\\s+of)?': '',\n",
    "                r'mild(?:\\s+degree\\s+of)?': 'mild',\n",
    "                r'moderate(?:\\s+degree\\s+of)?': 'moderate',\n",
    "                r'severe(?:\\s+degree\\s+of)?': 'severe',\n",
    "                r'small(?:\\s+amount\\s+of)?': 'small',\n",
    "                r'large(?:\\s+amount\\s+of)?': 'large',\n",
    "                r'(?:some\\s+)?degree\\s+of\\s+': '',\n",
    "                r'(?:a\\s+)?finding\\s+of\\s+': '',\n",
    "                r'presence\\s+of\\s+': '',\n",
    "                r'(?:most\\s+)?likely\\s+represents?': 'likely',\n",
    "                r'probably\\s+represents?': 'probably',\n",
    "                r'possibly\\s+represents?': 'possibly',\n",
    "            }\n",
    "\n",
    "            for pattern, replacement in conciseness_replacements.items():\n",
    "                result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
    "\n",
    "            # Clean up extra spaces and punctuation\n",
    "            result = re.sub(r'\\s+', ' ', result)\n",
    "            result = re.sub(r'\\s+([,.])', r'\\1', result)  # Remove space before punctuation\n",
    "            result = re.sub(r'([,.])([A-Z])', r'\\1 \\2', result)  # Add space after punctuation before capital\n",
    "            \n",
    "            # CLINICAL CLARITY - Standardize medical terminology\n",
    "            medical_standardizations = {\n",
    "                r'cardiomegaly': 'cardiomegaly',\n",
    "                r'pulmonary edema': 'pulmonary edema', \n",
    "                r'pleural effusion': 'pleural effusion',\n",
    "                r'pneumothorax': 'pneumothorax',\n",
    "                r'consolidation': 'consolidation',\n",
    "                r'atelectasis': 'atelectasis',\n",
    "                r'lymphadenopathy': 'lymphadenopathy',\n",
    "                r'hepatomegaly': 'hepatomegaly',\n",
    "                r'splenomegaly': 'splenomegaly'\n",
    "            }\n",
    "\n",
    "            # NUMBERING OPTIMIZATION for multiple findings\n",
    "            try:\n",
    "                if not result.startswith(('1.', '2.', '3.')) and ('. ' in result or ';' in result or ',' in result):\n",
    "                    # Split by various delimiters\n",
    "                    if ';' in result:\n",
    "                        sentences = [s.strip() for s in result.split(';') if s.strip()]\n",
    "                    elif '.' in result and len(result.split('.')) > 2:\n",
    "                        sentences = [s.strip() for s in result.split('.') if s.strip() and len(s.strip()) > 5]\n",
    "                    else:\n",
    "                        sentences = [s.strip() for s in result.split(',') if s.strip() and len(s.strip()) > 10]\n",
    "                    \n",
    "                    # Only number if we have 2+ substantial findings\n",
    "                    if len(sentences) >= 2 and all(len(s) > 8 for s in sentences):\n",
    "                        numbered_sentences = []\n",
    "                        for i, sentence in enumerate(sentences):\n",
    "                            if sentence and not sentence.endswith('.'):\n",
    "                                sentence += '.'\n",
    "                            numbered_sentences.append(f\"{i+1}. {sentence}\")\n",
    "                        \n",
    "                        if numbered_sentences:\n",
    "                            result = ' '.join(numbered_sentences)\n",
    "            except Exception:\n",
    "                pass  # Keep original if numbering fails\n",
    "\n",
    "            # Final length optimization for conciseness\n",
    "            words = result.split()\n",
    "            if len(words) > 50:  # If too verbose, prioritize key findings\n",
    "                sentences = [s.strip() for s in result.split('.') if s.strip()]\n",
    "                if sentences:\n",
    "                    # Priority scoring for medical relevance\n",
    "                    priority_terms = [\n",
    "                        'fracture', 'mass', 'tumor', 'hemorrhage', 'infarction', 'pneumonia',\n",
    "                        'effusion', 'pneumothorax', 'cardiomegaly', 'consolidation', 'embolism',\n",
    "                        'stenosis', 'occlusion', 'aneurysm', 'dissection', 'malignancy'\n",
    "                    ]\n",
    "                    \n",
    "                    scored_sentences = []\n",
    "                    for sentence in sentences:\n",
    "                        score = sum(2 for term in priority_terms if term in sentence.lower())\n",
    "                        score += len([w for w in sentence.split() if len(w) > 6])  # Medical terms tend to be longer\n",
    "                        scored_sentences.append((sentence, score))\n",
    "                    \n",
    "                    # Keep highest scoring sentences within word limit\n",
    "                    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "                    final_sentences = []\n",
    "                    current_words = 0\n",
    "                    \n",
    "                    for sentence, score in scored_sentences:\n",
    "                        sentence_words = len(sentence.split())\n",
    "                        if current_words + sentence_words <= 45:\n",
    "                            final_sentences.append(sentence)\n",
    "                            current_words += sentence_words\n",
    "                        if current_words >= 25:  # Ensure minimum content\n",
    "                            break\n",
    "                    \n",
    "                    if final_sentences:\n",
    "                        result = '. '.join(final_sentences)\n",
    "                        if not result.endswith('.'):\n",
    "                            result += '.'\n",
    "\n",
    "            # Final cleanup\n",
    "            result = result.strip()\n",
    "            if not result:\n",
    "                return \"No acute abnormalities identified.\"\n",
    "            \n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            return \"No acute abnormalities identified.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ëŒ€íšŒì™€ ì •í™•íˆ ë™ì¼í•œ í‰ê°€ í•¨ìˆ˜\n",
    "async def exact_competition_evaluation(train_csv_path: str, api_key: str):\n",
    "    \"\"\"ëŒ€íšŒ ì¡°ê±´ê³¼ ì •í™•íˆ ë™ì¼í•œ í‰ê°€\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ† ëŒ€íšŒ ì •í™•í•œ í‰ê°€ ì¡°ê±´ ì‹œë®¬ë ˆì´ì…˜ - Task B\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. ì „ì²´ Test ë°ì´í„° ë¡œë“œ (ëŒ€íšŒì™€ ë™ì¼)\n",
    "    print(\"1. ì „ì²´ Test ë°ì´í„° ë¡œë“œ ì¤‘...\")\n",
    "    test_df = pd.read_csv(train_csv_path)\n",
    "    \n",
    "    # ğŸ”§ NaN ê°’ ì²˜ë¦¬ ì¶”ê°€\n",
    "    print(\"   ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì¤‘...\")\n",
    "    print(f\"   ì „ì²´ ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "    \n",
    "    # NaN ê°’ í™•ì¸\n",
    "    nan_count = test_df['radiology report'].isna().sum()\n",
    "    print(f\"   NaN ê°’: {nan_count}ê°œ\")\n",
    "    \n",
    "    # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±°\n",
    "    test_df = test_df.dropna(subset=['radiology report', 'target'])\n",
    "    print(f\"   ìœ íš¨ ë°ì´í„°: {len(test_df)}ê°œ\")\n",
    "    \n",
    "    total_samples = len(test_df)\n",
    "    \n",
    "    # 2. ëŒ€íšŒì—ì„œ ì‚¬ìš©í•  í‰ê°€ ìƒ˜í”Œ í¬ê¸° ê²°ì • (ì‹¤ì œ Test ì„¸íŠ¸ í¬ê¸°ì™€ ìœ ì‚¬í•˜ê²Œ)\n",
    "    # Test 1: 300ê±´, Test 2: 300ê±´ì´ë¯€ë¡œ 300ê°œë¡œ í‰ê°€\n",
    "    eval_samples = min(300, total_samples)\n",
    "    \n",
    "    # 3. ì—°ì†ëœ ìƒ˜í”Œ ì‚¬ìš© (ëŒ€íšŒì—ì„œëŠ” íŠ¹ì • Test ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ bias ì—†ëŠ” ì—°ì† ìƒ˜í”Œ)\n",
    "    eval_df = test_df.iloc[:eval_samples].copy()  # ì²˜ìŒ 300ê°œ ì‚¬ìš©\n",
    "    print(f\"í‰ê°€ ìƒ˜í”Œ: {eval_samples}ê°œ (ì—°ì† ìƒ˜í”Œ, ëŒ€íšŒ Test ì„¸íŠ¸ì™€ ë™ì¼í•œ í¬ê¸°)\")\n",
    "    \n",
    "    # 4. ë°ì´í„° ë¶„í¬ í™•ì¸\n",
    "    print(f\"\\nğŸ“Š í‰ê°€ ë°ì´í„° ë¶„í¬:\")\n",
    "    print(f\"   ì„±ë³„ ë¶„í¬: {eval_df['gender'].value_counts().to_dict()}\")\n",
    "    print(f\"   ì—°ë ¹ ë¶„í¬: í‰ê·  {eval_df['anchor_age'].mean():.1f}ì„¸ (ë²”ìœ„: {eval_df['anchor_age'].min()}-{eval_df['anchor_age'].max()})\")\n",
    "    \n",
    "    # 5. TaskB ì²˜ë¦¬ê¸° ì´ˆê¸°í™”\n",
    "    print(\"\\n2. TaskB ì²˜ë¦¬ê¸° ì´ˆê¸°í™” (Llama ëª¨ë¸)...\")\n",
    "    processor = TaskBProcessor(api_key)\n",
    "    \n",
    "    # 6. ì˜ˆì¸¡ ìƒì„± (ëŒ€íšŒì™€ ë™ì¼í•œ ë°°ì¹˜ í¬ê¸°)\n",
    "    print(\"3. AI ì˜ˆì¸¡ ìƒì„± ì¤‘ (API ì œí•œ ì¤€ìˆ˜)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    data_batch = [{'radiology report': row['radiology report']} for _, row in eval_df.iterrows()]\n",
    "    \n",
    "    # ëŒ€íšŒ API ì œí•œ ì¤€ìˆ˜ (1ë¶„ë‹¹ 10ê±´)\n",
    "    results = []\n",
    "    batch_size = 8  # ì•ˆì „ ë§ˆì§„\n",
    "    \n",
    "    for i in range(0, len(data_batch), batch_size):\n",
    "        batch = data_batch[i:i+batch_size]\n",
    "        print(f\"   ë°°ì¹˜ {i//batch_size + 1}/{(len(data_batch)-1)//batch_size + 1} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # ì „ì²˜ë¦¬\n",
    "        preprocessed = [await processor.preprocess_data(row) for row in batch]\n",
    "        \n",
    "        # API í˜¸ì¶œ\n",
    "        tasks = [processor.chain.ainvoke(prep) for prep in preprocessed]\n",
    "        responses = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # í›„ì²˜ë¦¬\n",
    "        batch_results = [await processor.postprocess_result(r.content) for r in responses]\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "        # API ì œí•œ ì¤€ìˆ˜\n",
    "        if i + batch_size < len(data_batch):\n",
    "            print(f\"   API ì œí•œ ì¤€ìˆ˜ë¥¼ ìœ„í•´ 70ì´ˆ ëŒ€ê¸°...\")\n",
    "            await asyncio.sleep(70)\n",
    "    \n",
    "    predictions = results\n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"ì˜ˆì¸¡ ìƒì„± ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {generation_time:.1f}ì´ˆ)\")\n",
    "    \n",
    "    # 7. ì •ë‹µ ë°ì´í„° ì¤€ë¹„\n",
    "    references = eval_df['target'].tolist()\n",
    "    \n",
    "    # 8. ëŒ€íšŒ ì œê³µ BERTScore ê³„ì‚° (ì •í™•íˆ ë™ì¼í•œ ì„¤ì •)\n",
    "    print(\"\\n4. ëŒ€íšŒ BERTScore ê³„ì‚° ì¤‘...\")\n",
    "    bert_scorer = BertScore(model_type=\"distilbert-base-uncased\", batch_size=16)\n",
    "    bert_scores = bert_scorer(refs=references, hyps=predictions)\n",
    "    bert_mean = np.mean(bert_scores)\n",
    "    bert_std = np.std(bert_scores)\n",
    "    \n",
    "    # 9. ëŒ€íšŒ ì œê³µ ê³µì •ì„± ì§€í‘œ ê³„ì‚° (ì •í™•íˆ ë™ì¼í•œ ì„¤ì •)\n",
    "    print(\"5. ëŒ€íšŒ ê³µì •ì„± ì§€í‘œ ê³„ì‚° ì¤‘...\")\n",
    "    fairness_scorer = FairnessScore(bin_width=10, min_samples_per_group=1)\n",
    "    \n",
    "    # ì„±ë³„ ê³µì •ì„±\n",
    "    gender_fairness = fairness_scorer(\n",
    "        groups=eval_df['gender'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='sex'\n",
    "    )\n",
    "    gender_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # ì—°ë ¹ ê³µì •ì„±\n",
    "    age_fairness = fairness_scorer(\n",
    "        groups=eval_df['anchor_age'].tolist(),\n",
    "        scores=bert_scores,\n",
    "        type='age'\n",
    "    )\n",
    "    age_stats = fairness_scorer.last_stats\n",
    "    \n",
    "    # 10. ëŒ€íšŒ ì •í™•í•œ ê²°ê³¼ ì¶œë ¥\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¯ ëŒ€íšŒ ì •í™•í•œ í‰ê°€ ê²°ê³¼ - Task B (Test ë°ì´í„°)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"ğŸ“Š BERTScore (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   í‰ê· : {bert_mean:.6f}\")\n",
    "    print(f\"   í‘œì¤€í¸ì°¨: {bert_std:.6f}\")\n",
    "    print(f\"   ìµœê³ : {max(bert_scores):.6f}\")\n",
    "    print(f\"   ìµœì €: {min(bert_scores):.6f}\")\n",
    "    print(f\"   ì¤‘ì•™ê°’: {np.median(bert_scores):.6f}\")\n",
    "    \n",
    "    print(f\"\\nâš–ï¸ ê³µì •ì„± ì§€í‘œ (ëŒ€íšŒ ê³µì‹ ê³„ì‚°)\")\n",
    "    print(f\"   ì„±ë³„ ê³µì •ì„±: {gender_fairness:.6f}\")\n",
    "    print(f\"   ì„±ë³„ë³„ ì„±ëŠ¥: {gender_stats['by_group']}\")\n",
    "    print(f\"   ì„±ë³„ ê²©ì°¨: {gender_stats['gap']:.6f}\")\n",
    "    print(f\"   \")\n",
    "    print(f\"   ì—°ë ¹ ê³µì •ì„±: {age_fairness:.6f}\")\n",
    "    print(f\"   ì—°ë ¹ëŒ€ë³„ ì„±ëŠ¥: {age_stats['by_group']}\")\n",
    "    print(f\"   ì—°ë ¹ ê²©ì°¨: {age_stats['gap']:.6f}\")\n",
    "    \n",
    "    # 11. ì •ëŸ‰ í‰ê°€ ì ìˆ˜ ê³„ì‚° (ëŒ€íšŒ ê¸°ì¤€)\n",
    "    print(f\"\\nğŸ† ëŒ€íšŒ ì •ëŸ‰ í‰ê°€ ì ìˆ˜\")\n",
    "    \n",
    "    # BERTScore ì ìˆ˜ (3ì  ë§Œì )\n",
    "    bert_score_points = min(3.0, max(0.0, (bert_mean / 0.85) * 3.0))\n",
    "    \n",
    "    # ê³µì •ì„± ì ìˆ˜ (2ì  ë§Œì )\n",
    "    fairness_avg = (gender_fairness + age_fairness) / 2.0\n",
    "    fairness_points = min(2.0, max(0.0, (fairness_avg / 0.95) * 2.0))\n",
    "    \n",
    "    # ì´ì \n",
    "    total_quantitative = bert_score_points + fairness_points\n",
    "    \n",
    "    print(f\"   BERTScore: {bert_score_points:.3f}/3.000 ì \")\n",
    "    print(f\"   ê³µì •ì„± ì§€í‘œ: {fairness_points:.3f}/2.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ì´ì : {total_quantitative:.3f}/5.000 ì \")\n",
    "    print(f\"   ì •ëŸ‰ ë‹¬ì„±ë¥ : {total_quantitative/5.0*100:.1f}%\")\n",
    "    \n",
    "    # 12. ì„±ëŠ¥ ë“±ê¸‰ íŒì •\n",
    "    print(f\"\\nğŸ–ï¸ ì„±ëŠ¥ ë“±ê¸‰\")\n",
    "    if total_quantitative >= 4.5:\n",
    "        grade = \"Sê¸‰ (ìµœìš°ìˆ˜)\"\n",
    "        recommendation = \"ì¦‰ì‹œ ì œì¶œ ê¶Œì¥\"\n",
    "    elif total_quantitative >= 4.0:\n",
    "        grade = \"Aê¸‰ (ìš°ìˆ˜)\"\n",
    "        recommendation = \"ì œì¶œ ê¶Œì¥\"\n",
    "    elif total_quantitative >= 3.5:\n",
    "        grade = \"Bê¸‰ (ì–‘í˜¸)\"\n",
    "        recommendation = \"ì†Œí­ ê°œì„  í›„ ì œì¶œ\"\n",
    "    elif total_quantitative >= 3.0:\n",
    "        grade = \"Cê¸‰ (ë³´í†µ)\"\n",
    "        recommendation = \"ê°œì„  í•„ìš”\"\n",
    "    else:\n",
    "        grade = \"Dê¸‰ (ë¯¸í¡)\"\n",
    "        recommendation = \"ëŒ€í­ ê°œì„  í•„ìš”\"\n",
    "    \n",
    "    print(f\"   ë“±ê¸‰: {grade}\")\n",
    "    print(f\"   ê¶Œì¥ì‚¬í•­: {recommendation}\")\n",
    "    \n",
    "    # 13. ìƒ˜í”Œ ê²°ê³¼ ë¶„ì„\n",
    "    print(f\"\\nğŸ“ ì˜ˆì¸¡ í’ˆì§ˆ ìƒ˜í”Œ (ìƒìœ„/í•˜ìœ„ ê° 2ê°œ)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    sorted_indices = np.argsort(bert_scores)\n",
    "    \n",
    "    print(\"ğŸ† ìµœê³  ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[-(i+1)]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:120]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    print(\"âš ï¸ ìµœì € ì„±ëŠ¥ ìƒ˜í”Œ:\")\n",
    "    for i in range(2):\n",
    "        idx = sorted_indices[i]\n",
    "        print(f\"ìƒ˜í”Œ {idx} (BERTScore: {bert_scores[idx]:.4f})\")\n",
    "        print(f\"ì˜ˆì¸¡: {predictions[idx][:120]}...\")\n",
    "        print(f\"ì •ë‹µ: {references[idx][:120]}...\")\n",
    "        print()\n",
    "    \n",
    "    return {\n",
    "        'bert_score_mean': bert_mean,\n",
    "        'bert_score_std': bert_std,\n",
    "        'bert_scores': bert_scores,\n",
    "        'gender_fairness': gender_fairness,\n",
    "        'age_fairness': age_fairness,\n",
    "        'total_score': total_quantitative,\n",
    "        'grade': grade,\n",
    "        'predictions': predictions,\n",
    "        'references': references,\n",
    "        'evaluation_samples': eval_samples,\n",
    "        'processing_time': generation_time\n",
    "    }\n",
    "\n",
    "# ì‹¤í–‰ (taskB_test.csv ì‚¬ìš©)\n",
    "API_KEY = \"cfa06ca698c85aa9c9d4b55440aeef0f85ed94f644cd7b931fdd69f2421c6ecb\"\n",
    "TEST_CSV_PATH = \"../data/taskB_train.csv\"\n",
    "\n",
    "# ëŒ€íšŒ ì •í™•í•œ ì¡°ê±´ìœ¼ë¡œ Test ë°ì´í„° í‰ê°€ ì‹¤í–‰\n",
    "test_results = await exact_competition_evaluation(\n",
    "    train_csv_path=TEST_CSV_PATH,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ TaskB Test ë°ì´í„° í‰ê°€ ì™„ë£Œ!\")\n",
    "print(f\"ìµœì¢… ì˜ˆìƒ ì ìˆ˜: {test_results['total_score']:.3f}/5.000 ì \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117434c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
